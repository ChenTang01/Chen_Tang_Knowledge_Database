\documentclass[10pt]{report}
\input{chentang}

\title{\textbf{Chen Tang's \break Knowledge Database}}
\author{\textbf{ChenTang@link.cuhk.edu.cn}}
\date{Last updated on \today}

\begin{document}

\maketitle

\chapter*{Preface}
\addcontentsline{toc}{chapter}{Preface}
\begin{center}
    {\Large The following is a compendium of my academic notes
        spanning various domains.
        I present these notes
        publicly to share my methodological framework for
        managing and structuring an individual's knowledge
        networks.\\
        The inevitability of encountering
        occasional errors is acknowledged.\\
        \bigskip
        \textbf{This notebook will undergo continuous updates.}}
\end{center}

\tableofcontents

\chapter{Mathematics, Statistics \& Optimization}

\section{Calculus and Linear Algebra}

\subsection{Keys of Calculus}

\subsection{Keys of Linear Algebra}

The main contents of this subsection are notes from \cite{deisenroth2020mathematics}, \cite{axler2023linear}, \cite{petersen2008matrix}.
The properties of Matrix Multiplication:
\tab{
    \item Associativity: $(AB)C=A(BC)$;
    \item Distributivity: $A(C+D)=AC+AD$;
    \item Identity Multiplication: $I_mA=A,AI_n=A$
}
Only square matrixes have the inverse matrix, and the inverse is unique. If a matrix has an inverse, then it's called \tbf{regular/invertible/nonsingular}.
The properties of inverses and transposes:
\tab{
\item $(AB)^{-1}=B^{-1}A^{-1}$;
\item $(A+B)^{-1}\neq A^{-1}+B^{-1}$;
\item $(AB)^\top=B^\top A^\top $;
\item $(A+B)^\top=A^\top+B^\top $;
\item $(A^\top)^{-1} = (A^{-1})^\top$.
}
To find the inverse matrix, gaussian elimination can be applied: $[A\mid I]=[I\mid A^{-1}]$.\\
\sep{Solutions of Linear Systems}
\tbf{Reduced Row-Echelon Form Matrix}:
\eq{
    \boldsymbol{A}=\begin{bmatrix}1&3&0&0&3\\0&0&1&0&9\\0&0&0&1&-4\end{bmatrix}
}
There are \tit{pivot (basic variables)} and \tit{free variable}, and the column of free variable is dependent on pivots. The steps to find solutions of linear systems:
\lis{
    \item Find a particular solution for $Ax=\boldsymbol{b}$ by setting all free variable zero;
    \item Find all solutions for $Ax=0$;
    \item Add them up.
}
There is an iterative method to solve large-scale linear equations:
define error = $\|x^{(k+1)}-x_*\|$, then optimize the function $x^{(k+1)}=Cx^{(k)}+d$ and iterate it.\\
\sep{Vector Spaces, Basis and Rank}
\defi{Vector Space and Subspace}{
    A \tbf{Vector Space} $V=(\mathcal{V},+,\cdot)$ is a set $\cV$ with two operations:
    \lis{
        \item $+:\mathcal{V}\times\mathcal{V}\to\mathcal{V}$;
        \item $\cdot:\mathbb{R}\times\mathcal{V}\to\mathcal{V}$
    }
    For $\mathcal{U}\subseteq\mathcal{V}$ and $\mathcal{U}\neq\emptyset$, then $U=(\cU,+,\cdot)$ is a vector subspace.\\
    \re{
        \tab{
            \item $V=(\mathcal{V},+)$ is an \tit{Abelian group};
            \item The subspace needs to satisfy \tit{closure}, $\lambda x \in U, x+y\in U$.
        }
    }
}
If $v=\sum_{i=1}^{k}\lambda_ix_i$, then $v$ is a linear combination of $(x_1,x_2,\cdots,x_k)$. If \tbf{not} all values of a solution are
0, then it's called a non-trivial solution. For $\sum_{i=1}^{k}\lambda_ix_i=0$, if the non-trivial solution exists, then the vectors are called \tbf{linearly dependent}.
\defi{Basis and Rank}{
    \tab{
        \item \tbf{Generating Set}: if all vectors in $V$ can be expressed as a linear combination of $\mathcal{A}=\{\boldsymbol{x}_{1},\ldots,\boldsymbol{x}_{k}\}\subseteq\mathcal{V}$,
        then $\cA$ is a generating set of $V$;
        \item \tbf{Span}: The set of linear combinations of $\cA$ is its span;
        \item \tbf{Basis}: The minimal generating set (linearly independent) of a vector space $V$ is called its basis;
        \item \tbf{Rank}: the number of linearly independent column vectors in a matrix $\cA \in \RR&{m\times n}$.
    }
    \re{
        \tab{
            \item $\operatorname{rk}(A)=\operatorname{rk}(A^{\top})$;
            \item A matrix $A\in\mathbb{R}^{n\times n}$ is invertible if and only if $rk(A)=n$;
            \item The span of a matrix is also called its \tbf{image}, $dim(U)=rk(A)$;
            \item $Ax=b$ only has solution if and only if $rk(A)=rk(A\mid b)$;
            \item The solution (kernel, null space) to $Ax=0$ has a dimension of $n-rk(A)$;
        }
    }
}
\defi{Linear Mappings}{
    For vector spaces $V,W$, a mapping $\Phi:V\rightarrow W$ is called linear mapping if:
    \eq{
        \forall x,y\in V\forall\lambda,\psi\in\mathbb{R}:\Phi(\lambda x+\psi\boldsymbol{y})=\lambda\Phi(x)+\psi\Phi(\boldsymbol{y})
    }
}
\sep{Determinant and Trace}
\tbf{Determinant} is used to decide whether a matrix is invertible, denoted by $det(A)$ or $|A|$, its geometric meaning is
the signed volume. \tit{Laplace Expansian} can be used to compute the determinant for large matrixes.\\
If $det(A)=0$, then it's non-invertible. otherwise, it's invertible.\\
\tbf{Trace} is defined as $Tr(A)=\sum_{i=1}^n a_{ii}$. Both determinant and trace can only be applied to the squared matrix.
\sep{Eigenvalue and Postive-Definiteness}
Consider matrix $A\in \RR^{n\times n}$ as a linear mapping rather than a simple matrix, $A$ can map a n-dimensional space
to another n-dimensional space.
\fig{Mona-Lisa}{
    A shear mapping example
}{0.3}
The Mona Lisa example is a linear mapping of two-dimensional space, but during the mapping, there are some vectors (such as the blue vector)
that are only modified in length rather than direction. This is the idea of the eigenvector.
\defi{Eigenvalue and Eigenvector}{
    For a square matrix $A$, if there exists a scalar $\lambda$ and a vector $v\in \RR^n$, such that $A\cdot v=\lambda\cdot v$.
    Which would lead us to $(A-\lambda I)\cdot v=\tbf{0}$.
    \re{
        The following statements are equivalent:
        \tab{
            \item $\lambda$ is an eigenvalue of $A\in \RR^{n\times n}$;
            \item $rk(A-\lambda I)<n$;
            \item $det(A-\lambda I)=0$.
        }
    }
}
The definiteness is similar to the idea of eigenvalue. Consider $X^T MX$ as the inner product of $X$ and $MX$, where
$MX$ can be regarded as a transformed version of $X$. If $X^T MX>0$ for all $X$,
then we can get $\cos \theta=\frac{X^T \cdot MX}{||X||\cdot ||MX||}>0$, which means for all vectors, the linear mapping $M$ can map
the vector to an angle less than 90 degrees.\\
So to show whether a matrix is positive definite, calculate all the eigenvalues of the matrix. If all values are larger than 0, then it's positive definite.
\re{
    \tab{
        \item $Tr(A)=\sum_i^n \lambda_i$;
        \item $\det(A)=\prod_{i=1}^n\lambda_i$;
        \item $\mathrm{Tr(A+B)}=\mathrm{Tr(A)}+\mathrm{Tr(B)}$, $\quad \det(\mathbf{AB})=\det(\mathbf{A})\det(\mathbf{B})$;
        \item $\det(\mathbf{A}^{-1})=1/\det(\mathbf{A})$, $\quad \mathrm{det}(\mathbf{A}^n)=\mathrm{det}(\mathbf{A})^n$;
        \item $\det(\mathbf{I}+\mathbf{u}\mathbf{v}^T)=1+\mathbf{u}^T\mathbf{v}$.
    }
    The approximation of the determinant. For small $\epsilon$:
    \eq{
        \det(\mathbf{I}+\varepsilon\mathbf{A})\cong1+\det(\mathbf{A})+\varepsilon\mathrm{Tr}(\mathbf{A})+\frac12\varepsilon^2\mathrm{Tr}(\mathbf{A})^2-\frac12\varepsilon^2\mathrm{Tr}(\mathbf{A}^2)
    }
}
\sep{The $2\times 2$ Matrix}
Considering the $2\times 2$ matrix $\mathbf{A}$, we have:
\meq{
\det(\mathbf{A})=A_{11}A_{22}-A_{12}A_{21}\\
\mathrm{Tr}(\mathbf{A})=A_{11}+A_{22}\\
\lambda^2-\lambda\cdot\mathrm{Tr}(\mathbf{A})+\det(\mathbf{A})=0 \\
\left.\mathbf{A}^{-1}=\frac1{\det(\mathbf{A})}\left[\begin{array}{cc}A_{22}&-A_{12}\\-A_{21}&A_{11}\end{array}\right.\right]
}

\subsection{Norms \& Derivatives of the Matrix}

\defi{Norm}{
The norm $||\codt||$ is a mapping $V\to \RR$, which assigns each vector a length that satisfy:
\tab{
    \item \tit{Absolutely homogeneous}: $\|\lambda x\|=|\lambda|\|x\|$;
    \item \tit{Triangle inequality}: $\lVert x+y\rVert\leqslant\lVert x\rVert+\lVert y\rVert$;
    \item \tit{Positivity}: $||x||\ge0$ and $\|\boldsymbol{x}\|=0\iff\boldsymbol{x}=\boldsymbol{0}$.
}
The $L_p$ norm: $\|x\|_p=\left(|x_1|^p+|x_2|^p+\cdots+|x_n|^p\right)^{1/p}$.\\
\co{$L_p$ norm}{
    \lis{
        \item \tbf{Manhattan Norm}: $\|\boldsymbol{x}\|_1:=\sum_{i=1}^n|x_i|$;
        \item \tbf{Euclidean Norm}: $\|\boldsymbol{x}\|_2:=\sqrt{\sum_{i=1}^nx_i^2}=\sqrt{\boldsymbol{x}^\top\boldsymbol{x}}$ (mostly denoted as $||\cdot||$);
        \item \tbf{Infinity Norm}: $\|x\|_\infty=\max\left\{|x_1|,|x_2|,\ldots,|x_n|\right\}$.
    }
}
\proo{Infinity Norm}{
By squeeze theorem:\\
First show that $\|x\|_p=(\sum_i|x_i|^p)^{\frac1p}\leq(\sum_i\max_i|x_i|^p)^{\frac1p}=n^{\frac1p}\max_i|x_i|\to\max_i|x_i|=\|x\|_\infty $,\\
Next show that $\|x\|_p=(\sum_i|x_i|^p)^{\frac1p}\geq(\max_i|x_i|^p)^{\frac1p}=\max_i|x_i|=\|x\|_\infty $.
}

}
\sep{Taking derivatives of Norm}
\ex{$X\in \RR^m, f(x):\RR^m\to\RR^n, g(x)=||x||_2$, what is $\nabla g(X)$?}{
    First, $g(X)=\|f(X)\|_2=\sqrt{\sum_{i=1}^nf_i(X)^2}$;
    \eq{
        \nabla g(X)=\frac12\left(\sum_{i=1}^nf_i(X)^2\right)^{-\frac12}\left(\sum_{i=1}^n2f_i(X)\nabla f_i(X)\right)=\frac{J_f(X)^Tf(X)}{\|f(X)\|_2}
    }
}

\subsubsection{Matrix Norms}
\defi{Induced Matrix Normss}{
Let $A\in\RR^{W\times V}$ be a mapping from vector space $\cV$ to $\cW$.
\eq{
\|A\|_{\cV,\cW}:=\max_{x\neq0}\frac{\|Ax\|_{\cW}}{\|x\|_{\cV}}.
}
This measures how much the matrix $A$ can stretch an $n$ dimensional vector at most. Because norms are homogeneous to scaling, we also have:
\eq{
\|A\|_{\mathcal{V},\mathcal{W}}=\sup_{\|v\|_{\mathcal{V}=1}}\|Av\|_{\mathcal{W}}.
}
Besides the 3 satisfactions listed above, there are two more properties that a matrix norm needs to satisfy:
\tab{
    \item \tit{Sub-multiplicative:} $\|AB\|\leq\|A\|\cdot\|B\|\text{ for all }A,B$;
    \item \tit{Compatible with vector norm}: $\|Ax\|\leq\|A\|\cdot\|x\|\text{ for all }A \text{ and vector }x$.
}
\proo{Sub-multiplicative}{
    \eq{
        \|AB\|=\max_{\|v\|=1}\|ABv\|\le\max_{\|v\|=1}\|A\|\|Bv\|=\|A\|\|B\|.
    }
}
For a matrix $A\in\RR^{m\times n}$:
\eq{
\|A\|_1:=\max_{j\in\{1,\ldots,n\}}\sum_{i=1}^m|a_{ij}|, \|A\|_\infty:=\max_{i\in\{1,\ldots,m\}}\sum_{j=1}^n|a_{ij}|.
}
The $\|\cdot\|_1$ is the \bt{maximum absolute column sum}, the $\|\cdot\|_{\infty}$ is the \bt{maximum absolute row sum}.\\
The \tbf{spectral norm}:
\eq{
    \|A\|:=\|A\|_2:=\sqrt{\lambda_{\max}(A^\top A)}.
}
The \tbf{Frobenius Norm}:
\eq{
    \|A\|_F=\left(\sum_{i=1}^m\sum_{j=1}^n|a_{ij}|^2\right)^{1/2}=\sqrt{\operatorname{tr}(A^\top A)}.
}
}

\clearpage
\section{Probability Theory}

The main contents of this section are from \cite{Ross2014}.

\subsection{Probability Distribution}
\sep{Common Discrete Distribution}
\tbf{Bernoulli}(p)\\
\tit{pmf}: $P(X=x\mid p)=p^{x}(1-p)^{1-x};\quad x=0,1;\quad0\leq p\leq1$\\
\tit{mean}: $EX=p$; \tit{variance}: $VarX=p(1-p)$\\
\tab{
    \item A Bernoulli trial (named after James Bernoulli) is an experiment with only
    two possible outcomes;
    \item Bernoulli random variable $X = 1$ if “success” occurs and $X = 0$ if “failure”
    occurs where the probability of a “success” is $p$.
}
\tbf{Binomial}(n,p)\\
\tit{pmf}: $P(X=x\mid n,p)=\binom{n}{x}p^{x}(1-p)^{n-x},\quad x=0,1,\ldots,n;\quad0\leq p\leq1$\\
\tit{mean}: $EX=np$; \tit{variance}: $np(1-p)$\\
\tab{
    \item A Binomial experiment consists of $n$ independent identical Bernoulli trials;
    \item $X=\sum_{i=1}^nY_i$, where $Y_1,\cdots,Y_n$ are $n$ identical, independent Bernoulli random variables.
}
\tbf{Poisson}($\lambda$)\\
\tit{pmf}: $P(X=x\mid\lambda)=\frac{e^{-\lambda}\lambda^x}{x!};\quad x=0,1,\ldots;\quad0\leq\lambda<\infty $\\
\tit{mean}: $EX=\lambda$; \tit{variance}: $Var X=\lambda$\\
\tab{
    \item A Poisson distribution is typically used to model the probability distribution
    of the number of occurrences (with $\lambda$ being the intensity rate) per unit
    time or per unit area;
    \item Binomial pmf approximates Poisson pmf.
    Poisson pmf is also a limiting distribution of a negative binomial distribution;
    \item A useful result: By Taylor series expansion: $e^{\lambda}=\sum_{x=0}^{\infty}\frac{\lambda^x}{x!}$.
}
\ass{Poisson Process}{
    \lis{
        \item Let $X(\Delta)$ be the number of events that occur during an interval $\Delta$;
        \item The events are independent: if $\Delta_1,\cdots,\Delta_n$ are disjoint intervals,
        then $X(\Delta_1),\cdots,X(\Delta_n)$ are independent;
        \item $X(\Delta)$ only depends on the length of $\Delta$;
        \item The probability that exactly one event occurs in a small interval of length $\Delta t$ equals $\lambda \Delta t+\tit{o}(\Delta t)$;
        \item Poisson distribution is \hl{not} memoryless, but its interval (exponential distribution) is memoryless.
    }
}
\tbf{Geometric}($p$)\\
\tit{pmf}: $P(X=x\mid p)=p(1-p)^{x-1};\quad x=1,2,\ldots;\quad0\leq p\leq1$\\
\tit{mean}: $\frac{1}{p}$; \tit{variance}: $\frac{1-p}{p^2}$\\
\tab{
    \item The experiment consists of a sequence of independent trials;
    \item \emotree The property of memoryless: $P(X>s\mid X.t)=P(X>s-t)$.
}
\tbf{Negative Binomial}($r,p$)\\
\tit{pmf}: $P(X=x\mid r,p)=\binom{r+x-1}{x}p^r(1-p)^x,\quad x=0,1,\ldots;\quad0\leq p\leq1$\\
\tit{mean}: $EX=\frac{r(1-p)}{p}$; \tit{varaince}: $\frac{r(1-p)}{p^2}$\\
\tab{
    \item assume there are many independent and identical experiments, to observe the $r$th success, $X$ is the number of games to see the failure;
}
\tbf{Hypergeometric}($N,M,K$)\\
\tit{pmf}: $\begin{aligned}P(X=x\mid N,M,K) & =\frac{\binom{M}{k}\binom{N-M}{K-x}}{\binom{N}{K}};
               \quad x=0,1,2,\ldots,K;                                                \\M-(N-K)&\le x\le M;\quad N,M,K\ge0\end{aligned}$\\
\tit{mean}: $EX=\frac{KM}{N}$; \tit{varaince}: $\frac{KM}{N}\frac{(N-M)(N-K)}{N(N-1)}$\\

\sep{Common Continuous Distribution}
\tbf{Uniform}($a,b$)\\
\tit{pdf}:$f(x\mid a,b)=\frac{1}{b-a}$; \tit{mean}:$EX=\frac{b+a}{2}$; \tit{variance}: $Var X=\frac{(b-a)^2}{12}$.\\
\tbf{Exponential}($\beta$)\\
\tit{pdf}: $f(x\mid \beta)=\frac{1}{\beta}e^{-x/\beta},0\le x< \infty, \beta>0$; \tit{mean}: $EX=\beta$; \tit{variance}: $VarX=\beta^2$.\\
\tbf{Gamma}($\alpha,\beta$)\\
\tit{pdf}:$f(x\mid\alpha,\beta)=\frac{1}{\Gamma(\alpha)\beta^\alpha}x^{\alpha-1}e^{-x/\beta},\quad0\leq x<\infty,\quad\alpha,\beta>0$;
\tit{mean}:$\alpha\beta$; \tit{variance}:$\alpha\beta^2$.\\
\tab{
\item The \tit{gamma function} is defined as $\Gamma(\alpha)=\int_0^\infty t^{\alpha-1}e^{-t}dt$;
\item $\Gamma(\alpha+1)=\alpha\Gamma(\alpha), \alpha>0$;
\item $\Gamma(n)=(n-1)!,\quad\text{ for any integer }n>0$.
}
\tbf{Normal}($\mu,\sigma^2$)\\
\tit{pdf}:$f\left(x\mid\mu,\sigma^2\right)=\frac{1}{\sqrt{2\pi}\sigma}e^{-(x-\mu)^2/\left(2\sigma^2\right)},\quad-\infty<x<\infty$;
\tit{mean}:$\mu$; \tit{variance}:$\sigma^2$.\\
\ex{for $f(x)=\frac{\beta\alpha^{\beta}}{x^{\beta+1}},\quad\alpha<x<\infty,\quad\alpha>0,\quad\beta>0$: \\
    \tab{
        \item (a) Verify $f(x)$ is a pdf;
        \item (b) Derive the mean and variance of this distribution;
        \item (c) Prove that the variance does not exist if $\beta\leq2$.
    }}{
    (a)
    \eq{\begin{aligned}\int_{\alpha}^{\infty}f(x)dx&=\int_{\alpha}^{\infty}\frac{\beta\cdot \alpha^{\beta}}{x^{\beta+1}}dx=-x^{\beta\cdot\alpha^{\beta}|\alpha}\\&=0+\alpha^{-\beta}\cdot\alpha^{\beta}=1\end{aligned}}
    (b)
    \eq{\begin{aligned}\\Ex=\int_{\alpha}^{\infty}xf(x)dx=\int_{\alpha}^{\infty}\frac{\beta\cdot \alpha^{\beta}}{X^{\beta}}dx\\=\frac{\beta}{-\beta+1}\cdot\alpha^{\beta}\cdot x^{-\beta+1}|_{\alpha}^{\infty}=\frac{\beta\cdot\alpha}{\beta-1}\end{aligned}}
    \eq{\begin{aligned}Ex^{2}&=\int_{\alpha}^{\infty}\frac{\beta\cdot\alpha^{\beta}}{x^{\beta-1}}dx=\frac{\beta}{-\beta+2}\cdot\alpha^{\beta}\cdot x^{-\beta+2}|_{\alpha}^{\infty}\\&=\frac{\alpha^{2}\beta}{\beta-2}\end{aligned}}
    \eq{\operatorname{Var}X=EX^2-(EX)^2=\frac{\beta\alpha^2}{(\beta-1)^2(\beta-2)}}
    (c)\\
    If $\beta<2$, then the variance is negative.
}
\fig{type of distribution}{Type of distribution}{}

\section{Statistical Inference}
This section is mainly the notes from \cite{casella2021statistical}
\fig{statistical inference}{The scope of statistical inference}{}
Statistics uses observed data to \hl{inference} the statistical model.

\subsection{Exponential Families}
\defi{Exponential Families
}{A family of pdfs or pmfs is called an \tit{exponential family} if:
    \eq{
        f(x|\boldsymbol{\theta})=h(x)c(\boldsymbol{\theta})\exp\left(\sum_{i=1}^kw_i(\boldsymbol{\theta})t_i(x)\right)
    }
    Where $h(x)\le0,c(\boldsymbol{\theta})\le0$, and $h(x),t_i(x)$ don't depend on $\boldsymbol{\theta}$. $c(\boldsymbol{\theta}),
        w_i(\boldsymbol{\theta})$ don't depend on $x$.
    \tab{
        \item Continuous: normal, gamma, beta, exponential;
        \item Discrete: binomial, poisson, nagative binomial;
        \item $\boldsymbol{\theta}=\theta_1,\theta_2, \codts,\theta_d$, $k$ must $\ge d$;
        \item If $k=d$, then it's a \tit{full exponential family}, if $k>d$, then it's a \tit{curved exponential family} (For example,
        most normal distributions are \tit{full exponential family}, but normal distribution satisfy $\mu=\sigma^2$ is a \tit{curved exponential family}).
    }
}
To verify a pdf is an exponential family, identify the function $h(x),c(\boldsymbol{\theta}),t_i(x),w_i(\boldsymbol{\theta})$, then
verify these functions satisfy the condition above.
\ex{
    Show that Binomial, Poisson, Exponential and Normal distribution belongs to the exponential families.
}{
    \tbf{Binomial}:\\
    \eq{
        \begin{gathered}
            f(x|p) =\binom nxp^x(1-p)^{n-x}=\binom nx(1-p)^n\left(\frac p{1-p}\right)^x \\
            =\begin{pmatrix}n\\x\end{pmatrix}(1-p)^n\exp\left(x\log\left(\frac{p}{1-p}\right)\right),
        \end{gathered}
    }
    Among which $\begin{pmatrix}n\\x\end{pmatrix}$ is $h(x)$, $(1-p)^n$ is $c(\theta)$, $x$ is $t_1(x)$, and $\log\left(\frac{p}{1-p}\right)$ is $w_i(\theta)$.
    Note that $f(x\mid p)$ is only in exponential families when $0<p<1$.\\
    \tbf{Poisson}:\\
    \eq{
        \begin{aligned}
             & f(x|\lambda)=\frac{\lambda^{x}e^{-\lambda}}{x!}=\frac{1}{x!}e^{-\lambda}\exp\left(x\log(\lambda)\right) \\
             & \text{then}                                                                                             \\
             & h(x)=\frac{1}{x!},c(\lambda)=e^{-\lambda},t(x)=x\mathrm{~and}w(\lambda)=\log(\lambda).
        \end{aligned}
    }
    \tbf{Exponential}:\\
    \eq{
        \begin{aligned}
             & f(x|\beta)=\frac{1}{\beta}\exp\left(-\frac{x}{\beta}\right)                   \\
             & \text{then}                                                                   \\
             & h(x)=1,c(\beta)=\frac{1}{\beta},t(x)=x\mathrm{~and}w(\beta)=-\frac{1}{\beta}.
        \end{aligned}
    }
    \tbf{Normal}:\\
    \eq{
        \begin{aligned}
             & f(x|\mu,\sigma^{2})=\frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{(x-\mu)^{2}}{2\sigma^{2}}\right)=\frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{x^{2}}{2\sigma^{2}}+\frac{x\mu}{\sigma^{2}}-\frac{\mu^{2}}{2\sigma^{2}}\right) \\
             & \text{then}                                                                                                                                                                                                                 \\
             & \begin{aligned}h(x)=1,c(\mu,\sigma)=\frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{\mu^{2}}{2\sigma^{2}}\right),\end{aligned}                           \\
             & t_{1}(x)=-\frac{x^{2}}{2},w_{1}(\mu,\sigma)=\frac{1}{\sigma^{2}},t_{2}(x)=x\mathrm{~and~}w_{2}(\mu,\sigma)=\frac{\mu}{\sigma^{2}}.
        \end{aligned}
    }
}
\ex{Show the following are exponential families:
\tab{
    \item Gamma family with either $\alpha,\beta$ is unknown or both unknown;
    \item Beta family with either $\alpha,\beta$ is unknown or both unknown;
    \item Negative Binomial family when $r$ is unkown.
}}{
\tbf{Gamma $\alpha$ unkown}:
\eq{f(x|\alpha;\beta)=e^{-x/\beta}\frac1{\Gamma(\alpha)\beta^\alpha}\exp((\alpha-1)\log x)}
thus $h(x)=e^{-x/\beta},x>0;c(\alpha)=\frac1{\Gamma(\alpha)\beta^\alpha};w_1(\alpha)=\alpha-1;t_1(x)=\log x.$\\
\tbf{Gamma $beta$ unknown}:
\eq{f(x|\beta;\alpha)=\frac1{\Gamma(\alpha)\beta^\alpha}x^{\alpha-1}e^{\frac{-x}\beta}}
thus $h(x)=\frac{x^{\alpha-1}}{\Gamma(\alpha)},x>0;c(\beta)=\frac1{\beta^\alpha};w_1(\beta)=\frac1\beta; t_1(x)=-x.$\\
\tbf{Gamma both unknown}:
\eq{f(x|\alpha,\beta)=\frac1{\Gamma(\alpha)\beta^\alpha}\exp((\alpha-1)\log x-\frac x\beta)}
thus $h(x)=I_{\{x>0\}}(x);c(\alpha,\beta)=\frac{1}{\Gamma(\alpha)\beta^\alpha};w_1(\alpha)=\alpha-1;t_1(x)=\log x;w_2(\alpha,\beta)=-1/\beta;t_2(x)=x.$\\
\tbf{Beta $\alpha$ unknown}:\\
$h(x)=(1-x)^{\beta-1}I_{[0,1]}(x),\quad c(\alpha)=\frac{1}{B(\alpha,\beta)},\quad w_1(x)=\alpha-1,\quad t_1(x)=\log x$\\
\tbf{Beta $\beta$ unkown}:\\
$h(x)=x^{\alpha-1}I_{[0,1]}(x),c(\beta)=\frac{1}{B(\alpha,\beta)},w_1(\beta)=\beta-1,t_1(x)=\log(1-x)$\\
\tbf{Beta both unknown}:\\
$h(x)=I_{[0,1]}(x),c(\alpha,\beta)=\frac{1}{B(\alpha,\beta)},w_1(\alpha)=\alpha-1,t_1(x)=\log x,w_2(\beta)=\beta-1,t_2(x)=\log(1-x).$\\
\tbf{Negative Binomial}:\\
\eq{h(x)=\left(\begin{matrix}r+x-1\\x\end{matrix}\right)I_{\NN}(x),\quad c(p)=\left(\frac{p}{1-p}\right)^r,\quad w_1(p)=\log(1-p),\quad t_1(x)=x.}
}
\thm{
    Expectation and Variance of Exponential Families
}{
    If X is a random variable that satisfies any distribution from the exponential families, then:
    \eq{
        \begin{aligned}
             & 1.\quad\operatorname{E}\left(\sum_{i=1}^{k}\frac{\partial w_{i}(\boldsymbol{\theta})}{\partial\theta_{j}}t_{i}(X)\right)=-\frac{\partial}{\partial\theta_{j}}\log\big(c(\boldsymbol{\theta})\big);                                                                                                                           \\
             & 2.\quad\mathrm{Var}\left(\sum_{i=1}^{k}\frac{\partial w_{i}(\boldsymbol{\theta})}{\partial\theta_{j}}t_{i}(X)\right)=-\frac{\partial^{2}}{\partial\theta_{j}^{2}}\log\left(c(\boldsymbol{\theta})\right)-\mathrm{E}\left(\sum_{i=1}^{k}\frac{\partial^{2}w_{i}(\boldsymbol{\theta})}{\partial\theta_{j}^{2}}t_{i}(X)\right).
        \end{aligned}
    }
}
\ex{Derive the mean and variance for binomial and normal distribution using the above theorem.}{
    \tbf{Binomial}:\\
    \eq{
        \begin{aligned}
             & h(x)=\begin{pmatrix}n\\x\end{pmatrix},c(p)=(1-p)^n,t(x)=x\mathrm{and}w(p)=\log\biggl(\frac{p}{1-p}\biggr).                                     \\
             & \mathrm{Then},                                                                                                                                 \\
             & \frac{\mathrm{d}}{\mathrm{d}p}w(p)=\frac{\mathrm{d}}{\mathrm{d}p}\log\left(\frac{p}{1-p}\right)=\frac{1}{p(1-p)},                              \\
             & \frac{\mathrm{d}^{2}}{\mathrm{d}p^{2}}w(p)=-\frac{1}{p^{2}}+\frac{1}{(1-p)^{2}}=\frac{2p-1}{p^{2}(1-p)^{2}},                                   \\
             & \frac{\mathrm{d}}{\mathrm{d}p}\log\big(c(p)\big)=\frac{\mathrm{d}}{\mathrm{d}p}n\log(1-p)=-\frac{n}{1-p},                                      \\
             & \frac{\mathrm{d}^{2}}{\mathrm{d}p^{2}}\log\left(c(p)\right)=-\frac{n}{(1-p)^{2}}.                                                              \\
             & \text{Therefore, from Theorem 3.4.2, we have}                                                                                                  \\
             & \operatorname{E}\left(\frac1{p(1-p)}X\right)=\frac n{1-p}\Rightarrow\operatorname{E}(X)=np,                                                    \\
             & \mathrm{Var}\left(\frac{1}{p(1-p)}X\right)=\frac{n}{(1-p)^2}-\mathrm{E}\left(\frac{2p-1}{p^2(1-p)^2}X\right)\Rightarrow\mathrm{Var}(X)=np(1-p)
        \end{aligned}
    }
    \tbf{Normal}:\\
    \eq{
        \begin{aligned}
             & \text{For Normal Distribution, we have}                                                                                                                                                                                                          \\
             & h(x)=1,c(\mu,\sigma)=\frac1{\sqrt{2\pi}\sigma}\exp\left(-\frac{\mu^2}{2\sigma^2}\right),                                                                                                                                                         \\
             & t_{1}(x)=-\frac{x^{2}}{2},w_{1}(\mu,\sigma)=\frac{1}{\sigma^{2}},t_{2}(x)=x\mathrm{and}w_{2}(\mu,\sigma)=\frac{\mu}{\sigma^{2}}.                                                                                                                 \\
             & \mathrm{Then},                                                                                                                                                                                                                                   \\
             & \begin{aligned}\frac{\partial w_1(\mu,\sigma)}{\partial\mu}=\frac{\partial(1/\sigma^2)}{\partial\mu}=0,\end{aligned}                                                                                                                             \\
             & \begin{aligned}\frac{\partial w_2(\mu,\sigma)}{\partial\mu}=\frac{\partial(\mu/\sigma^2)}{\partial\mu}=\frac{1}{\sigma^2},\end{aligned}                                                                                                          \\
             & \begin{aligned}\frac{\partial w_1(\mu,\sigma)}{\partial\sigma}=\frac{\partial(1/\sigma^2)}{\partial\sigma}=-\frac{2}{\sigma^3},\end{aligned}                                                                                                     \\
             & \frac{\partial w_2(\mu,\sigma)}{\partial\sigma}=\frac{\partial(\mu/\sigma^2)}{\partial\sigma}=-\frac{2\mu}{\sigma^3},                                                                                                                            \\
             & \frac{\partial}{\partial\mu}\log\left(c(\mu,\sigma)\right)=\frac{\partial}{\partial\mu}\left(-\frac{\log(2\pi)}{2}-\log(\sigma)-\frac{\mu^{2}}{2\sigma^{2}}\right)=-\frac{\mu}{\sigma^{2}},                                                      \\
             & \frac\partial{\partial\sigma}\log\left(c(\mu,\sigma)\right)=\frac\partial{\partial\sigma}\left(-\frac{\log(2\pi)}2-\log(\sigma)-\frac{\mu^2}{2\sigma^2}\right)=-\frac1\sigma+\frac{\mu^2}{\sigma^3}.                                             \\
             & \operatorname{E}\left(\frac{1}{\sigma^{2}}X\right)=\frac{\mu}{\sigma^{2}}\operatorname{and}\operatorname{E}\left(-\frac{2}{\sigma^{3}}\left(-\frac{X^{2}}{2}\right)-\frac{2\mu}{\sigma^{3}}X\right)=\frac{1}{\sigma}-\frac{\mu^{2}}{\sigma^{3}}, \\
             & \text{which implies}                                                                                                                                                                                                                             \\
             & \operatorname{E}(X)=\mu,\operatorname{E}(X^{2})=\mu^{2}+\sigma^{2}\mathrm{and}\mathrm{Var}(X)=\mathrm{E}(X^{2})-(\mathrm{E}X)^{2}=\sigma^{2}
        \end{aligned}
    }
}
\defi{
    The indicator function
}{
    I_A(x)=\begin{cases}1,&\text{if }x\in A\\0,&\text{if }x\notin A\end{cases}
}
\ex{
Show that $f(x\mid \theta)=\frac{1}{\theta}exp(1-\frac{X}{\theta})$ is \tbf{NOT} an exponential family.
}{\eq{
f(x|\theta)=\frac1\theta\exp\left(1-\frac x\theta\right)I_{[\theta,\infty)}(x)
}
At here $h(x)=I_{[\theta,\infty)}(x)$, which is not independent with $\theta$.}
\defi{
    Reparameterization of Exponential Families
}{
    \eq{
        f(x|\boldsymbol{\eta})=h(x)c^*(\boldsymbol{\eta})\exp\left(\sum_{i=1}^k\eta_it_i(x)\right)
    }
    Where $h(x)$ and $t_i(x)$ are identical with the original parameterization. $\boldsymbol{\eta}=(\eta_1,\eta_2,\cdots,\eta_n)$ and
    $\eta_i=w_i(\bold{\theta})$. And to make it a pdf (integrates to 1):
    \eq{
        c^*(\boldsymbol{\eta})=\left[\int_{-\infty}^\infty h(x)\exp\left(\sum_{i=1}^k\eta_it_i(x)\right)dx\right]^{-1}
    }
}

\subsection{Scale and Location}
step 1: define the standard pdf $f(Z)$, for example: $f(Z)=e^{-Z},Z\ge0$;\\
step 2: find the relationship between $X$ and $X$: $\sigma Z+\mu=X$, where $\sigma$ is the scale parameter and $\mu$ is the location parameter;\\
step 3: replace $Z$ using $X$: $f(x)=\frac{1}{\sigma}f(\frac{x-\mu}{\sigma})$ is a distribution transformed from the standard pdf.
\thm{Sacle-Location Family}{If f(x) is any pdf, for $\mu, \sigma>0$, then the function
    $g(x\mid \mu,\sigma)=\frac{1}{\sigma}f(\frac{x-\mu}{\sigma})$ is a valid pdf.
    \proo{}{
        \eq{
            \begin{aligned}g(x|\mu,\sigma)&=\frac{1}{\sigma}f\left(\frac{x-\mu}{\sigma}\right)\geq0\\\\\int_{-\infty}^{\infty}g(x|\mu,\sigma)dx&=\int_{-\infty}^{\infty}\frac{1}{\sigma}f\left(\frac{x-\mu}{\sigma}\right)dx\xrightarrow{\left(y=\frac{x-\mu}{\sigma}\right)}\int_{-\infty}^{\infty}f(y)dy=1.\end{aligned}
        }
    }
    \re{
        \tab{
            \item For discrete R.V.(pmf), the above theorem doesn't hold. (ignore the $\frac{1}{\sigma}$);
            \item The $\sigma$ is the scale parameter, the $\mu$ is the location parameter.
        }
    }}\label{thm:ScaleLocation}
\ex{\begin{align}
    \quad\int_{\mathbf{X}}(x_1\mid\theta) & = \frac1\theta\exp\{1-\frac x\theta\},\quad x \ge \theta                   \\
                                          & =\frac1\theta\exp\{-\frac{x-\theta}\theta\}\cdot\mathrm{I}_{[0,\infty)}(x)
\end{align} Is $\theta$ a scale parameter or a location parameter?}{
It depends on the standard pdf:\\
If the standard pdf is $f_{Z}(z)=\exp\{-z\}\cdot\mathcal{I}_{[0,+\infty)}(z)$:\\
then $\theta$ serves as both parameters because $f_X(x)=\frac1\theta\exp\{-\frac{x-\theta}\theta\}\cdot I_{[0,+\infty)}(\frac{x-\theta}\theta)$.\\
Else if the standard pdf is $f_{Z}(z)=\exp\{1-z\}\cdot\mathcal{I}_{[0,+\infty)}(z)$:\\
then $\theta$ is only a scale parameter because $f_X(x)=\frac1\theta\exp\{1-\frac{x}\theta\}\cdot I_{[0,+\infty)}(\frac{x-\theta}\theta)$.
}
\thm{For any pdf $f(\cdot)$, and $\sigma, \mu>0$. Then $X$ is a random variable with pdf $\frac{1}{\sigma}f\left(\frac{x-\mu}{\sigma}\right)$
    \tbf{if and only if} there is a random variable $Z$ with pdf $f(z)$ and $X=\sigma Z+\mu$.}{
    \proo{Rigorous Proof}{The ncessity of the proof can be found in \ref{thm:ScaleLocation}, here the sufficiency need }
    \proo{Intuitive Proof}{}
}

\subsection{Data Reduction}

The idea of data reduction is to summarize or reduce the data $X_1,X_2,\cdots,X_n$ to get the information of the unknown parameter $\theta$.\\
There are many sample point $\bold{x}=(x_1,x_2,\cdots,x_n)$, which are realizations (observations) of the random variable $\bold{X}=(X_1,X_2,\cdots,X_n)$.
A \tbf{Statistic} $T(\bold{X})$ is a form of data reduction, or a summary of the data, $T(\bold{x})$ is an observation of $T(\bold{X})$. $\cX$ is
the \tbf{sample space}. $\mathcal{T}=\{t:t=T(\mathbf{x}),\mathrm{~for~}\mathbf{x}\in\mathcal{X}\}$ is the image of $cX$ under $T(\bold{X})$.
$T(\bold{X})$ partition the sapmle sapce $\cX$ into sets $A_t=\{\mathbf{x}:T(\mathbf{x})=t,\mathbf{x}\in\mathcal{X}\}$.
\ex{
    Give a two-dimensional example for random variable, sample point, Statistic, observation of Statistic, sample space, image and $A_t$.
}{
    Assume $\bold{X}=X_1,X_2$, among which $X_1,X_2$ are Bernoulli R.V. with $p=0.5$. $\bold{x}=(0,1)$ is a sample point.\\
    The sample space $\cX$ is $(0,0), (0,1), (1,0), (1,1)$, set $T(\bold{X})$ as the statistic, then the image $\cT$ is $(0,1,2)$.\\
    $A_1(\bold{x})=((1,0),(0,1))$, $A_2(\bold{x})=(1,1)$, $A_0(\bold{x})=(0,0)$.
}

\subsection{Moment Generating Function}
The \tbf{Moment Genarating Function} (mgf) is:
\eq{
    M_X(t)=\mathbb{E}\left(e^{tX}\right).
}
If taking the $n$ order derivatives of the mgf by $t$, and let $t=0$, then:
\eq{
M_X^{(n)}(t)|_{t=0}=\mathbb{E}\left(X^n\right).
}

\subsection{Concentrtion Inequality}
\begin{center}
    \rt{\tit{Averages of independent random variables concentrate around their expectations.}}
\end{center}
\emogood Denote $\overline{X}=\frac{1}{n}\sum_{i=1}^{n}X_i$, where $X_1,\cdots,X_n$ are generated \tit{i.i.d.}, the general form of concentration inequality:
\eq{
    \mathbb{P}(|\overline{X}-\mathbb{E}[\overline{X}]|\geq\varepsilon(n))\leq1-\delta(n)
}
where $\varepsilon(n)$ and $\delta(n)$ converge to $0$ when $n\to\infty$.
\thm{Markov Inequality}{
    For a positive random variable $X$:
    \eq{
        \mathbb{P}(X\geq t)\leq\frac{\mathbb{E}[X]}{t}=O\left(\frac{1}{t}\right).
    }
    \proo{}{
        \eq{
            \mathbb{E}[X]=\int_0^\infty xp(x)dx\geq\int_t^\infty xp(x)dx\geq t\int_t^\infty p(x)dx=t\mathbb{P}(X\geq t).
        }
    }
}
\thm{Chebyshev's inequality}{
    Let $Var(X)=\sigma^2$, then:
    \eq{
        \mathbb{P}\Big(|X-\mathbb{E}[X]|\geq t\sigma\Big)\leq\frac{1}{t^2}=O\left(\frac{1}{t^2}\right).
    }
    \proo{}{
        \begin{aligned}
            \mathbb{P}(|X-\mathbb{E}[X]|\geq t\sigma) & =\mathbb{P}(|X-\mathbb{E}[X]|^{2}\geq t^{2}\sigma^{2})               \\
                                                      & \leq\frac{\mathbb{E}(|X-\mathbb{E}[X]|^2)}{t^2\sigma^2}=\frac1{t^2}.
        \end{aligned}
    }
    \co{}{
        If we consider $\widehat{\mu}_n=\frac{1}{n}\sum_{i=1}^nX_i$ with variance $\frac{\sigma^2}{n}$, applying the Chebyshev's inequality would get:
        \eq{
            \mathbb{P}\left(|\widehat{\mu}_n-\mu|\geq\frac{t\sigma}{\sqrt{n}}\right)\leq\frac{1}{t^2}.
        }
    }
}
\lem{Chernoff Method}{
\emocool This trick is extremely important.\\
Assume the mgf for $X$ is finite for all $|t|\le b,b>0$. Then we can use mgf to produce a tail bound by Markov inequality:
\eq{
\mathbb{P}((X-\mu)\geq u)=\mathbb{P}(\exp(t(X))\geq\exp(t(u+\mu)))\leq\frac{\mathbb{E}[\exp(tX)]}{\exp(t(u+\mu))}
}
Now consider $t$ as a parameter, to achieve the tightest bound, we can tune $t$ that satisfies:
\eq{
    \mathbb{P}((X-\mu)\geq u)\leq\inf_{0\leq t\leq b}\exp(-t(u+\mu))\mathbb{E}[\exp(tX)].
    }
    This is called the \tbf{Chernoff Bound}. The Chernoff bound "plays" nicely with summations, assume $X_i$ are independent:
    \eq{
        M_{X_1+\cdotp\cdotp\cdotp+X_n}(\lambda)=\prod_{i=1}^n M_{X_i}(\lambda),
    }
    this means that when we calculate a Chernoff bound of a sum of i.i.d. variables, we need  need only calculate the moment generating function for \tit{one} of them, assume $n$ zero-mean independent variables:
    \eq{
        \begin{aligned}
            \mathbb{P}\bigg(\sum_{i=1}^nX_i\geq t\bigg) & \leq\frac{\prod_{i=1}^n\mathbb{E}\left[\exp(\lambda X_i)\right]}{e^{\lambda t}} \\
                                                        & =(\mathbb{E}[e^{\lambda X_1}])^ne^{-\lambda t},
        \end{aligned}
    }
}
\thm{Gaussisn Tail Bound}{
    Suppose $X\sim N(\mu,\sigma^2)$, the mgf is $M_X(t)=\mathbb{E}[\exp(tX)]=\exp(t\mu+t^2\sigma^2/2)$, to apply the Chernoff bound we can compute:
    \eq{
        \inf\limits_{t\ge0}\exp(-t(u+\mu))\exp(t\mu+t^2\sigma^2/2)=\inf\limits_{t\ge0}\exp(-tu+t^2\sigma^2/2),
    }
    when $t=\frac{u}{\sigma^2}$, we can obtain:
    \eq{
        \mathbb{P}(X-\mu\geq u)\leq\exp(-u^2/(2\sigma^2)).
    }
    This is just the right side of the distribution, taking the absolute value:
    \eq{
        \mathbb{P}(|X-\mu|\geq u)\leq2\exp(-u^2/(2\sigma^2)).
    }
    Now consider $\overline{X}$, we can obtain:
    \eq{
        \mathbb{P}(|\widehat{\mu}-\mu|\geq t\sigma/\sqrt{n})\leq2\exp(-t^2/2).
    }
    \re{
        Comparing the Gaussian tail bound with the corollary of Chebyshev's inequality, we can find with probability $1-\delta$:
        \tab{
            \item Chebyshev tells that $|\widehat{\mu}-\mu|\leq\frac{\sigma}{\sqrt{n\delta}}$;
            \item Gaussian tells that $|\widehat{\mu}-\mu|\leq\sigma\sqrt{\frac{2\ln(2/\delta)}{n}}$, which decrease faster when $n\to\infty$.
        }
    }
}
\thm{Bounded RVs: Hoeffding's inequality}{
\lem{Hoeffding's Lemma}{
Let $X$ be a random variable bounded in $[a,b]$, let $X^\prime$ to be an independent copy of $X$, then:
\eq{
\mathbb{E}_X[\exp(\lambda(X-\mathbb{E}_X[X]))]=\mathbb{E}_X[\exp(\lambda(X-\mathbb{E}_{X^{\prime}}[X^{\prime}]))]\overset{(i)}{\operatorname*{\leq}}\mathbb{E}_X[\mathbb{E}_{X^{\prime}}\exp(\lambda(X-X^{\prime}))],
}
where $i$ follows Jensen's inequality. Let $S\in\{-1,1\}$ to be a random sign variable, then $S(X-X^\prime)$ has the same distribution as $X-X^\prime$:
\eq{
    \begin{aligned}
        \begin{aligned}\mathbb{E}_{X,X'}[\exp(\lambda(X-X'))]\end{aligned} & =\mathbb{E}_{X,X^\prime,S}[\exp(\lambda S(X-X^\prime))]                                         \\
                                                                           & =\mathbb{E}_{X,X^{\prime}}[\mathbb{E}_S[\exp(\lambda S(X-X^{\prime}))\mid X,X^{\prime}]].\right \\
                                                                           & \le \exp{\frac{\lambda^2(X-X^\prime)^2}{2}}                                                     \\
                                                                           & \le \exp{\frac{\lambda^2(b-a)^2}{2}}
    \end{aligned}
}
}
Now use the Chernoff bound and Hoeffding's lemma to show:
\eq{
    \begin{aligned}
        \begin{aligned}\mathbb{P}\left(\frac{1}{n}\sum_{i=1}^n(X_i-\mathbb{E}[X_i])\geq t\right)\end{aligned} & \begin{aligned}=\mathbb{P}\left(\sum_{i=1}^n(X_i-\mathbb{E}[X_i])\geq nt\right)\end{aligned}                                                                                      \\
                                                                                                              & \leq\mathbb{E}\left[\exp\left(\lambda\sum_{i=1}^n(X_i-\mathbb{E}[X_i])\right)\right]e^{-\lambda nt}                                                                               \\
                                                                                                              & =\left(\prod_{i=1}^n\mathbb{E}[e^{\lambda(X_i-\mathbb{E}[X_i])}]\right)e^{-\lambda nt}{\operatorname*{\leq}}\left(\prod_{i=1}^ne^{\frac{\lambda^2(b-a)^2}8}\right)e^{-\lambda nt}
    \end{aligned}
}
Rewriting this and minimize over $\lambda\ge 0$, we have
\eq{
    \begin{aligned}\mathbb{P}\left(\frac1n\sum_{i=1}^n(X_i-\mathbb{E}[X_i])\geq t\right)\leq\min_{\lambda\geq0}\exp\left(\frac{n\lambda^2(b-a)^2}8-\lambda nt\right)=\exp\left(-\frac{2nt^2}{(b-a)^2}\right)\end{aligned}
}
}

\subsection{Convergence}

\clearpage
\section{Theory of Optimization}

\subsection{NP-Hard}

\defi{Polynomial Time Algorithms}{
    For an input with size $n$, the worst-case running time is $O(n^k)$ for some constant $k$. 
    A problem that is solvable by polynomial-time algorithms is said to be \tbf{tractable, easy, efficient}.
}
\sep{Decision Problem V.S. Optimization Problem}
Decision problems need to decide simply `yes' or `no', optimization needs to decide a specific value. 
Every optimization problem has a decision version that is \rt{no harder than} the optimization problem.
\ex{
    $A_{opt}$: given a graph, find the length of the shortest path.\\
    $A_{dec}$: given a graph, determine whether there is a path $\le k$.
}{
    Using $A_{opt}$ to solve $A_{dec}$: check if optimal value $\le k$.\\
    Using $A_{dec}$ to solve $A_{opt}$: apply binary search on the value range, logarithmic at most.
}

\subsubsection{P and NP}
\lis{
    \item Class P: problems that can be solved in $O(n^k)$;
    \item Class NP: problems that can be verified in $O(n^k)$ ($\mathbb{P}\subseteq\mathbb{NP}$);
    \item Class NP-hard: problems that are “at least as hard as all NP problems”;
    \item Class NP-complete: problems in both NP and NP-hard.
}
\fig{P and NP}{P and NP}{0.6}

\subsubsection{Polynomial-Time Reduction}
A reduction is an algorithm for \tbf{transforming a problem instance into another}.
\fig{Polynomial-Time Reduction}{Polynomial-Time Reduction}{0.5}
\re{
    \tab{
        \item $A\leq_p B$ if A can be reduced to B \tbf{in polynomial time};
        \item Reduction from $A$ to $B$ implies $A$ is no harder than $B$, so if $A$ is hard, then so is $B$;
        \item One way to prove two problems have equal difficulty is to prove $A\leq_p B$ and $B\leq_p A$.
    }
}

\subsection{Convexity and Continuity}

\defi{Local Minimum (Relative Minimum)}{
    A point $x^* \in \Omega$ is said to be a local minimum point of $f$ over $\Omega$ if there is an $\epsilon>0$, such that for any $x\in\Omega$ and
    $|x-x^*|<\epsilon$, we have $f(x)\ge f(x^*)$.
}

\defi{Differentiability}{
    A mapping $F:\mathbb{R}^n\to\mathbb{R}^m$ is said to be \tit{differentiable} at $x$ if there is a
    function: $DF:\mathbb{R}^n\to\mathbb{R}^{m\times n}$ such that:
    \eq{
        \lim_{h\to0}\frac{\|F(x+h)-F(x)-DF(x)\cdot h\|}{\|h\|}=0.
    }
    The matrix $DF(x)\in \RR^{m\times n}$ is the \rt{Jacobian Matrix}.\\
    If $F$ is a $\mathbb{R}^n\to\mathbb{R}$ mapping, then the \tbf{gradient} can be expressed as:
    \eq{
        \nabla f(x)=Df(x)^\top=\begin{pmatrix}\frac\partial{\partial x_1}f(x)\\\vdots\\\frac\partial{\partial x_n}f(x)\end{pmatrix}
    }
    The \tbf{Hessian Matrix} (symmetric matrix) can be expressed by:
    \eq{
        \nabla^2f(x)=H_f(x)=\begin{pmatrix}\frac{\partial f}{\partial x_1\partial x_1}(x) & \frac{\partial f}{\partial x_1\partial x_2}(x) & \cdots                                         & \frac{\partial f}
               {\partial x_1\partial x_n}(x)                                                                                                                                        \\\cdot&\cdot&\cdots&\cdot\\\cdot&\cdot&\cdot&\cdot\\\cdot&\cdot&
               \cdot                                          & \cdot                                                                                                               \\\cdot&\cdot&\cdots&\cdot\\\frac{\partial f}{\partial x_n\partial x_1}(x)&\frac{\partial f}
               {\partial x_n\partial x_2}(x)                  & \cdots                                         & \frac{\partial f}{\partial x_n\partial x_n}(x)\end{pmatrix}
    }
}
\ex{Calculate the gradient of $f:\mathbb{R}^n\to\mathbb{R},\quad f(x)=\frac12x^\top Ax+b^\top x+c$, where $A\in\mathbb{R}^{n\times n}\text{ be symmetric}$.}{
    Use the definition of Differentiability, first calculate $f(x+h)-f(x)$:
    \eq{\begin{align}f(x+h)-f(x) & =\frac12(x+h)^TA(x+h)+b^T(x+h)+c-f(x)                     \\
                         & = (x+h)^TAx+(x+h)^TAh+b^Tx+b^Th-\frac12x^\top Ax+b^\top x \\
                         & = \frac12h^TAx+\frac12x^TAh+\frac12h^TAh+b^Th             \\
                         & = \frac12(h^TAx)^T+\frac12x^TAh+\frac12h^TAh+b^Th         \\\\
                         & = X^TAh+b^Th
        \end{align}
    }
    Thus $\nabla f(x)=Ax+b$.\\
    If $A$ is not symmetric, then $\nabla f(x)=\frac{A^T+A}{2}x+b$.
}

\subsubsection{First and Second Order Conditions}
\re{
    \tbf{FONC (First-Order Necessary Conditions)}\\
    If $x^{\star}$ is a local minimizer of the unconstrained problem, then we must have $\nabla f(x^*)=0.$
}
\ex{We want to use a $n$th-order polynomial function to estimate the original function $g(x)$ after we observe $m$ points: $x_1,x_2,\cdots,x_m$:
\eq{
h(x)=a_nx^n+a_{n-1}x^{n-1}+\ldots+a_0, n<m
}}{
This can be transformed into an unconstrained problem:
\eq{
\min_{\mathbf{a}} f(\mathbf{a})=\sum\limits_{k=1}^m[g(x_k)-(a_nx_k^n+a_{n-1}x_k^{n-1}+\ldots+a_0)]^2
}
We define $q_{ij}=\sum_{k=1}^m(x_k)^{i+j}$, $b_j=\sum_{k=1}^mg(x_k)(x_k)^j$, and $c=\sum_{k=1}^{m}g(x_{k})^{2}$. With some algebra, we can get:
\eq{
    f(\mathbf{a})=\mathbf{a}^T\mathbf{Q}\mathbf{a}-2\mathbf{b}^T\mathbf{a}+c
}
The FONC can be reduced to $Qa=b$.
}
\thm{SONC (Second Order Necessary Condition)}{
If $x^*$ is a local minimizer of $f$, then it holds that:
\lis{
\item $\nabla f(x^*)=0$;
\item $\mathrm{For~all~}d\in\mathbb{R}^n{:}d^\top\nabla^2f(x^*)d\geq0$ ($\nabla^2f(x^*)$ is positive semidefinite);
}
\defi{Saddle Point}{
    A point satisfying FONC is a \tbf{stationary point}, a stationary point with an indefinite Hessian matrix is called \tbf{saddle point}.
}
}
\fig{An example of saddle point}{An example of saddle point}{}
\thm{SOSC (Second Order Sufficient Conditions)}{
\lis{
\item $\nabla f(x^*)=0$;
\item  $\mathrm{For~all~}d\in\mathbb{R}^n{:}d^\top\nabla^2f(x^*)d>0$ ($\nabla^2f(x^*)$ is positive definite);
}
Then $x^*$ is a \tit{strict local minimum} of $f$.
\proo{}{
    By \tit{taylor expansion}, $f(x^*+td)=f(x^*)+\frac12t^2d^\top\nabla^2f(x^*)d+o(t^2)>f(x^*)$.
}
}
\defi{Coercivity}{
    A continuous function $f:\mathbb{R}^n\to\mathbb{R}$ is said to be \tbf{coercive} if:
    \eq{
        \lim_{\|x\|\to\infty}f(x)=+\infty
    }
    What's more, if $f$ is a coercive function, then the level set $L_{\leq\alpha}:=\{x\in\mathbb{R}^n:f(x)\leq\alpha\}$ is \tit{compact} (closed and bounded),
    and has at least one \tit{global minimizer} (This is called \rt{the Weierstrass Theorem}).
}

\subsubsection{Zero Order Conditions}
Consider the constrained problem:
\eq{
    \label{eq:zoc}
    \begin{aligned}
        \min & f(x)                      \\
        s.t. & x \in \Omega, x \in \RR^n
    \end{aligned}
}
where $f$ is convex, $x^*$ is the optimal solution and $f^*$ is the optimal value. Consider the epigraph $\Gamma\subset \RR^{n+1}=\{(r,\mathbf{x}):r\geqslant f(\mathbf{x}),\mathbf{x}\in \RR^n\}$, and construct $B\in\RR^{n+1}$ with cross
section $\Omega$ and extending vertically from $-\infty$ up to $f^*$. $B$ only overlaps $\Gamma$ at the boundary point $(f^*, x^*)$.
\fig{Zero Order Conditions}{Zero Order Conditions}{0.5}
By the separating hyperplane theorem, there exists a vector $(s, \lambda)\in\RR^{n+1}$ satisfy:
\eq{\label{eq:zoc1}
    \begin{aligned}
         & sr+\boldsymbol{\lambda}^T\mathbf{x}\geq c\quad\text{for all x}\in \RR^n\text{ and }r\geq f(\mathbf{x}) \\
         & sr+\boldsymbol{\lambda}^T\mathbf{x}\leq c\quad\text{for all x}\in\Omega\text{ and }r\leq f^*.
    \end{aligned}
}
$s>0$ otherwise equation \ref{eq:zoc1} don't hold, so we can scale the equations by setting $s=1$.
\pro{Zero-order necessary conditions (\tbf{ZONC})}{
    If $x^*$ solves equation \ref{eq:zoc}, then there is a nonzero $\lambda \in \RR^n$ that $x^*$ is a solution to:
    \eq{\label{eq:zoc2}
        \begin{aligned}
            \min & f(x) + \lambda^T x \\
            s.t. & \quad x \in \RR^n
        \end{aligned}
    }
    And
    \eq{\label{eq:zoc3}
        \begin{aligned}
            \max & \lambda^T x        \\
            s.t. & \quad x \in \Omega
        \end{aligned}
    }
    \proo{}{
        The equation \ref{eq:zoc2} and \ref{eq:zoc3} follow from equation \ref{eq:zoc1}, where $c$ is attained at the point $f^*,x^*$ for both inequalities.
    }
    \re{
        The FONC of \ref{eq:zoc2} implies $\nabla f(x^*) = -\lambda$, and the signal of $\lambda$ can give us the hint of the signal of $x^*$ from \ref{eq:zoc3}.
    }
}
\pro{Zero-order sufficiency conditions \tbf{ZOSC}}{
    If there is a $\lambda$ such that $x^*\in\Omega$ solves equation \ref{eq:zoc2} and \ref{eq:zoc1}, then $x^*$ solves euqation \ref{eq:zoc}.
    \proo{}{
        Suppose $x_1$ is any other point in $\Omega$, from equation \ref{eq:zoc2}:
        \eq{
            f(x_1)+\boldsymbol{\lambda}^T x_1\geqslant f(x^*)+\boldsymbol{\lambda}^T x^*.
        }
        This can be written as:
        \eq{
            f(x_1)-f(x^*)\geqslant\boldsymbol{\lambda}^T x^*-\boldsymbol{\lambda}^T x_1.
        }
        By equation \ref{eq:zoc3}, we can get $f(x_1) \ge f(x^*)$
    }
    \re{
        \hl{ZOSC don't require $f$ to be convex}.
    }
}

\subsubsection{Convexity}
\defi{Convex Sets and Functions}{
    \tbf{Convex Sets}\\
    A set $X\subseteq\mathbb{R}^n$ is \tbf{convex} if for any $x,y\in X$, and any $\lambda \in [0,1]$, we have $\lambda x+(1-\lambda)y\in X$.\\
    For example, the half space $H:=\{x\in\mathbb{R}^n:a^\top x\leq b\}$ and the closed ball $B_r(a):=\{x\in\mathbb{R}^n:\|x-a\|\leq r\}$ are both convex sets.\\
    \co{Intersection of Convex Sets}{
        The intersection of convex sets is a convex set. For example, the \tit{Polyhedral Sets}: $\{x\in\mathbb{R}^n:Ax\leq b\}$.
    }
    \tbf{Convex Functions}\\
    A function $f$ is said to be \tit{convex} on a \tit{convex sets} $X$ is for every $x,y\in X$ and any $0\le \lambda \le 1$:
    \eq{
        f(\lambda x+(1-\lambda)y)\leq\lambda f(x)+(1-\lambda)f(y)
    }
    Examples are The Euclidean norm $f(x)=\|x\|=\sqrt{x^\top x}$ and affine-linear functions $f(x)=a^\top x+b$.\\
    If a function $f$ has the property $f(\lambda\mathbf{x}+(1-\lambda)\mathbf{y})<\lambda f(\mathbf{x})+(1-\lambda)f(\mathbf{y})$,
    then it's said to be \tbf{strictly convex}.\\
    \tbf{Strongly Convex} (with parameter $\mu$)\\
    \eq{
        f(\lambda x+(1-\lambda)y)+\frac{\mu\lambda(1-\lambda)}2\|y-x\|^2\leq\lambda f(x)+(1-\lambda)f(y)
    }
    This is equivalent to $f-\frac{\mu}{2}\lVert \cdot \rVert^2$.
    \lem{General Composition}{
        Let $h:X\to \RR$ be \tit{convex} and $g:Y\to \RR$ be \tit{convex} and \tbf{non-decreasing}. Then $f(x)=g(h(x))$ is convex.
    }
}
\sep{Operations that preserve convexity}
\emocool This part is extremely \hl{important}!
\tab{
    \item \tbf{Nonnegative weighted sums};
    \item \tbf{Pointwise maximum};
}
\tbf{Composition with an affine mapping}: Suppose $f:\mathbb{R}^n\to\mathbb{R},A\in\mathbb{R}^{n\times m},$ and $b\in\RR^n$. Define: $g:\RR^m\to\RR$ as
\eq{
    g(x) = f(Ax+b)
}
with $dom(g)=\{x\mid Ax+b\in dom(f)\}$. Then, if $f$ is convex (concave), so is $g$.
\proo{}{
    Let $x_1,x_2\in dom(g)$, $\alpha\in (0,1)$, we have:
    \eq{
        \begin{aligned}
            g(\alpha x_1 + (1-\alpha x_2)) & = f(A(\alpha x_1 + (1-\alpha x_2)) + b)   \\
                                           & = f(\alpha(A x_1+b)+(1-\alpha)(Ax_2+b))   \\
                                           & \le \alpha f(Ax_1+b) + (1-\alpha)(Ax_2+b) \\
                                           & = \alpha g(x_1) + (1-\alpha) g(x_2)
        \end{aligned}
    }
}
\no\tbf{Composition of non-decreasing functions}: Let $h: X\to \RR$ be convex and $g: Y\to\RR$ be convex and \tbf{non-decreasing}.
Suppose that $X \subset \RR^n$ and $Y \subset \RR$ are convex sets with $h(X) \subset Y$. Then, $f=g\circ h:X\to\mathbb{R},f(x)=g(h(x))$ is convex.\\
\tbf{Power of non-negative function}: If $f$ is convex and non-negative and $k\ge 1$, then $f^k$ is convex.
\proo{}{
    Assume $f$ is twice differentiable. Let $g=f^k$: Then:
    \eq{
        \begin{aligned}
             & \nabla g(x)=kf^{k-1}\nabla f(x);                                                          \\
             & \nabla^{2}g(x)=k((k-1)f^{k-2}\nabla f(x)\nabla f^{T}(x)+f^{k-1}\nabla^{2}f(x)) \succeq 0.
        \end{aligned}
    }
}
\no\tbf{Epigraph}: If $f\in \RR^n$ is a convex function, then its epigraph $epi(f)\in \RR^{n+1}$ is a convex set:
\eq{
    \text{epi}(f)=\begin{Bmatrix}(x,w)\mid x\in X,w\in\RR,f(x)\leq w\end{Bmatrix}
}
\fig{Examples of Epigraph}{Examples of Epigraph}{0.5}
\thm{Convexity and Differentiability}{
    $f$ is convex \tbf{if and only if}:
    \eq{
        f(y)-f(x)\geq\nabla f(x)^\top(y-x),\quad\forall\mathrm{~}x,y\in X
    }
    \proo{}{
        First, we prove the "only if" part: suppose $f$ is convex, we have:
        \meq{
            f(\alpha\mathbf{y}+(1-\alpha)\mathbf{x})\leqslant\alpha f(\mathbf{y})+(1-\alpha)f(\mathbf{x});\\
            \frac{f(\mathbf{x}+\alpha(\mathbf{y}-\mathbf{x}))-f(\mathbf{x})}\alpha\leqslant f(\mathbf{y})-f(\mathbf{x}).
        }
        Letting $\alpha\to 0$, we obtain:
        \eq{
            \mathbf{\nabla}f(\mathbf{x})(\mathbf{y}-\mathbf{x})\leqslant f(\mathbf{y})-f(\mathbf{x}).
        }
        Now, we prove the "if" part, assuming for all $\mathbf{x}$:
        \eq{
            \begin{aligned}
                 & f(\mathbf{x}_1)\geqslant f(\mathbf{x})+\nabla f(\mathbf{x})(\mathbf{x}_1-\mathbf{x})  \\
                 & f(\mathbf{x}_2)\geqslant f(\mathbf{x})+\nabla f(\mathbf{x})(\mathbf{x}_2-\mathbf{x}).
            \end{aligned}
        }
        Multiplying the first by $\alpha$ and the second by $1-\alpha$, and adding:
        \eq{
            \alpha f(\mathbf{x}_1)+(1-\alpha)f(\mathbf{x}_2)\geqslant f(\mathbf{x})+\nabla f(\mathbf{x})[\alpha\mathbf{x}_1+(1-\alpha)\mathbf{x}_2-\mathbf{x}].
        }
        Replacing $\mathbf{x}=\alpha\mathbf{x}_{1}+(1-\alpha)\mathbf{x}_{2}$, we obtain:
        \eq{
            \alpha f(\mathbf{x}_1)+(1-\alpha)f(\mathbf{x}_2)\geqslant f(\alpha\mathbf{x}_1+(1-\alpha)\mathbf{x}_2).
        }
    }
    See the figure below.
    \re{
        The 'iff' condition can be replaced to:
        \eq{
            h^\top\nabla^2f(x)h\geq0,\quad\forall\mathrm{~}h\in\mathbb{R}^n,\quad\forall\mathrm{~}x\in X
        }
    }
}
\fig{Sufficient and Necessary Condition for Convexity}{Sufficient and Necessary Condition for Convexity}{0.5}
\thm{Convexity and Optimality}{
    If $f$ is a convex function and $X$ is a convex set, then for the problem $\min f(x)\quad\mathrm{s.t.}\quad x\in X$, we have:
    \tab{
        \item Every local minimizer is also a global minimizer;
        \item If $f$ is strongly convex, then it has at most one global minimizer, which is the stationary point.
    }
}
\lem{Convexity and Level Sets}{
    Let $f$ be a convex function, then for any $\alpha$, the level set $\mathcal{L}_{\leq\alpha}=\{x:f(x)\leq\alpha\}$ is a convex set.\\
    Linear constraints are always convex.
}
\ex{Is the optimization problem:
    \eq{
        \begin{aligned}\mathrm{maximize}_{x,y,z}&&\mathrm{xyz}\\\mathrm{s.t.}&&x+2y+3z\leq3\\&&x,y,z\geq0\end{aligned}
    }
    convex or not?}{
    All constraints are convex since they are linear. The objective $xyz$ is not concave. However,
    we can turn the above problem into an equivalent form:
    \eq{
        \begin{aligned}\mathrm{maximize}_{x,y,z}&&\log xyz\\\mathrm{s.t.}&&x+2y+3z\leq3\\&&x,y,z\geq0\end{aligned}
    }
    This problem is convex since $\log xyz = \log x +\log y+\log z$, which is a concave-preserved operation on concave functions.
}

\subsubsection{Lipschitz Continuity}
\defi{Lipschitz Continuous \& Smooth}{
    The $\nabla f$ is called \rt{Lipschitz continuous} if:
    \eq{
        \|\nabla f(x)-\nabla f(y)\|\leq L\|x-y\|,\quad\forall\mathrm{~}x,y\in\mathbb{R}^n,
    }
    where $L$ is the \rt{Lipschitz constant} and $f$ is called \rt{Lipschitz smooth}.\\
    \ex{Consider $f(x)=\frac12x^\top Ax+b^\top x+c$}{
        \eq{
            \begin{aligned}\|\nabla f(x)-\nabla f(y)\|&=\|(Ax+b)-(Ay+b)\|\\&=\|A(x-y)\|\leq\|A\|\cdot\|x-y\|.\end{aligned}
        }
        So $f(x)$ is Lipschitz Smooth with $L=\|A\|$.
    }
    \lem{Descent Lemma}{
        Let $f$ be Lipschitz Smooth with $L$, then:
        \eq{
            f(y)\leq f(x)+\nabla f(x)^\top(y-x)+\frac L2\|y-x\|^2\quad\forall\left.x,y\in\mathbb{R}^n.\right.
        }
        The descent lemma provides a quadratic upper bound for $f$.
        \proo{}{
            By Taylor's Expansion, the vector $z_t = x+t(y-x)$ can be expressed by $f(y)=f(x)+\int_0^1\langle\nabla f(z_t),y-x\rangle dt$.
            Subtracting $\langle\nabla f(z_t),y-x\rangle$ on each side, we have:
            \eq{
                \begin{aligned}
                    \begin{aligned}f(y)-f(x)-\langle\nabla f(x),y-x\rangle\end{aligned}    & \begin{aligned}=\quad\int_0^1\langle\nabla f(z_t)-\nabla f(x),y-x\rangle dt\end{aligned} \\
                    \begin{aligned}&|f(y)-f(x)-\langle\nabla f(x),y-x\rangle|\end{aligned} & =\quad\left|\int_0^1\langle\nabla f(z_t)-\nabla f(x),y-x\rangle dt\right|                \\
                                                                                           & \leq\quad\int_0^1|\langle\nabla f(z_t)-\nabla f(x),y-x\rangle|dt                         \\
                                                                                           & \leq\quad\int_0^1\|\nabla f(z_t)-\nabla f(x)\|_2\cdot\|y-x\|_2dt                         \\
                                                                                           & \leq\quad L\int_0^1t\|x-y\|_2^2dt                                                        \\
                                                                                           & =\quad\frac L2\|x-y\|_2^2.
                \end{aligned}
            }
        }
    }
}
\thm{Lipschitz Continuity via Hessians}{
    Let $f$ be a twice cont. differentiable function. Then, the following two conditions are equivalent:
    \tab{
        \item $f$ is Lipschitz smooth with constant $L$;
        \item $\|\nabla^2 f(x)\|\le L$ for any $x\in\RR^n$.
    }
}


\clearpage
\section{Linear Programming}

\subsection{Introduction to Linear Programming}

The main contents of this section are notes from \cite{luenberger1984linear}, \cite{bertsimas1997introduction}.
The \tit{standard form} of the linear programming:
\eq{
    \begin{array}{rl}\text{minimize}&c_1x_1+c_2x_2+\ldots+c_nx_n\\\text{subject to}&a_{11}x_1+a_{12}x_2+\ldots+a_{1n}x_n=b_1\\&a_{21}x_1+a_{22}x_2+\ldots+a_{2n}x_n=b_2\\&\vdots\quad\vdots\\&a_{m1}x_1+a_{m2}x_2+\cdots+a_{mn}x_n=b_m\\\text{and}&x_1\geqslant0,x_2\geqslant0,\ldots,x_n\geqslant0,\end{array}
}
or the above equations can be concisely written in:
\eq{
    \begin{array}{ll}\text{minimize}&\mathbf{c}^T\mathbf{x}\\\text{subject to}&\mathbf{A}\mathbf{x}=\mathbf{b}\quad\text{and}\quad\mathbf{x}\geqslant\mathbf{0}.\end{array}
}
\sep{Convertion to standard form LP}
\tbf{Slack Variable}\\
For this kind of LP formation:
\eq{
    \begin{array}{rl}\text{minimize}&c_1x_1+c_2x_2+\ldots+c_nx_n\\\text{subject to}&a_{11}x_1+a_{12}x_2+\ldots+a_{1n}x_n\le b_1\\&a_{21}x_1+a_{22}x_2+\ldots+a_{2n}x_n\le b_2\\&\vdots\quad\vdots\\&a_{m1}x_1+a_{m2}x_2+\cdots+a_{mn}x_n\le b_m\\\text{and}&x_1\geqslant0,x_2\geqslant0,\ldots,x_n\geqslant0,\end{array}
}
The above formulation can be transformed into the following standard form:
\eq{
    \begin{array}{rlr}\text{minimize}&c_1x_1+c_2x_2+\cdots+c_nx_n\\\text{subject to}&a_{11}x_1+a_{12}x_2+\cdots+a_{1n}x_n+y_1=b_1\\&a_{21}x_1+a_{22}x_2+\cdots+a_{2n}x_n+y_2=b_2\\&\vdots\quad\vdots\\&a_{m1}x_1+a_{m2}x_2+\cdots+a_{mn}x_n+y_m=b_m\\\text{and}&x_1\geqslant0,x_2\geqslant0,\ldots,x_n\geqslant0,\\\text{and}&y_1\geqslant0,y_2\geqslant0,\ldots,y_m\geqslant0.\end{array}
}
Now the constraint would be modified from $\tbf{A}$ to $\tbf{[A, I]}$, and the number of unknowns is changed from $n$ to $n+m$.\\
\tbf{Surplus Variable}\\
Similar to the slack variable, formulation like:
\eq{
a_{i1}x_1+a_{i2}x_2+\cdots+a_{in}x_n\geqslant b_i
}
can be transformed into:
\eq{
a_{i1}x_1+a_{i2}x_2+\cdots+a_{in}x_n-y_i=b_i
}\\
\tbf{Free Variable}\\
For some variables without the constraint of the sign, there are two methods to transform it into the standard form. One is
to $x_i=u_i-v_i$, where both $u_i$ and $v_i$ are larger or equal to zero. Another method can be used when the following condition holds:\\
\eq{
    a_{1}x_1+\cdots + a_{i}x_i+\cdots+a_{n}x_n=b_i
}
where $x_i$ is the free variable, thus we can replace $x_i$ by $b_i-\sum_{j\ne i}^n a_jx_j$, which can eliminate one variable and one constraint simultaneously.\\
One important example of the linear programming model is the \tbf{maximal flow problem}:
\fig{A network with capacities}{A network with capacities}{0.3}
This problem can be formulated into:
\eq{
    \begin{aligned}
         & \text{minimize} \text{f}                                               \\
         & \text{subject to} \sum_{j=1}^nx_{1j}-\sum_{j=1}^nx_{j1}-f=0            \\
         & \sum_{j=1}^nx_{ij}-\sum_{j=1}^nx_{ji}\quad=0,               & i\neq1,m \\
         & \sum_{j=1}^nx_{mj}-\sum_{j=1}^nx_{jm}+f=0                              \\
         & 0\leq x_{ij}\leq k_{ij},\quad\quad\text{for all }i,j
    \end{aligned}
}
\sep{Basic Solutions}
The system of linear equalities:
\eq{
    \mathbf{A}\mathbf{x}=\mathbf{b}
}
where $\tbf{A}$ is a $m\times n$ constraint matrix and $\tbf{x}$ is the $n \times 1$ decision variables.
\ass{
    Full Rank Assumption
}{The $m\times n \tbf{A}$ has $m<n$, and the $m$ rows of $\tbf{A}$ are linearly independent}.
This assumption makes the linear equalities always have at least one basic solution.
At least $m$ linearly independent columns $\to \tbf{B}$, then would get:
\eq{
\mathbf{B}\mathbf{x}_{\mathbf{B}}=\mathbf{b}
}
\defi{Basic Feasible Solution}{$\mathbf{x}=(\mathbf{x}_{\mathbf{B}},\mathbf{0})$ is the \tbf{basic solution}, the components of
$\mathbf{x}$ related to the columns of $\mathbf{B}$ are \tbf{basic variables}\\
If the solution also satisfies $\mathbf{x}\ge0$, then it's called a \tbf{basic feasible solution}.\\
If some of the basic variables have value zero, then it's called a \tbf{degenerate basic solution}.\\
Similar to the definition above, we have \tbf{degenerate basic feasible solution}.}
\thm{Fundamental Theorem of Linear Programming}{
    Given a linear program in standard form, where $\tbf{A}$ is an $m\times n$ matrix of rank $m$:
    \lis{
        \item If there is a feasible solution, there is a basic feasible solution;
        \item If there is an optimal feasible solution, there is an optimal basic feasible solution.
    }
    \proo{}{
        (1)\\
        If there is a feasible solution $\mathbb{x}$, the solution satisfy:
        \eq{x_1\mathbf{a}_1+x_2\mathbf{a}_2+\cdots+x_n\mathbf{a}_n=\mathbf{b}}
        Assume there are $p$ of variable $x_i>0$, then the following equation holds:
        \eq{x_1\mathbf{a}_1+x_2\mathbf{a}_2+\cdots+x_p\mathbf{a}_n=\mathbf{b}}
        Then there are two cases:
        \lis{
            \item if $\mathbf{a}_{1},\mathbf{a}_{2},\ldots,\mathbf{a}_{p}$ are linearly independent, then $p\le m$, which means $\mathbf{x}$ is already a basic solution;
            \item otherwise, there would be a non-trivial solution for $y_1\mathbf{a}_1+y_2\mathbf{a}_2+\cdots+y_p\mathbf{a}_p=\mathbf{0}$, which
            means $(x_1-\varepsilon y_1)\mathbf{a}_1+(x_2-\varepsilon y_2)\mathbf{a}_2+\cdots+(x_p-\varepsilon y_p)\mathbf{a}_p=\mathbf{b}$.\\
            Then for any value of $\epsilon$, $\mathbf{x}-\varepsilon\mathbf{y}$ is a solution but may violate the signal constraint.\\
            Mention that there is at least one $y_i$ that is negative or positive, thus there is at least one $x_i$ decreasing when we increase the $\epsilon(\epsilon > 0)$,
            thus we set $\varepsilon=\min\{x_i/y_i:y_i>0\}$, which would bring us to a new feasible solution but the number of zeros is larger.\\
            Through such iteration, we can get at least one basic feasible solution.
        }\\
        (2)\\
        The idea is the same as the first one. In case one it's obvious. in case two, we need to prove for any $\epsilin$, $\mathbf{x}-\varepsilon\mathbf{y}$ is still optimal.\\
        Note the new value is $\mathbf{c}^T\mathbf{x}-\varepsilon\mathbf{c}^T\mathbf{y}$. Because $\varepsilon$ can be both positive and
        negative, thus $\mathbf{c}^T\mathbf{y}=0$, which makes $\mathbf{x}-\varepsilon\mathbf{y}$ still as optimal solution.
    }
    \re{
        This theorem reduce the original problem to the size of $\begin{pmatrix}n\\m\end{pmatrix}=\frac{n!}{m!(n-m)!}$.
    }}
\sep{Relationship with the Convex Optimization}
\defi{Extreme Point}{A point $\tbf{x}$ in a convex set $\cC$ is an \tit{extreme point} if there are \tbf{no} two distinct
    $\tbf{x}_1,\tbf{x}_2\in\cC$ such that $\mathbf{x}=\alpha\mathbf{x}_{1}+(1-\alpha)\mathbf{x}_{2}$ for some $\alpha, 0<\alpha<1$}
\thm{Equivalence of extreme points and basic solutions}{Let \tbf{A} be an $m \times n$ matrix with rank $m$, Let \tbf{K} denote the \tit{convex polytope}
consisting all vector $\tbf{x}$ satisfying $\mathbf A\mathbf x=\mathbf b,\mathbf x\geqslant\mathbf0$.\\
A vector \tbf{x} is an extreme point of \tbf{K} if and only if \tbf{x} is a basic feasible solution.
\proo{}{
(1) BFS $\to$ extreme point:\\
Suppose $\textbf{x}=(x_1,x_2,\ldots,x_m,0,0,\ldots,0)$ is a BFS, it satisfies $x_1\mathbf{a}_1+x_2\mathbf{a}_2+\cdots+x_m\mathbf{a}_m=\mathbf{b}$.
If $\mathbf{x}=\alpha\mathbf{y}+(1-\alpha)\mathbf{z}$, since the value in $\mathbf{y}, \mathbf{z}$ is larger than 0, then we have:
\eq{y_1\mathbf{a}_1+y_2\mathbf{a}_2+\cdots+y_m\mathbf{a}_m=\mathbf{b}\\z_1\mathbf{a}_1+z_2\mathbf{a}_2+\cdots+z_m\mathbf{a}_m=\mathbf{b}}
Because the vectors $\mathbf{a}_1,\mathbf{a}_2,\ldots,\mathbf{a}_m$ are linearly independent, then we can get $\mathbf{x}=\mathbf{y}=\mathbf{z}$,
which means that $\tbf{z}$ is an extreme point.\\
(2) Extreme point $\to$ BFS:\\
Assume that $\tbf{x}$ has $k$ components larger than zero, then we have:
$y_1\mathbf{a}_1+y_2\mathbf{a}_2+\cdots+y_k\mathbf{a}_k=0$. Assume that $\mathbf{a}_1, \mathbf{a}_2, \cdots, \mathbf{a}_k$ are
linearly dependent, which would lead to:
\eq{
    y_1\mathbf{a}_1+y_2\mathbf{a}_2+\cdots+y_k\mathbf{a}_k=0
}
Define $\mathbf{y}=(y_{1},y_{2},\ldots,y_{k},0,0,\ldots,0)$, it's obvious to see the following can exist:
\eq{
    \mathbf x+\epsilon\mathbf y\geqslant0,\quad\mathbf x-\epsilon\mathbf y\geqslant0
}
Contradicts that $\mathbf{x}$ is an extreme point $\to$ $\mathbf{a}_1, \mathbf{a}_2, \cdots, \mathbf{a}_k$ are
linearly independent $\to$ $\mathbf{x}$ is a BFS.
}}
\co{}{\lis{
        \item If the convex set $\tbf{K}$ is nonempty, there is at least one extreme point;
        \item If there is a finite optimal solution to a linear programming problem, there is a finite optimal solution which is an extreme point of the constraint set.;
        \item The constraint set $\tbf{K}$ possesses at most a finite number of extreme points;
        \item If \tit{K} is bounded, then it's a \tit{convex polyhedron}.
    }}

\subsection{Simplex Method}
The standard form linear programming can be transformed to the \tbf{canonial form} (reduced row-echelon form/tabular method
). The canonical form provides basic variables and non-basic variables. \tbf{Pivot equations} can transform a non-basic variable
into a basic variable.
\eq{
    \begin{cases}\bar a_{ij}'=\bar a_{ij}-\frac{\bar a_{iq}}{\bar a_{pq}}\bar a_{pj},i\neq p\\\bar a_{pj}'=\frac{\bar a_{pj}}{\bar a_{pq}}.\end{cases}
}

\clearpage
\section{Unconstrained Optimization}

\subsection{Single Variable Problem}

\tbf{Bisection method} uses the idea that the local minimizer must satisfy the FONC: $f^{\prime}(x)=0.$
\fig{Bisection Methods}{Bisection Method}{0.5}
The bisection method can only help us find an approximate \tit{stationary point}. When $f$ is convex, this is an approximate global minimizer of $f$.\\
The iteration of the bisection method takes at most $\log_2(\frac{x_r-x_\ell}\epsilon)$ steps.\\
\tbf{Golden section method} can optimize those problems whose first derivative is difficult to obtain, but it requires us to know the range of the local optimal solution.\\
\fig{Golden Section Method}{Golden Section Method}{0.5}
If we set $\phi=\frac{3-\sqrt{5}}2$ (comes from $\frac{r}{1}=\frac{1-2r}{1-r}$), then we can save one function evaluation during each iteration.
\fig{Golden Section Graph}{Golden Section Graph}{0.4}
The iteration of the golden section method takes at most $\log_2(\frac{x_r-x_\ell}\epsilon)$ steps. A necessary condition for the minimizer is $g(x)=f^\prime(x)=0$. \\
\tbf{Newton's Method in single dimension} approximates $g$ using a first-order
Taylor expansion for each $x^k$: $g(x)\approx g(x^k)+g^{\prime}(x^k)(x-x^k).$ By setting the right-hand side to 0 and solving:
\meq{
    x^{k+1}=x^k-\frac{g(x^k)}{g^{\prime}(x^k)};\\
    x^{k+1}=x^k-\frac{f^{\prime}(x^k)}{f^{\prime\prime}(x^k)}.
}
where we assume $g^\prime(x^k)\ne0$ at each step. However, Newton's method may not converge for every initial point.
\thm{Quadratic Convergence of Newton's method in 1-D case}{
If $g$ is twice continuous and $x^\star$ is a root of $g$ (global minimizer) and $g^\prime (x^\star)\ne0$. When $|x^0-x^\star|$ is sufficiently small, the sequence
by Newton's method $x^{k+1}=x^k-\frac{g(x^k)}{g^{\prime}(x^k)}$ will satisfy:
\eq{
|x^{k+1}-x^*|\leq C|x^k-x^*|^2.
}
}
Another interpretation of Newton's method: consider a second-order Taylar expansion for $f$:
\eq{
f(x)\approx f(x^k)+f^{\prime}(x^k)(x-x^k)+\frac12f^{\prime\prime}(x^k)(x-x^k)^2.
}
The Newtion's method is a minimizer for the quadratic model. If the original objective function is quadratic, then Newton's method converges in one step.

\subsection{Basic Gradient Descent Theory and Algorithms}

Consider:
\eq{
\mathrm{minimize}_{x\in\mathbb{R}^n}\quad f(x).
}
We typically use iterative steps of the form:
\eq{
    x^{k+1}=x^k+\alpha_kd^k.
}
The $d^k\in \RR^n$ is called a \rt{descent direction} of $f$ if $\nabla f(x)^\top d<0.$ The
\rt{step size} $\alpha_k$ may be chosen in accordance with some line search (\tit{one-dimensioanl}) rules.\\
\fig{Abstract Descent Method}{Abstract Descent Method}{0.5}
A popular stopping criterion is $\|\nabla f(x^k)\|\leq\text{to}1$ with \rt{tolerance} $tol>0$.
\defi{Steepest Descent Directions}{
    Let $f$ be convex and let $x\in \RR^n$ be given with $nabla f(x)\ne 0$. Let $d\in\RR^n$ denote
    the solution of:
    \eq{
        \min_{\|\boldsymbol{d}\|=1}\nabla f(\mathbf{x})^\top\boldsymbol{d}.
    }
    Every vector of the form $s=\lambda d, \lambda>0$, is called \tbf{steepest descent direction}.\\
    By Cauchy-Schwarz inequality: $|\nabla f(x)^{T}d|\leq||\nabla f(x)||\cdot||d||$, which implies
    $\nabla f(x)^{T}d\ge -||\nabla f(x)||\cdot||d||$, thus, we can have $\nabla f(x)^{T}d\ge -||\nabla f(x)||$.
    So $d^\star=-\nabla f(x)/\|\nabla f(x)\|.$\\
    We can choose the descent directions $d=-\lambda \nabla f(x),\lambda>0$.
}
\re{
    \tab{
        \item The steepest descent norm depends on the chosen norm (here it is Euclidean norm);
        \item The gradient $\nabla f(y)$ is \tit{perpendicular} to the level set and it points towards a
        direction where $f$ is decreasing the most.
    }
}
\fig{Gradient Descent Method}{Gradient Descent Method}{0.7}

\subsubsection{Step Size Strategies}
There is one simple step size strategy: \tbf{constant step size}, by which we choose $\alpha_k=\bar{\alpha}$ for all $k$.
\sep{Exact Line Search}
We choose $\alpha_k$ such that:
\eq{
    \alpha_k=\arg\min_{\alpha\geq0}f(x^k+\alpha d^k).
}
Golden section methods can solve $\alpha_k$. In some cases, there are analytical solutions for $\alpha_k$.
\pro{The "zig-zag" path of ELS}{
The directions between two consecutive steps are perpendicular:
\eq{
(d^{k+1})^\top d^k=0,\\
(x^{k+2}-x^{k+1})^T(x^{k+1}-x^k)=0.
}
\proo{}{
    For the $k$th iteration, the $alpha_k$ need to satisfy $\alpha_k=\arg\min_{\alpha\geq0}f(x^k+\alpha d^k)$. Denote $\phi(\alpha)=f(x^k+\alpha d^k)$,
    so $\phi^{\prime}(\alpha)=\nabla f(x^k+\alpha d^k)^T d^k$. When $alpha=0$, $\phi^{\prime}(\alpha)=-\|\nabla f(x^k)\|^2<0$. To minimize the formula, we
    need to set $\phi^{\prime}(\alpha)=0$, then:
    \eq{
        \nabla f(x^k+\alpha^k d^k)^\top d^k=0\\
        (d^{k+1})^\top d^k=0.
    }
}
}
\sep{Backtracking Line Search}
Let $\sigma,\gamma \in (0,1)$ be given, choose $\alpha_k$ as the largest element from $(1,\sigma,\sigma^2,\sigma^3,\cdots)$ such that:
\eq{
    f(x^k+\alpha_kd^k)-f(x^k)\leq\gamma\alpha_k\cdot\nabla f(x^k)^\top d^k.
}
This is called \tbf{\rt{Armijo condition}}.\\
Procedure:
\lis{
    \item Start with $\alpha=1$ or $\alpha=s$;
    \item If $f(x^k+\alpha d^k)\leq f(x^k)+\gamma\alpha\cdot\nabla f(x^k)^\top d^k,$ choose $\alpha_k=\alpha$, else set $\alpha=\sigma\alpha$ and repeat.
}
By Taylor expansion, for small $\alpha$:
\eq{
    f(x^k+\alpha d^k)\approx f(x^k)+\alpha\nabla f(x^k)^\top d^k<f(x^k)+\gamma\alpha\cdot\nabla f(x^k)^\top d^k.
}
\fig{Armijo Rule}{Armijo Rule}{0.4}
\sep{Diminishing Step Sizes}
Choose $\alpha_k\to0$ and $\sum_{k=0}^{\infty}\alpha_k=\infty$, this is frequently used in stochastic programming.

\subsubsection{The Convergence Rate}
\defi{Accumulation Point}{
    A point $x$ is an \tbf{accumulation point} of $\{x^k\}_k$ if for every $\epsilon>0$, there are infinitely many numbers $k$ with $x^k\in B_\varepsilon(x)$.
    \re{
        \tab{
            \item If $x$ is an \tbf{accumulation point} of $\{x^k\}_k$, then there is a subsequence of $\{x^k\}_k$ that converges to $x$;
            \item If $\{x^k\}_k$ converges to some $x\in\RR^n$, then $x$ is the unique accumulation point of $\{x^k\}_k$;
            \item A bounded sequence has at least one accumulation point.
        }
    }
}
\defi{Global Convergence}{
    The gradient method can find the stationary point \tbf{independent} of the chosen initial point.
    \thm{Global Convergence}{
        Let $f:\RR^n\to\RR$ be continuously differentiable, $\{x^k\}_k$ be the sequence generated by gradient method for:
        \eq{
            \min_xf(x)\quad\mathrm{s.t.}\quad x\in\mathbb{R}^n,
        }
        with \tit{exact line search} or \tit{Armijo rule} step size strategies, $\{f(x^k)\}_k$ has the properties:
        \tab{
            \item it's nonincreasing;
            \item every accumulation point of $\{f(x^k)\}_k$ is a stationary point of $f$.
        }
        \re{
            \tab{
                \item It does not guarantee the existence of an accumulation point or convergence to a single limit point;
                \item If $f$ is \tit{coercive} (bounded), then $\{f(x^k)\}_k$ must have an accumulation point.
            }
        }
    }
}
Rigorously speaking, we define the algorithm $A$ as a point-to-point mapping:
\eq{
    x_{k+1} = A(x_k),
}
or a point-to-set mapping:
\eq{
    x_{k+1} \in A(x_k).
}
We further define the continuous descent function $Z$ on $X$ given the solution set $\Gamma \subset X$ if $Z$ satisfy:
\lis{
    \item if $x \notin \Gamma$ and $y \subset A(x)$, then $Z(y) < Z(x)$;
    \item if $x \in \Gamma$ and $y \subset A(x)$, then $Z(y) \le Z(x)$.
}
\defi{Closed Mapping}{
    A point-to-set mapping $A$ from $X$ to $Y$ is said to be \tbf{closed} at $x\in X$ if $x_k \to x,x_k \in X$ and $y_k\to y, y_k\in A(x_k)$
    imply that $y\in A(x)$.
}
\fig{Examples of Closed and Unclosed Mapping}{Examples of Closed and Unclosed Mapping}{0.5}
We can further define the \tit{composite mapping}. Let $A:X\to Y$ and $B:Y\to Z$ be point-to-set mappings, the composite mapping $C+BA$
is defined as $C:X\to Z$ with:
\eq{
    \mathbf{C}(\mathbf{x})=\underset{\mathbf{y}\in\mathbf{A}(\mathbf{x})}{\operatorname*{\cup}}\mathbf{B}(\mathbf{y}).
}
\fig{Composition of mappings}{Composition of mappings}{0.6}
\pro{Closedness of Composite Mapping}{
    Let $\mathbf{A}:X\to Y$ and $\mathbf{B}:Y\to Z$ be point-to-set mappings. If $\mathbf{A}$ is closed at $x$, $\mathbf{B}$ is closed on $A(x)$. If there exists a $y$ and a subsequence $\{y_{ki}\}$,
    and $\{y_{ki}\}\to y$, then the composite mapping $C = BA$ is closed at point $x$.
    \proo{}{
        Let $x_k\to x$ and $z_k\to z, (z_k \in C(x_k))$, according to the definition, if we can prove $z\in C(x)$, the proposition holds. According to the assumption,
        we can find a $y$ such that $x_k\to x, \{y\in A(x_k)\}\to y$. Since $A$ is closed at point $x$, we can get $y\in A(x)$, and $B$ is closed at $y$.\\
        When $x_k\to x$, $z_k \in C(x_k) (B(A(x_k))) \to z$, we can show that $z\in B(y)$. Thus, $z\in B(A(x)), z\in C(x)$.
    }
    \co{}{
        \tab{
            \item Let $A:X\to Y$ and $B:Y\to Z$ be point-to-set mappings. If $A$ is closed at $x$, $B$ is closed on $A(x)$ and $Y$ is compact, then $C=BA$ is closed at $x$;
            \item Let $A:X\to Y$ be a point-to-point mapping and $B:Y\to Z$ a point-to-set mapping. If $A$ is continuous at $x$ and $B$ is closed on $A(x)$, then $C=BA$ is closed at $x$.
        }
    }
}
\thm{(Rigorous) Global Convergence Theorem}{
    Let $A$ be an algorithm on $X$, given $x_0$, the sequence $\{x_k\}$ is generated by $x_{k+1} \in A(x_k)$. Let a solution $\Gamma \subset X$ be given, and suppose:
    \tab{
        \item all points $x_k$ are contained in a compact set $S\subset X$;
        \item $A$ is the descent function for $A$ on $X$;
        \item the mapping $A$ is closed outside $\Gamma$.
    }
    Then the \tbf{limit of any convergent subsequence} pf $\{x_k\}$ is a solution.
}
\thm{Convergence under Lipschitz Continuity}{
    Let $f:\RR^n\to\RR$ be Lipschitz Smooth, $\{x^k\}_k$ be the sequence generated by gradient method for:
    \eq{
        \min_xf(x)\quad\mathrm{s.t.}\quad x\in\mathbb{R}^n,
    }
    with \tit{constant step size} $\bar{\alpha}\in(0,\frac{2}{L})$, \tit{exact line search}, \tit{diminishing} step size
    or \tit{Armijo rule} step size strategies, $\{f(x^k)\}_k$ either:
    \tab{
        \item converge to $-\infty$;
        \item or converge to a finite value that $\|\nabla f(x^k)\|\to0.$
    }}
\co{Complexity Bounds}{
Complexity bounds characterize the behavior and progress of an algorithm during the very first iterations.\\
If $f$ is Lipschitz smooth with $L$ and lower bounded by $\bar{f}$, then it holds:
\eq{
    \min_{i=0,\cdots,T-1}\|\nabla f(x^{i})\|\leq\frac{f(x^{o})-\overline{f}}{\sqrt{\beta T}}=\mathcal{O}(1/\sqrt{T})\quad\forall T.
}
here $\{x^k\}$ are generated by constant step size and $\bar{\beta}=\bar{\alpha}(1-\frac{L\bar{\alpha}}{2})$.
\re{
    \tab{
        \item We need to perform the gradient method for $T>\frac{1}{tol^2}$ iterations to ensure $\min_{i=0,\cdots,T-1}\|\nabla f(x^{i})\|\le tol$;
        \item If $f$ is convex and the problem has a solution, then the whole sequence converges to a global minimum;
        \item Convergence can be ensured if the stationary points are \gt{isolated};
        \item This works for backtracking, constant \& diminishing step sizes.
    }
}
\proo{}{
Recall the descent lemma:
\eq{
    f(y)\leq f(x)+\nabla f(x)^\top(y-x)+\frac L2\|y-x\|^2\quad\forall\left.x,y\in\mathbb{R}^n.\right.
}
Set $y=x^{k+1}$ and $x=x^k$, so $x^{k+1}-x^k=-\bar{\alpha}\nabla f(x^k)$, reducing the descent lemma to:
\eq{
    f(x^{k+1})\le f(x^k)-(1-\frac{L\bar{\alpha}}{2}\bar{\alpha}\cdot \|\nabla f(x^k)\|^2).
}
Denote $\bar{beta}=\bar{\alpha}(1-\frac{L\bar{\alpha}}{2})$, we have $\bar{\beta}\|\nabla f(x^k)\|^2\le f(x^{k})-f(x^{k+1})$. Then sum this for $k=0,\cdots,T-1$:
\eq{
    \bar{\beta}\cdot \sum_{k=0}^{T-1}\|\nabla f(x^k)\|^2 \le f(x^0)-f(x^T)\le f(x^0)-\bar{f}.
}
For the left part:
\eq{
    \bar{\beta}\cdot \sum_{k=0}^{T-1}\|\nabla f(x^k)\|^2 \ge \bar{\beta}\cdot \sum_{k=0}^{T-1}\min_{i=0,\cdots,T-1}\|\nabla f(x^k)\|^2=T\cdot \min_{i=0,\cdots,T-1}\|\nabla f(x^k)\|^2.
}
Finally, we have:
\eq{
    \min_{i\to0,\ldots,T\to1}\|\nabla f(x^{i})\|\leq\frac{f(x^{0})-\overline{f}}{\sqrt{\beta T}}=\mathcal{O}(1/\sqrt{T})\quad\forall T.
}
}
}
\defi{Order of Convergence}{
Let $\{r_k\to r^*\}$, the order is the supremum of the $p\ge 0$ satisfying:
\eq{
0\leqslant\underset{k\to\infty}{\operatorname*{\overline{lim}}}\frac{|r_{k+1}-r^*|}{|r_k-r^*|^p}<\infty.
}
In most of the cases, we can use a simpler definition:
\eq{
\beta=\lim_{k\to\infty}\frac{|r_{k+1}-r^*|}{|r_k-r^*|^p}
}
}
\defi{Linear Convergence}{
$\{x^k\}_k$ converges linear with rate $\eta\in(0,1)$ to $x^\star \in \RR^n$ if there is $\cL > 0$ such that:
\eq{
\|x^{k+1}-x^\star\|\le \eta\cdot\|x^k-x^\star\|, \forall k\ge \cL
}
\re{
\tab{
    \item linear convergence with $\cL=0$ implies that $\|x^k-x^\star\| \le \eta^k \|x^0-x^\star\|$.
}
The sequence with $r_k=a^k$ where $0<a<1$ converges to zero with linear convergence, since $r_{k+1}/r_k=a$.\\
The sequence with $r_k=a^{2^k}$ where $0<a<1$ converges to zero with quadratic convergence, since $r_{k+1}/r_k^2=1$.\\
}
\thm{Rates for Strongly Convex Problems}{
Let $f$ be Lipschitz smooth with $L$ and suppose there is $\mu>0$ such that:
\eq{
    \mu\|\boldsymbol{h}\|^2\le\boldsymbol{h}^\top\nabla^2f(\boldsymbol{x})\boldsymbol{h}(\le\|\boldsymbol{h}\|^2)\forall\boldsymbol{h},\forall\boldsymbol{x}.
}
Then $\{x^k\}_k$ converges linearly to $x^\star$ if it's generated by the gradient method. Further, for $\eta=1-2M_{\mu}$, it follows:
\eq{
    f(x^k)-f(x^*)\leq\eta^k\cdot[f(x^0)-f(x^*)].
}
, where
\eq{
    M=\begin{cases}\tilde\alpha(1-\frac{L\bar\alpha}2)&\text{constant step size:}\bar\alpha\in(0,\frac2L),\\\frac1{2L}&\text{exact line search},\\\gamma\min\{1,\frac{2\sigma(1-\gamma)}L\}&\text{Armijo line search}.\end{cases}
}
The optimal rate can be $\eta=1-\frac\mu L=1-\frac1\kappa, \kappa=\frac{L}{\mu}$. The larger $\kappa$, the lower the convergence rate.
}
}

\subsection{Conjugate Direction Methods}
The \rt{conjugate gradient method} (CG-method)  is an iterative approach for solving quadratic optimization and linear equations $Ax=b$, where $A$ is symmetric and PD.
This problem can be treated as a minimization problem:
eq{
        \min_{x\in\mathbb{R}^n}\phi(x)=\frac12x^\top Ax-b^\top x.
    }
These two problems are equivalent because of $\nabla \pho(x)=Ax-b$. We are interested in the convergence of the residuals:
\eq{
    r^k:=Ax^k-b=\nabla\phi(x^k)\quad\text{at the k-th iter.}
}
\defi{Q-conjugate Directions}{
    Let $Q$ be a symmetric matrix. $\{d_1,d_2,\ldots,d_k\}$ vectors $d_i\in \RR^n, \d_i\ne0$ are Q-conjugate w.r.t. $Q$ if:
    \eq{
        d_i^TQd_j=0\text{,}\forall i\neq j
    }
}
\lem{Linear Independence for Q-conjugate Directions}{
    If $Q$ is PD, then directions $\{d_1,d_2,\ldots,d_k\}$ are linearly independent.
    \proo{}{
        Proof by contradiction. If $\begin{aligned}d_k=\alpha_1d_1+\ldots+\alpha_{k-1}d_{k-1}\end{aligned}$, then:
        \eq{
            \begin{aligned}
                0<d_k^TQd_k & \begin{aligned}=d_k^TQ(\alpha_1d_1+\ldots+\alpha_{k-1}d_{k-1})\end{aligned} \\
                            & =\alpha_{1}d_{k}^{T}Qd_{1}+\ldots+\alpha_{k-1}d_{k}^{T}Qd_{k-1}=0
            \end{aligned}
        }
    }
}
Now, reconsider the quadratic minimization problem, let $x^*$ denote the solution. Since the linear Independence lemma:
\eq{
x^*=\alpha_0d_0+\ldots+\alpha_{n-1}d_{n-1}
}
Therefore,
\meq{
d_{i}^{T}Qx^{*}=d_{i}^{T}Q(\alpha_{0}d_{0}+\ldots+\alpha_{n-1}d_{n-1})=\alpha_{i}d_{i}^{T}Qd_{i}\\
\alpha_i=\frac{d_i^TQx^*}{d_i^TQd_i}=\frac{d_i^Tb}{d_i^TQd_i}
}
Which can lead to the solution of $x^*$ without the matrix inversion:
\eq{
    x^*=\sum\limits_{i=0}^{n-1}\alpha_id_i=\sum\limits_{i=0}^{n-1}\frac{d_i^Tb}{d_i^TQd_i}d_i
}
\thm{Conjugate Direction}{
Let $\{d_0,d_1,\ldots,d_{n-1}\}$ be Q-conjugate, $x_0$ be an arbitrary starting point. By updating:
\meq{
    x_{k+1}=x_k+\alpha_kd_k\\
    g_k=Qx_k-b\\
    \alpha_k=-\frac{g_k^Td_k}{d_k^TQd_k}=-\frac{(Qx_k-b)^Td_k}{d_k^TQd_k}
}
After $n$ steps, $x_n=x^*$.
\proo{}{
To prove the theorem above, we only need to prove why we have the form $\alpha_k=-\frac{(Qx_k-b)^Td_k}{d_k^TQd_k}$.\\
We have:
\eq{
    \begin{aligned}
        d_k^TQ(x^*-x_0) & =d_k^TQ(\alpha_0d_0+\ldots+\alpha_{n-1}d_{n-1})=\alpha_kd_k^TQd_k \\
                        & \Rightarrow\alpha_k=\frac{d_k^TQ(x^*-x_0)}{d_k^TQd_k}             \\
        d_k^TQ(x_k-x_0) & =d_k^TQ(\alpha_0d_0+\alpha_1d_1+\ldots+\alpha_{k-1}d_{k-1})=0     \\
        d_k^TQ(x^*-x_0) & =d_k^TQ(x^*-x_k+x_k-x_0)=d_k^TQ(x^*-x_k)
    \end{aligned}
}
So,
\eq{
\alpha_{k}=\frac{d_{k}^{T}Q(x^{*}-x_{0})}{d_{k}^{T}Qd_{k}}=\frac{d_{k}^{T}Q(x^{*}-x_{k})}{d_{k}^{T}Qd_{k}}=-\frac{d_{k}^{T}g_{k}}{d_{k}^{T}Qd_{k}}
}
}
}
The CG-method is a \tit{special}  conjugate direction method that iteratively generates the conjugate vectors. $d^k$ is chosen as a linear
combination of the previous $d^{k-1}$ and the gradient $g(d^k)$:
\eq{
    p^k=-g^k_\beta_kp^{k-1}
}
To satisfy $(d^{k-1})^{T}Qd^k=0$, we obtain:
\eq{
\beta_k=\frac{(g^k)^{T}Qd^{k-1}}{(d^{k-1})^TQd^{k-1}}
}
\fig{The CG Method}{The CG Method}{0.7}
The CG-method generates a set of conjugate directions and stops after at most $n$ steps returning a solution of the linear system.

\subsection{Newton's Method}
To solve $\min_{x\in\mathbb{R}^n}f(x)\mathrm{~with~}f:\mathbb{R}^n\to\mathbb{R}$ using Newton's method, we approximate the function by its second-order Taylor
expansion at $x^k$:
\eq{
    f(x)\approx f(x^k)+\nabla f(x^k)^\top(x-x^k)+\frac12(x-x^k)^\top\nabla^2f(x^k)(x-x^k).
}
Minimizing this leads to:
\eq{
    x=x^k-(\nabla^2f(x^k))^{-1}\nabla f(x^k).
}
where $d^k = -(\nabla^2f(x^k))^{-1}\nabla f(x^k).$ is called the Newton direction. It's easy to find that $\nabla f(x)^\top\boldsymbol{d}=-\nabla f(x)^\top(\nabla^2f(x))^{-1}\nabla f(x)$,
so whether $\nabla f(x)^T d\le0$ depends on whether $\nabla^2 f(x)$ is positive semi-definite.
\fig{The Newton Method}{The Newton Method}{0.7}
\thm{Quadratic Convergence of Newton's Method}{
Let $f$ be twice continuously differentiable and let $x^\star$ be a local minimizer of $f$. For some $\epsilon>0$, if in $B_{\epsilon}(x^\star)$:
\tab{
    \item $f$ is strongly convex with $\mu$;
    \item $f$ is Lipschitz smooth with $L$.
}
Then $\{x^k\}_k$ follows:
\eq{
\|x^{k+1}-x^*\|\leq\frac L{2\mu}\|x^k-x^*\|^2.
}
In addition, if $\|x^0-x^*\|\leq\frac{\mu\min\{1,\varepsilon\}}L$:
\eq{
    \|x^k-x^*\|\leq\frac{2\mu}L\left(\frac12\right)^{2^k},\quad k=0,1,2,\ldots
}
}
\re{
    Comparison between gradient descent method and Newton's method:
    \tab{
        \item Newton takes fewer steps of iterations, but each iteration is more expensive;
        \item Newton requires more memory to store the Hessian matrix
    }
}

\subsubsection{Inexact Newton Steps}
The idea of \tit{inexact Newton methods} is to solve the linear system:
\eq{
    \nabla^2f(x^k)\cdot d^k=-\nabla f(x^k)
}
only approximately to get a cheaper Newton step. In practice, the strategy aims to find an approximate direction $d^k$ such that:
\eq{
    \|\nabla^2f(x^k)d^k+\nabla f(x^k)\|\leq\text{to}1,\quad\mathrm{to}1\geq0.
}
When $tol=0$, then $\nabla^2f(x^k)\cdot d^k=-\nabla f(x^k)$, which is the original Newton step.
\proo{Inexact Newton Methods Can Work}{
\eq{
    \begin{aligned}
        \|x^{k+1}-x^*\| & = \| x^k+d^k-x^{*}\|                                                                 \\
                        & = \| \nabla^2f(x^k)^{-1}\cdot[\nabla^2f(x^k)(x^k-x^{*})+\nabla^2f(x^k)d^k]\|         \\
                        & \le \|\nabla^2f(x^k)^{-1}\|\cdot \|[\nabla^2f(x^k)(x^k-x^{*})]+[\nabla^2f(x^k)d^k]\| \\
                        & = C_H\cdot (tol+o(\|x^k-x^*\|))
    \end{aligned}
}
If $tol_k$ satisfy $tol_k=o(\|x^k-x^*\|)$, then we can ensure $\|x^{k+1}-x^*\|=o(\|x^k-x^*\|)$. One popular example is to set $tol_k=\rho_k\cdot \|\nabla f(x^k)\|$, and $\rho_k\to 0$.
}
When $\nabla^2f(x^k)$ is singular, we can regularize the optimization problem to:
\eq{
\|[\nabla^2f(x^k)+\delta_kI]d^k+\nabla f(x^k)\|\leq\text{to}1_k, \delta_k\ge 0.
}
To implement the inexact Newton method, we can combine it with the CG-method:
\fig{The Full Newton-CG Method}{The Full Newton-CG Method}{0.8}

\subsubsection{Quasi-Newton Methods}
The principle idea of quasi-Newton methods is similar to inexact Newton-type methods. Computing the Hessian is expensive, We consider an approximation
to the Taylor expansion:
\eq{
    f(x^k+d)\approx q_k(d)=f(x^k)+\nabla f(x^k)^\top d+\frac12d^\top B_kd.
}
If $B_k$ is PD, then we can yield the step $x^{k+1}=x^k+d=x^k-B_k^{-1}\nabla f(x^k)$. The idea of the quasi-Newton methods is that the gradient of $q_{k+1}$ matches
$\nabla f$ at point $x^k$ and $x^{k+1}$. This is called the \bt{\tbf{Secant Condition}}:
\eq{
\nabla f(x^{k+1})-\nabla f(x^k)=B_{k+1}(x^{k+1}-x^k).
}
Quasi-Newton methods generate a sequence of $B_k$ based on this rule. However, the $B_k$ generated by the secant condition may not be PSD, which may point to an ascent direction.
\sep{SR-1 Update}
Let's denote $s^k=x^{k+1}-x^k$, and $y^k=\nabla f(x^{k+1})-\nabla f(x^k)$, the \tbf{symmetric rank-1} updating rule uses a simple idea to update $B_k$ by a \tit{rank-1 update}:
\eq{
    B_{k+1}=B_k+\rho\boldsymbol{u}\boldsymbol{u}^\top,\quad\rho\in\{+1,-1\},\quad\boldsymbol{u}\in\mathbb{R}^n.
}
Here, $\boldsymbol{u}\boldsymbol{u}^\top$ is a rank-1 matrix. Incorporating it into the Secant condition, we can get the formal SR-1 updating rule:
\eq{
    B_{k+1}^{SR-1}=B_k+\frac{(y^k-B_ks^k)(y^k-B_ks^k)^\top}{(s^k)^\top(y^k-B_ks^k)}.
}
\lem{Sherman-Morrison-Woodbury Formula}{
Suppose $A\in\RR^{n\times n}$ is invertible, and let $u,v\in\RR^n$ be given, then $A+uv^T$ is invertible if and only if $1+v^\top A^{-1}\boldsymbol{u}\neq0$, or say:
\eq{
(A+uv^\top)^{-1}=A^{-1}-\frac{A^{-1}uv^\top A^{-1}}{1+v^\top A^{-1}u}
}
}
Using the SMW formula, we can calculate the inverse of the SR-1 update explicitly. (Because to implement the Newton method using the approximation $B_k$, we need the inverse of $B_k$ to update every step).
\eq{
    H_{k+1}^{\mathrm{SR-1}}=H_k+\frac{(s^k-H_ky^k)(s^k-H_ky^k)^\top}{(s^k-H_ky^k)^\top y^k}.
}
\re{The SR-1 update doesn't maintain the PD of $H_k$, which means $d^k=-H_k\nabla f(x^k)$ is not necessarily a descent direction.}
\sep{SR-2: BFGS}
This is the most successful quasi-Newton method, updated by:
\eq{
    B_{k+1}=B_k+\gamma\text{ии}^\top+\delta vv^\top.
}
We use the \tbf{Broyden-Fletcher-Goldfarb-Shanno-update} (BFGS) to update $H_k$:
\eq{
    H_{k+1}^{\mathrm{BFGS}}=H_k+\frac{w^k(s^k)^\top+s^k(w^k)^\top}{(s^k)^\top y^k}-\frac{(w^k)^\top y^k}{((s^k)^\top y^k)^2}\cdot s^k(s^k)^\top,
}
where $w^k=s^k-H_ky^k$。
\fig{Globalized BFGS-Method}{Globalized BFGS-Method}{0.8}
\re{
    \tab{
        \item If $B_k$ is PD, then $B_{k+1}^{BFGS}$ is also PD;
        \item To compute $d^k$, the Hessian is not required, and no linear system needs to be solved;
        \item Only matrix-vector multiplication is required, which is much faster than solving a linear system;
        \item If $f$ is strongly convex, then fast local convergence can be shown.
    }
}

\subsection{Large-Scale Optimization}

\subsubsection{Inpainting}
\sep{TV-Huber problem}
Assume we have access to a binary mask matrix $\in \RR^{m\times n}$ with:
\eq{
    \text{Mask}_{ij}=\begin{cases}1&\text{the pixel }(i,j)\text{is not damaged},\\0&\text{the pixel }(i,j)\text{is damaged}.&\end{cases}
}
Then define $\text{mask}:=vec(\text{Mask}_{ij})\in\RR^{mn}, s:=\sum_{i=1}^{mn}\text{mask}_i$ as the number of undamaged pixels,
and $\mathcal{I}:=\{i:\mathrm{mask}_i=1\}=\{q_1,q_2,\cdots,q_s\}$. Then we can define the \gt{slection matrix}:
\eq{
    $A=\begin{bmatrix}-&e_{q_1}^\top&-\\&\vdots&\\-&e_{q_s}^\top&-\end{bmatrix}\in\mathbb{R}^{s\times mn},\quad[\mathbf{e}_j]_i=\begin{cases}1&\mathrm{~if~}i=j,\\0&\mathrm{~otherwise},&\end{cases}\quad\forall j\in\mathcal{I}.$
}
Let $U\in \RR^{m\times n}$ be the \tit{original undamaged image}, and $u=vec(U)\in\RR^{mn}$. Then we can formalize $b=Au\in\RR^s$ as
the information of all undamaged pixels in $U$. Just solving $Ax=b$ can produce infinite solutions. Instead, we inpaint via \tbf{total variation minimization}:
\eq{
\min_x\frac12\|Ax-b\|^2+\mu\sum_{i=1}^{m-1}\sum_{j=1}^{n-1}\|D_{(i,j)}\boldsymbol{x}\|_2,
}
where $\mu$ is a regularization parameter and $D_{(i,j)}\in\mathbb{R}^{2\times mn}$ is the \tit{image gradient} at pixel ($i,j$).
$D_{(i,j)}x=(\delta_1,\delta_2)^\top $, where
\eq{
\delta_1=X_{(i+1)j}-X_{ij}\quad\mathrm{and}\quad\delta_2=X_{i(j+1)}-X_{ij}.
}
However, the L2-norm $\|\cdot\|_2$ is not smooth at $0$, hence, we use \tbf{Huber-norm} to smooth the objective function:
\meq{
    \varphi_{\mathrm{hub}}(\mathbf{y})=\begin{cases}\frac1{2\delta}\|\mathbf{y}\|^2&\mathrm{if~}\|\mathbf{y}\|\leq\delta,\\\|\mathbf{y}\|-\frac12\delta&\mathrm{if~}\|\mathbf{y}\|>\delta,&\end{cases}\delta>0;\\
    \min_x\frac12\|Ax-b\|^2+\mu\sum_{i=1}^{m-1}\sum_{j=1}^{n-1}\varphi_{\mathrm{hub}}(D_{(i,j)}x).
}
This is called \tbf{\rt{TV-Huber problem}}.
\tab{
    \item TV-Huber problem is convex;
    \item Newton's method can not be applied to this problem (Huber norm is not twice continuously differentiable).
}
\sep{Reconstructing Sparse Signals}
Given $(A,b)$ and an appropriate basis $\Psi$, which can transform $\Psi x$ into a sparse matrix. To recover the original image $x$,
we can model it to a $\cL_0$ problem:
\eq{
    \min_x\quad\|\Psi x\|_0\quad\mathrm{s.t.}\quad Ax=b
}
This is a combinatorial problem, we can transform it into a convex problem:
\eq{
    \min_x\quad\|\Psi x\|_1\quad\mathrm{s.t.}\quad Ax=b
}
But it's still undifferentiable. We can use \tit{compressed sensing} to transform it into:
\eq{
    \min_x\quad\frac12\|A\Psi^{-1}x-b\|_2^2+\mu\cdot\varphi(x),\quad\mu>0.
}
Where $\varphi$ is a smooth and non-convex alternative to the $\cL_1$ norm:
\eq{
    \varphi(x)=\sum_{i=1}^{mn}\log\left(1+\frac{x_i^2}\nu\right),\quad\nu>0.
}

\subsection{Optimization Acceleration}

\subsubsection{Nosterov's Extrapolation: AGM}
The principle idea of many acceleration techniques is to perform an \rt{extrapolation step}:
\eq{
    y^{k+1}=x^k+\beta_k(x^k-x^{k-1}),\quad\beta_k>0
}
to approximate the next iteration $y^{k+1}\approx x^{k+1}.$
\fig{AGM}{Abstract Accelerated Gradient Method}{0.7}
To see a graphical illustration:
\fig{AGM illustration}{AGM illustration}{0.6}
The challenge here is to decide the parameter $\beta_k$ to guarantee faster convergence.
\re{
    A good selection of $\beta_k$ should satisfy:
    \lis{
        \item $\beta_k=\frac{\theta_k(1-\theta_{k-1})}{\theta_{k-1}}=\frac{\theta_k}{\theta_{k-1}}-\theta_k$;
        \item $\theta_{-1}=\theta_0=1$;
        \item $0\leq\theta_k\leq\frac2{k+2}$;
        \item $\frac{1-\theta_{k+1}}{\theta_{k+1}^2}\leq\frac1{\theta_k^2}$.
    }
    A popular choice is $\beta_k=\frac{k-2}{k+1}$.
}
\thm{Improved Complexity of the Acceleration}{
    Let $f$ be convex and Lipschitz smooth with $L$, and assume the solution to $\min_x f(x)$ is non-empty. Let $\{x^k\}_k,\{\alpha_k\}_k,\{\beta_k\}_k$ be
    generated by the accelerated gradient method with $\alpha_k=\bar{\alpha}\in(0,\frac1L]$, and $\beta_k$ satisfies the condition listed above. Then we have:
    \eq{
        $f(x^k)-f(x^*)\leq\frac{2\|x^0-x^*\|^2}{\bar{\alpha}(k+1)^2}\quad\forall\mathrm{~}k\in\mathbb{N}.$
    }
    \re{
        Only $\mathcal{O}(\varepsilon^{-\frac12})$ steps s are required to ensure $f(x^k)-f(x^*)\leq\varepsilon $!
    }
}
Sometimes the  Lipschitz constant $L$ is unknown, then we can determine $\apha_k$ through:
\tab{
\item Choose $\sigma\in(0,1)$;
\item Set $\alpha_k=\alpha_{k-1}$ and $x^{k+1}=y^{k+1}-\alpha_k\nabla f(y^{k+1})$;
\item While $f(x^{k+1})>f(y^{k+1})-\frac{\alpha_k}2\|\nabla f(y^{k+1})\|^2$: set $\alpha_k=\sigma\alpha_k$ and repeat.
}
These steps work because of the descent lemma. If $f$ is Lipschitz smooth, $\alpha_k$ can be found after finite iterations.

\subsubsection{Momentum and IGM}
The momentum trick (or the inertial gradient method: IGM) considers the same extrapolation step:
\eq{
    y^{k+1}=x^k+\beta_k(x^k-x^{k-1}),\quad\beta_k>0
}
But the new iterate $x^{k+1}$ follows:
\eq{
x^{k+1}=y^{k+1}-\alpha_k\nabla f(x^k)=x^k-\alpha_k\nabla f(x^k)+\beta_k(x^k-x^{k-1}).
}
The gradient is now evaluated at $x_k$ and not at $y_{k+1}$! The updating formula contains the term $\beta_k(x^k-x^{k-1})$, is called
the \rt{momentum}. Compared to AGM, IGM doesn't enjoy the same convergence guarantee, but it's still popular, especially in non-convex scenarios.

\clearpage
\section{Constrained Optimization}

\tbf{General nonlinear optimization problem}:
\eq{
    \begin{aligned}\mathrm{minimize}_{\mathbf{x}\in\mathbb{R}^n}&&f(\mathbf{x})\\\mathrm{subject~to}&&g_i(\mathbf{x})\leq0,&&\forall\mathrm{~}i=1,\ldots,m,\\&&h_j(\mathbf{x})=0,&&\forall\mathrm{~}j=1,\ldots,p.\end{aligned}
}
The goal is to \tit{derive optimality conditions} for this more general class of problems.
\re{
    Some subclasses of optimization:
    \lis{
        \item When all functions are affine-linear: \yt{linear optimization problem};
        \item If $f(x)=\frac12x^\top Cx+c^\top x$, and $C\in\RR^{n\times n}$ is symmetric: \yt{quadratic
            optimization problem};
    }
}

\subsubsection{FONC for Constrained Optimization}
\defi{Feasible Direction}{
    Given $x\in X$, we call $d$ a \tbf{feasible direction} at $x$ if exists $\bar{t}>0$ such that
    $x+td\in X$ for all $0\le t \le \bar{t}$.
    \re{
        For example, if $\mathcal{X}=\{x:Ax=b\}$, then all feasible directions at $x$ are given by $\{d: Ad=0\}$.
    }
}
\thm{FONC for Constrained Problems}{
    Let $x^*$ be a local minimum of $\min_{x\in X}f(x)$. Then for \tit{any} feasible direction $d$ at $x^*$,
    we must have $\nabla f(x^*)^\top d\geq0.$\\
    When $x^*$ lies in the \tit{interior} of $X$, or the unconstrained case, it can be reduced to $\nabla f(x^*)=0$.
}
More than the FONC, we want to generalize the optimality conditions for the general nonlinear problems.
\defi{Active and Inactive Constraint}{
    Reconsidering the general nonlinear optimization problem, at point $x\in X$, the set $\mathcal{A}(x):=\{i:g_i(x)=0\}$
    denotes the set of \tbf{active constraints}. $\mathcal{I}(x):=\{i:g_i(x)<0\}$ denotes \tbf{inactive constraints}.
}
\lem{Farkas' Lemma}{
    Let $A\in \RR^{m\times n}$, $B\in \RR^{p\times n}$, and $c\in\RR^n$ be \tbf{given}. The following two
    statements are equivalent:
    \lis{
        \item For all $d \in \RR^n$ with $Ad<0$ and $Bd=0$ we have $c^T d\le 0$;
        \item There exists $u \in\RR^m, u\ge 0$, and $v\in \RR^p$ with $c=A^T u +B^Tv$.
    }
    These results remain valid in the cases $m=0$ and $p=0$.
}
\thm{FONC for Linearly Inequality Constrained Problems}{
    If $x^\star$ is a local minimum of:
    \eq{
        \mathrm{minimize}_xf(x)\quad\mathrm{s.t.}\quad Ax\leq b,
    }
    then there exists some $y\ge 0$ with:
    \eq{
        \begin{aligned}&\nabla f(x^*)+A^\top y=0\\&y_i\cdot(a_i^\top x^*-b_i)=0\quad\forall\mathrm{~}i,\end{aligned}
    }
    where $a_i^T$ is the $i$-th row of $A$.
    \proo{}{
        Since $x^*$ is a local minimum, so $\nabla f(x^*) d\ge0$ for every feasible direction $d$ at $x^*$.\\
        Now considering $d$, $d$ is feasible if $a_i^Td\le0$ for all $i$ with $a_i^Tx^*=b_i$, which are active constraints $\cA$. Let's define:
        \eq{
            \tilde{A}=(a_i^T)_{i\in \cA(x^*)}\in \RR^{|\cA|\times n}
        }
        Now we can refine the optimality condition as: for all $d$ with $\tilde{A}d\le0$, we have $-\nabla f(x^*) d \le 0$. By applying the Farkas' lemma,
        by setting $A=\tilde{A}, B=0, c=-\nabla(x^*)$, we can transform the optimality condition to:\\
        \begin{center}
            $\exists u \in \RR^{|\cA|} \ge 0$, with $-\nabla f(x^*)=\tilde{A}^Tu$.
        \end{center}
        Now we can define $y\in\RR^m$ with $y_\cA(X^*):=u$ and $y_\cI(X^*):=0$.
    }
}
\thm{FONC for Linearly Equality Constrained Problems}{
    The FONC condition for equality constraint is easier than the inequality constraint case. If $x^*$ is
    a local minimum for $\mathrm{min}_xf(x)\quad\mathrm{s.t.}\quad Ax = b,$ the FONC is:
    \begin{center}
        \eq{
            \exists y \quad s.t. \nabla f(x^*)+A^\top y=0.
        }
    \end{center}
    Compared to the FONC for inequality problems, the condition reduces the term $y_i\cdot(a_i^\top x^*-b_i)=0$.
    This is easy to understand since every constraint is active so $a_i^\top x^*-b_i=0$.\\
    Another difference is that $y$ doesn't need to be greater than 0.
}

\subsubsection{The KKT Conditions}
Now, we can extend the FONC for the general nonlinear optimization problem, this is called the \tbf{\rt{KKT}}
condition, named by \tit{H. Kuhn}, \tit{A. Tucker} and \tit{W.Karush}. The first step is to construct the \tbf{\bt{Lagrangian multiplier}}:
$g_i(x)\leq0 \cdots \lambda_i$, $h_j(x) =0 \cdots&\mu_j$. Then we define the \tbf{\bt{Lagrangian}} of the problem:
\eq{
    L(x,\lambda,\mu):=f(x)+\sum_{i=1}^m\lambda_ig_i(x)+\sum_{j=1}^p\mu_jh_j(x).
}
We require "$\lambda_i\ge0$, $\mu_i$ free". These conditions are called \tbf{\bt{dual feasibility}} conditions.
Taking the derivative with respect to $x$ and setting it to zero, forms the \rt{main condition} of KKT conditions. Besides,
we have a set of \rt{complementary conditions}: $\lambda_i\cdot g_i(x)=0, \forall i$.
\fig{KKT Conditions}{KKT Conditions}{0.7}
\re{
    \tbf{Linearly Independent Constraint Qualification}\\
    We require the collection of gradients:
    \eq{
        \{\nabla g_i(x^*):i\in\mathcal{A}(x^*)\}\cup\{\nabla h_j(x^*):j=1,\ldots,p\}
    }
    to be \tbf{linearly independent} (full rank). A feasible point satisfying the LICQ is called \tit{regular}.
}
\re{
    A (feasible) point satisfying the KKT conditions is called a \rt{KKT point}.
    The KKT conditions are only necessary conditions, meaning the KKT points are only candidates for
    local optimal solutions — just like stationary points
}
\pro{Convexity and KKT Conditions}{
    We call the problem $\min_{x\in\mathbb{R}^n}f(x)\quad\mathrm{s.t.}\quad g(x)\leq0,\quad h(x)=0$ convex
    if $f, g_i$ are convex and $h$ is an affine-linear mapping. If a problem is convex, we have the following
    propositions:
    \tab{
        \item Every local solution is a global solution;
        \item If $x^*\in X$ is the global solution and assume LICQ holds, then the KKT-conditions are satisfied;
        \item If $x^*$ is a KKT point, then $x^*$ is a global solution.
    }
}
\defi{Slater's Condition}{
    If a problem's constraints are convex (affine-linear), we say that \gt{Slater's condition} holds if:
    \eq{
        \exists\hat{x}:\quad g_i(\hat{x})<0,\quad i=1,\ldots,m,\quad h(\hat{x})=0.
    }
    Then proposition 2 above can be reduced to:
    \tab{
        \item Every global solution $x^*\in X$ satisfies the KKT conditions.
    }
}

\subsubsection{SOC for Constrained Optimization}
If we assume that $f, g_i, h_j$ are twice continuously differentiable, the Hessian of the Lagrangian is given by:
\eq{
    \nabla_{xx}^2L(x,\lambda,\mu)=\nabla^2f(x)+\sum_{i=1}^m\lambda_i\nabla^2g_i(x)+\sum_{j=1}^p\mu_j\nabla^2h_j(x).
}
\defi{Critical Cone}{
    \eq{
        \begin{aligned}\mathcal{C}(\mathbf{x}):=\{\boldsymbol{d}\in\mathbb{R}^n:\nabla f(\mathbf{x})^\top\boldsymbol{d} & =0,                                                           \\
               \nabla g_i(\mathbf{x})^\top\boldsymbol{d}                                                        & \leq0,\mathrm{~}\forall\mathrm{~}i\in\mathcal{A}(\mathbf{x}), \\
               \nabla h_j(\mathbf{x})^\top\boldsymbol{d}                                                        & =0,\mathrm{~}\forall j\}.\end{aligned}
    }
}
\thm{SOC for Constrained Problems}{
    Let $x^*$ be a regular point and local minimum. Then, the KKT conditions hold and there are \tbf{unique} (followed from LICQ)
    multiplier $\lambda\in\RR^m$ and $\mu\in\RR^p$ such that:
    \eq{
        \begin{aligned}&\nabla f(x^*)+\nabla g(x^*)\lambda+\nabla h(x^*)\mu=0,\\&g(x^*)\leq0,\quad h(x^*)=0,\quad\lambda\geq0,\quad\lambda_i\cdot g_i(x^*)=0\quad\forall i\end{aligned}
    }
    If we have:
    \eq{
        d^\top\nabla_{xx}^2L(x^*,\lambda,\mu)d\geq0\quad\forall\mathrm{~}d\in\mathcal{C}(x^*).
    }
    These conditions compose the SONC. The SOSC is just to modify the last inequation to:
    \eq{
        d^\top\nabla_{xx}^2L(x^*,\lambda,\mu)d>0\quad\forall\mathrm{~}d\in\mathcal{C}(x^*)\backslash\{0\}.
    }
}
\fig{Second-OrderConditions Comparison}{Second-OrderConditions: Comparison}{0.8}
\re{
    The strategy for solving nonlinear constrained programs:
    \lis{
        \item Check LICQ (if required);
        \item Derive KKT conditions;
        \item Discuss different easy cases via the complementarity conditions (set multiplier or
        constraints to 0) to find all KKT points;
        \item Calculate $\mathcal{C}(x)$ and $\nabla_{xx}^2L(x,\lambda,\mu)$ at KKT points;
        \item Check the SOC.
    }
    Other notes for solving the problem:
    \tab{
        \item Check if $f$ is coercive or bounded: has global solutions;
        \item If the LICQ holds, then $\lambda$ and $\mu$ are always unique.
    }
}

\subsubsection{Algorithms for Constrained Problems}
The first class of algorithm is the \tbf{penalty method}, the basic idea is to solve the constrained
problem via a sequence of unconstrained optimization problems, which are built by adding \gt{penalty terms}
with positive \gt{penalty parameters} for the constraints to the objective function.
\eq{
    \min_{x\in\mathbb{R}^n}f(x)+\alpha p(x),\quad\alpha>0,
}
where $p:\RR^n\to\RR$ satisfies: $p(x)=0$ for all $x\in X$ and $p(x)>0$ on $\RR \backslash X$.\\
The procedure is given below:
\lis{
    \item Generate a sequence of penalty problems corresponding to a growing sequence of
    penalty parameters $\{\alpha_k\}_k$;
    \item The solution $x^k$ of the $k$-th subproblem will be used as the initial point for the next problem.
}
\sep{The Quadratic Penalty Function}
\eq{
    \begin{aligned}
        P_{\alpha}(x) & :=f(x)+\frac\alpha2\sum_{i=1}^m(\max\{0,g_i(x)\})^2+\frac\alpha2\sum_{j=1}^ph_j(x)^2 \\
                      & =f(x)+\frac\alpha2\|(g(x))_+\|^2+\frac\alpha2\|h(x)\|^2.
    \end{aligned}
}
$P_\alpha$ is once continuously differentiable with:
\eq{
    \nabla P_\alpha(x)=\nabla f(x)+\alpha\sum_{i=1}^m(g_i(x))_+\nabla g_i(x)+\alpha\sum_{j=1}^ph_j(x)\nabla h_j(x).
}
\fig{The Quadratic Penalty Method}{The Quadratic Penalty Method}{0.5}
By applying the penalty method, every accumulation point of $\{x^k\}_k$ is a \tit{global solution}.\\
\tbf{Projected Gradient Method}\\
Applying the gradient method to the constraint problem directly may produce $x^k$ outside the feasible set, one idea is
to project $x^k$ onto the feasible set.
\defi{Euclidean Projections}{
    Let $X$ be the convex feasible set, the (Euclidean) projection of $x$ is defined as:
    \eq{
        \min_y\frac12\|x-y\|^2\quad\mathrm{s.t.}\quad y\in X
    }
    We write $y^*=\cP X(x)$, which is the point in $X$ that has the minimum distance to $x$.
}
For example, if $X=\{x:Ax=b\}$ and $X$ has full row rank, then it holds that:
$\cP_X(\mathbf{x})=\mathbf{x}-\mathbf{A}^\top(\mathbf{A}\mathbf{A}^\top)^{-1}[\mathbf{A}\mathbf{x}-\mathbf{b}].$
Different constraints correspond to different $\cP_X(x)$.

\clearpage
\section{Nonsmooth Optimization}

\clearpage
\section{Simulation}

\subsection{Variance Reduction Technique}

\subsubsection{Control Variates}
Suppose we want to estimate a parameter with mean $\mu$, and we have an unbiased statistic $X$. Suppose we have another similar pair $Y,\eta$,
then $X^{\prime}=X+c(Y-\eta)$ is also an unbiased estimater.
\eq{
    \begin{aligned}
        \mathbb{V}[X^{\prime}] & =\mathbb{V}[X+c(Y-\eta)]                                    \\
                               & =\mathbb{V}[X]+c^2\mathbb{V}[Y-\eta]+2c\text{Cov}(X,Y-\eta) \\
                               & =\mathbb{V}[Y-\eta]c^2+2\mathrm{Cov}(X,Y)c+\mathbb{V}[X].
    \end{aligned}
}
To minimize this objective function, set $c=-\frac{\operatorname{Cov}(X,Y)}{\operatorname{V}[Y]}\mathrm{~,}$ we can get:
\eq{
    \mathbb{V}[X^{\prime}]=\left(1-\mathrm{Corr}(\mathrm{X},\mathrm{~Y})\right)\mathbb{V}[X],
}
which reduces the original variance.

\clearpage
\chapter{Economics and Econometrics}

\section{Game Theory and Mechanism Design}

The contents of this section referenced \cite{fudenberg1991game}.
\subsection{Dynamic \& Static, Complete \& Incomplete Information}
\sep{Static game}
\defi{Game}{
    A strategic (normal form) game $G=(N,(A_i)_{i\in N},(u_i)_{i\in N})$ consists of:
    \tab{
        \item a finite set of players $N$;
        \item A non-empty set of actions for each player: $A_i, i\in N$;
        \item A payoff function for each player: $u_i:A\to\mathbb{R}(A=\prod_{i\in N}A_i)$.
    }
}
\thm{von Neumann-Morgenstern’s Utility Theorem}{
    Let $C$ be a set of outcomes and $P$ be the set of distributions over outcomes, a utility function $U:P \to \RR$ has an
    expected utility form if there exists a function $u: C \to \RR$ s.t. $U(x)=\sum_{c\in C}x(c)u(c)$ for all $x\in P$. In this
    case, $U(\cdot)$ is called an expected utility function, and $u(\cdot)$ is called a \tbf{VNM utility function}.\\
    The VNM (\cite{von2007theory}) is an axiomatic way to formalize the preference. 
    If a preference $\gtrsim$ among the lotteries (distributions) $U(\cdot)$ has an expected utility representation (follow the following axioms):
    \tab{
        \item \tit{Completeness}: for any two lotteries (distributions) $x$ and $y$, at least one of $x\gtrsim y$ or $y\gtrsim x$ holds;
        \item \tit{Transitivity}: if $x\gtrsim y$ and $y\gtrsim z$, then $y\gtrsim z$;
        \item \tit{Continuity}: if $x\gtrsim y\gtrsim z$, then exists $p\in [0,1]$ s.t. $px + (1-p)z \sim y$;
        \item \tit{Independence}: For any lottery $z$ and $p\in [0,1]$, $x\gtrsim y$ iff $px+(1-p)z\gtrsim py+(1-p)z$.
    }
    , there exists a VNM utility function $u$ such that for every lottery in $U$:
    \eq{\label{eq:eut1}
        P\succsim Q\iff\sum_{x\in X}P(x)u(x)\geq\sum_{x\in X}Q(x)u(x)
    }
}
\proo{}{
        The proof is finding and constructing $u_i$ that satisfies the axioms.
        Suppose there are $n$ outcomes $C_1,\cdots,C_n$, by the axioms of completeness and transitivity, we can rank them by order:
        \[C_1\lesssim C_2\lesssim\cdots\lesssim C_n\]
        In those circumstance where $C_1\sim C_n$, the therom holds obviously; otherwise $C_1\lesssim C_n$, we can define:
        \[L(p)=p\cdot C_1+(1-p)\cdot C_n, 0\le p \le 1\]
        By the axiom of continuity, for any $C_i$, we can always find a $p_i$ such that:
        \[L(p_i)\sim C_i\]
        and 
        \[C_1\lesssim L(p_i)\lesssim C_n \forall i\] 
        Then we can define the VNM utility for every outcome as $L(p_i)$, and the expected utility for every lottery $P=sum_i \alpha_i C_i$ as 
        \[U(P)=u(\sum_i \alpha_i C_i)=\sum_i \alpha_i u(C_i)\]
        Since the decision maker is indifferent among $C_i$ and $p_i\cdot C_1+(1-p_i)\cdot C_n$, by the reduction axiom, we can obtain:
        \eq{
            C_i = \frac{u(C_i)-u(C_1)}{u(C_n)-u(C_1)}C_1 + \frac{u(C_n)-u(C_i)}{u(C_n)-u(C_1)}C_n
        }
        This means the lottery $C_i$ s, in effect, a lottery in which the best outcome is won with probability $\frac{u(C_i)-u(C_1)}{u(C_n)-u(C_1)}$, 
        and the worst outcome otherwise. Note that $\frac{u(C_i)-u(C_1)}{u(C_n)-u(C_1)}$ is positive first-order linear on $u(C_i)$, so 
        a rational decision maker would prefer the lottery $P$ to $Q$ ($P\succsim Q$) if and only if $u(C_p)\ge u(C_q)$.
}
}
\defi{Best Response (BR)}{
    $a_i\in A_i\mathrm$ is a best response to $a_{-i}\in A_{-i}$ if $u_i(a_i,a_{-i})\geq u_i(a_i^{\prime},a_{-i})\mathrm{~}\forall a_i^{\prime}\in A_i.$
    The best response $BR_i(a_{-i})$ is not a function, but a \tbf{correspondence} (there may be multiple BRs).
}
\thm{Existence of NE (Nash 1951)}{
    Any game with a finite set of players and a finite set of strategies has an NE of mixed strategies (this theorem emphasizes on \tit{finite games}).
    \lem{Kakutani Fixed Point}{
            Let $f:A \rightrightarrows A$ be a correspondence, if:
            \tab{
                \item $A$ is a compact and convex set;
                \item $f(x)$ is non-empty for all $x\in A$;
                \item for all $x\in A$, $f(x)$ is a convex set;
                \item $f(x)$ has a closed graph: if $\{x^n,y^n\}\to\{x,y\}$ with $y^n\in f(x^n)$, then $y\in f(x)$。
            }
            Then $f$ has a fixed point: $\exists x\in A$, s.t. $x\in f(x)$.
        }
}
\proo{}{
    First, we show that the action set $\Sigma=\prod_{i\in\mathcal{I}}\Sigma_i$ is compact, convex, and non-empty. 
    This conclusion is quite natural, since each $\Sigma_i=\{x\mid\sum_jx_j=1\}$ is a simplex, thus compact, convex, and non-empty, 
    so is the product. Then we note the best correspondence $B(\sigma)$ is not empty because:
    \[B_i(\sigma_{-i})=\arg\max_{x\in\Sigma_i}u_i(x,\sigma_{-i})\]
    is non-empty by Weirstrass’s theorem.\\
    Next, we show that $B(\sigma)$ is a convex set for all $\sigma$. When $\|B(\sigma)\|=1$, it's a convex set by nature. When $B(\sigma)$ 
    is not a single element, we take $\sigma_i^\prime, \sigma_i^{\prime\prime}$ to denote any two elements among $B(\sigma_{-i})$:
    \eq{
        \begin{aligned}
            &u_i(\sigma_i^{\prime},\sigma_{-i})\geq u_i(\tau_i,\sigma_{-i})\quad\text{ for all }\tau_i\in\Sigma_i,\\
            &u_i(\sigma_i^{\prime\prime},\sigma_{-i})\geq u_i(\tau_i,\sigma_{-i})\quad\text{ for all }\tau_i\in\Sigma_i.
        \end{aligned}
    }
    We can show that for all $\lambda\in(0,1)$:
    \eq{
        \lambda u_i(\sigma_i^{\prime},\sigma_{-i})+(1-\lambda)u_i(\sigma_i^{\prime\prime},\sigma_{-i})\geq u_i(\tau_i,\sigma_{-i})\quad\mathrm{~for~all~}\tau_i\in\Sigma_i.
    }
    By the linearity of $u_i$:
    \eq{
        u_i(\lambda\sigma_i^{\prime}+(1-\lambda)\sigma_i^{\prime\prime},\sigma_{-i})\geq u_i(\tau_i,\sigma_{-i})\quad\mathrm{~for~all~}\tau_i\in\Sigma_i.
    }
    Therefore, $\lambda\sigma_i^{\prime}+(1-\lambda)\sigma_i^{\prime\prime}\in B_i(\sigma_{-i})$, indicating that $B(\sigma)$ is a convex correspondence.\\
    At last, we prove that $B(\sigma)$ is a closed graph by contradiction. Suppose not, then exists a sequence $(\sigma^n,\hat{\sigma}^n)\to(\sigma,\hat{\sigma})$ with 
    $\hat{\sigma}^n\in\mathcal{B}(\sigma^n)$, but $\hat{\sigma}_i\notin B_i(\sigma_{-i})$. This implies for any $\epsilon>0$, there always exists some $\sigma_i^\prime$ such that:
    \[u_i(\sigma_i^{\prime},\sigma_{-i})>u_i(\hat{\sigma}_i,\sigma_{-i})+3\epsilon.\] 
    But by the fact that $\sigma_{-i}^n\to\sigma_{-i}$, we can find $n$ such that:
    \[u_i(\sigma_i^{\prime},\sigma_{-i}^n)\geq u_i(\sigma_i^{\prime},\sigma_{-i})-\epsilon.\]
    This leads us to:
    \eq{
        u_i(\sigma_i^{\prime},\sigma_{-i}^n)>u_i(\hat{\sigma}_i,\sigma_{-i})+2\epsilon\geq u_i(\hat{\sigma}_i^n,\sigma_{-i}^n)+\epsilon.
    }
    By the Kakutani Fixed Point theorem, for any $\sigma$, $B(\sigma)$ is not empty.
}
\defi{Strict Dominance}{
    $a_i\in A_i$ if \tbf{strictly dominated} if $a_i$ is strictly dominated by mixed strategy $\alpha_i \in \Delta(A_i)$:
    \eq{
        u_i(\alpha_i,a_{-i})>u_i(a_i,a_{-i})\mathrm{~}\forall a_{-i}\in A_{-i}
    }
    $a^*\in A$ is a \tit{dominant strategy equilibrium} if $\forall i \in N$: $u_i(a_i^*,a_{-i})\geq u_i(a_i,a_{-i}), \forall a_i\in A_i,\forall a_{-i}\in A_{-i}$.
}
\ex{Vickery auction (second price auction)}{
    In the second price auction, the dominant strategy equilibrium for each player is to pose the actual value of the bid.
}
Iterated elimination of strictly dominated strategies:
\fig{Iterated elimination}{Pure Strategy}{0.5}
\fig{Iterated elimination mixed}{Mixed Strategy}{0.6}
$D_i^\infty$ is the set of strategies that survive iterated strict dominance.
\defi{Rationalizability}{
    First, we need to define the \tbf{belief}: a belief of player $i$ about the other players' actions is a 
    probability measure over $S_{-i}$, denoting as $\Delta(S_{-i})$. We allow correlation in our belief: $\Delta(S)$ denotes a probability distribution over $S$.\\
    For a game $G$, $a_i\in A$ is rationalizable if $\exists Z_j\subset A_j,j\in N\mathrm{~s.t.}$:
    \lis{
        \item $a_i\in Z_i$;
        \item each $a_j\in Z_j$ is a BR to some belief $\mu_j\in\Delta(Z_{-j})$.
    }
    Another definition of rationalizability is given: A strategy is said to be rationalizable 
    if and only if it survives the iterated elimination of never best response.
}
\defi{Never Best Responses (NBR)}{
    A pure strategy $s_i$ is a never-best response if for all beliefs $\sigma_{-i}$ there exists $\sigma_i \in \sum_i$ s.t.:
    \eq{
        u_i(\sigma_i,\sigma_{-i})>u_i(s_i,\sigma_{-i})
    }
    \re{
        \tab{
            \item A strictly dominated strategy is a never-best response;
            \item The reverse does not hold (hold in correlated beliefs).
        }
    }
}
\defi{Correlated Equilibrium}{
    A correlated equilibrium is a joint distribution $\pi in \Delta(S)$ such that $R$ is a R.V. distributed according to $\pi$:
    \eq{
        \sum_{s_{-i}\in S_{-i}}\operatorname{Prob}(R=s|R_i=s_i)\left[u_i(s_i,s_{-i})-u_i(t_i,s_{-i})\right]\geq0
    }
    for all $t_i\in S_i$.
}
\defi{Nash Equilibrium}{
    $a^*=(a_1^*,...,a_n^*)\in A$ is an NE if $a_i^*\in B_i(a_{-i}^*)\mathrm{~}\forall i\in\mathbb{N}$. An NE can be either pure-strategy or mixed-strategy, 
    every NE is also a correlated equilibrium. 
    \re{
        The relationship is given as:
        \eq{
            NE_i\subseteq R_i^\infty\subseteq D_i^\infty ，
        }
        where $NE_i$ is the Nash Equilibrium, $R_i^\infty$ is the set of rationalizable strategies, and $D_i^\infty$ is the set of 
        strategies that survive iterated strict dominance.
    }
    Not all games have an NE (usually due to the dis-coutinuity in $u_i$).
}
\thm{Existence of NE (Nash 1951)}{
    Any game with a finite set of players and a finite set of strategies has an NE of mixed strategies (this theorem emphasizes on \tit{finite games}).
    \lem{Kakutani Fixed Point}{
            Let $f:A \rightrightarrows A$ be a correspondence, if:
            \tab{
                \item $A$ is a compact and convex set;
                \item $f(x)$ is non-empty for all $x\in A$;
                \item for all $x\in A$, $f(x)$ is a convex set;
                \item $f(x)$ has a closed graph: if $\{x^n,y^n\}\to\{x,y\}$ with $y^n\in f(x^n)$, then $y\in f(x)$。
            }
            Then $f$ has a fixed point: $\exists x\in A$, s.t. $x\in f(x)$.
        }
}
\proo{}{
    First, we show that the action set $\Sigma=\prod_{i\in\mathcal{I}}\Sigma_i$ is compact, convex, and non-empty. 
    This conclusion is quite natural, since each $\Sigma_i=\{x\mid\sum_jx_j=1\}$ is a simplex, thus compact, convex, and non-empty, 
    so is the product. Then we note the best correspondence $B(\sigma)$ is not empty because:
    \[B_i(\sigma_{-i})=\arg\max_{x\in\Sigma_i}u_i(x,\sigma_{-i})\]
    is non-empty by Weirstrass’s theorem.\\
    Next, we show that $B(\sigma)$ is a convex set for all $\sigma$. When $\|B(\sigma)\|=1$, it's a convex set by nature. When $B(\sigma)$ 
    is not a single element, we take $\sigma_i^\prime, \sigma_i^{\prime\prime}$ to denote any two elements among $B(\sigma_{-i})$:
    \eq{
        \begin{aligned}
            &u_i(\sigma_i^{\prime},\sigma_{-i})\geq u_i(\tau_i,\sigma_{-i})\quad\text{ for all }\tau_i\in\Sigma_i,\\
            &u_i(\sigma_i^{\prime\prime},\sigma_{-i})\geq u_i(\tau_i,\sigma_{-i})\quad\text{ for all }\tau_i\in\Sigma_i.
        \end{aligned}
    }
    We can show that for all $\lambda\in(0,1)$:
    \eq{
        \lambda u_i(\sigma_i^{\prime},\sigma_{-i})+(1-\lambda)u_i(\sigma_i^{\prime\prime},\sigma_{-i})\geq u_i(\tau_i,\sigma_{-i})\quad\mathrm{~for~all~}\tau_i\in\Sigma_i.
    }
    By the linearity of $u_i$:
    \eq{
        u_i(\lambda\sigma_i^{\prime}+(1-\lambda)\sigma_i^{\prime\prime},\sigma_{-i})\geq u_i(\tau_i,\sigma_{-i})\quad\mathrm{~for~all~}\tau_i\in\Sigma_i.
    }
    Therefore, $\lambda\sigma_i^{\prime}+(1-\lambda)\sigma_i^{\prime\prime}\in B_i(\sigma_{-i})$, indicating that $B(\sigma)$ is a convex correspondence.\\
    At last, we prove that $B(\sigma)$ is a closed graph by contradiction. Suppose not, then exists a sequence $(\sigma^n,\hat{\sigma}^n)\to(\sigma,\hat{\sigma})$ with 
    $\hat{\sigma}^n\in\mathcal{B}(\sigma^n)$, but $\hat{\sigma}_i\notin B_i(\sigma_{-i})$. This implies for any $\epsilon>0$, there always exists some $\sigma_i^\prime$ such that:
    \[u_i(\sigma_i^{\prime},\sigma_{-i})>u_i(\hat{\sigma}_i,\sigma_{-i})+3\epsilon.\] 
    But by the fact that $\sigma_{-i}^n\to\sigma_{-i}$, we can find $n$ such that:
    \[u_i(\sigma_i^{\prime},\sigma_{-i}^n)\geq u_i(\sigma_i^{\prime},\sigma_{-i})-\epsilon.\]
    This leads us to:
    \eq{
        u_i(\sigma_i^{\prime},\sigma_{-i}^n)>u_i(\hat{\sigma}_i,\sigma_{-i})+2\epsilon\geq u_i(\hat{\sigma}_i^n,\sigma_{-i}^n)+\epsilon.
    }
    By the Kakutani Fixed Point theorem, for any $\sigma$, $B(\sigma)$ is not empty.
}
\fig{Examples of no fixed point}{Examples of no fixed point}{0.5}
\ex{A game with no NE but has correlated equilibria, consider the game with 3 players: 
    \begin{table}[H]
        \centering
        \begin{tabular}{|c|c|c|}
            \hline
            Player & Action Space & Action Notation \\
            \hline
            1 & $[0,1]\times\mathbb{N}$ & $(x,m)$  \\
            \hline
            2 & $[0,1]\times\mathbb{N}$ & $(y,n)$  \\
            \hline
            3 & $[0,1]$ & $x^\prime$   \\
            \hline
        \end{tabular}
    \end{table}
    The payoff function for each player is given as:
    \lis{
        \item $-2\cdot\tbf{1}(x=x^\prime) + 2\cdot\tbf{1}(y=x\ne x^\prime) + \tbf{1}(y\ne x\ne x^\prime)\cdot [\tbf{1}(m>n)+\tbf{1}(m<n)]$;
        \item $-2\cdot\tbf{1}(x=x^\prime) + 2\cdot\tbf{1}(y=x\ne x^\prime) + \tbf{1}(y\ne x\ne x^\prime)\cdot [\tbf{1}(n>m)+\tbf{1}(n<m)]$;
        \item $\tbf{1}(x=x^\prime)$ 
    }
    }{
        The game has no NE: To avoid the circumstance of $\tbf{1}(x=x^\prime)=1$, player 1 has to decide $x$ stochasticity on $[0,1]$, 
        thus $\tbf{1}(y\ne x\ne x^\prime)=1$ almost surely. This would lead the game to a zero-sum game between player 1 and palyer 2, where 
        $A_i$ is not compact for each player (can not apply the Minimax Theorem). This dual-player game has no NE.\\
        But this game has correlated equilibriums, under which players 1 and 2 make decisions $x$ and $y$ based on a joint distribution $\{(x,y)\| x=y\}$ (only with mass point on the diagonal $x=y$). 
        From player 3's perspective, the marginal distribution of player 1 is still stochastic. This can make $\tbf{1}(y=x\ne x^\prime)=1$ almost surely, and this is a set of correlated equilibriums.
}
\thm{Minimax Theorem}{
    Let $G=(\{1,2\},(A_i),(u_i))$ be a \tbf{zero-sum} game where $A_i$ is compact and $u_i$ is continuous. Then $a^*$ is NE iff:
    \lis{
        \item $a_i^*$ is a maxminimizer: $a_i^* = \arg\max_{a_i}\min_{a_{-i}}u_i(a_i, a_{-i})$;
        \item $u_i(a^*)=\max_{a_i}\min_{a_{-i}}u_i(a)=\min_{a_{-i}}\max_{a_i}u_i(a)$.
    }
}
\thm{Glicksberg}{
    \defi{Continuous games}{
        A game $\langle\mathcal{I},(S_i),(u_i)\rangle $ where $\cI$ is a finite set, the $S_i$ are nonempty compact spaces, and $u_i$ are continuous functions 
        is called a continuous game.
    }
    Any continuous game has a mixed strategy NE.
    \proo{}{
        \lis{
            \item First we approximate the original game with a sequence of finite games;
            \item Then show these finite games have equilibriums using Nash's theorem;
            \item At last using the continuity and closed assumption to show these converge to the original game.
        }
    }
}
Nash Equilibrium sometimes can bring unreasonable behavior. In this game, both $(A,A)$ and $(B,B)$ are NEs, but $(A,A)$ is surely more reasonable.
\fig{Nash Equilibrium unreasonable}{Example of unreasonable NE}{0.4}
\defi{Trembling Hand Perfect Equilibrium}{
    A strategy $\sigma$ is a trembling hand perfect equilibrium if there exists a sequence of \tbf{totally mixed} 
    ($\sigma_i(s_i)>0$ for all $s_i\in S$) strategy $\sigma^n$ converging to $\sigma$, such that $\sigma_i\in BR_i(\sigma^n_{-i})$ for all $n, i$.\\
    Using this definition, we can observe that $(B,B)$ is not a trembling hand perfect equilibrium, because it's not the best response for any totally mixed strategy.
    \re{
        \tab{
            \item Every trembling hand perfect equilibrium is an NE;
            \item For every \tit{finite game}, there exists a trembling hand perfect equilibrium.
        }
    }
}
\sep{Dynamic Game}
Consider a sequential game, denoting $h^{0}=\emptyset $ as the initial \tbf{history}, $a^k=(a_1^k,\cdots, a_n^k)$ as the actions at stage $k$, 
and $h^{k+1}=(a^0,a^1,\ldots,a^k)$ as the hostory at stage $k+1$. Let $H=\cup_{k=0}^{K+1}H^k$ be the set of all possible histories. Under this definition, 
the pure strategy in the sequential games is a plan for every possible history $h_k$: $S_i(H^k)=\bigcup_{h^k\in H^k}S_i(h^k)$.
\defi{Information Set}{
    The information set is a subset of the history, partitioning the nodes of the game tree. A player who is choosing an action at $x$ is uncertain 
    if he is at $x$ or at $x^\prime \in h(x)$.
    \re{
        \tab{
            \item When $x^{\prime}\in h(x)$, $A(x)=A(x^{\prime})$;
            \item A game has \rt{perfect information} if all its information sets are singletons.
        }
    }
}
\defi{Sub-Game \& SPNE}{
    A subgame $G^\prime$ of an extensive form game $G$ consists of a single node and all its successors in $G$, with the property that 
    if $x^\prime \in V_{G^\prime}, x^{\prime\prime}\in h(x^{\prime})$, then $x^{\prime\prime}\in V_{G^{\prime}}$.\\
    A \tbf{Subgame Perfect (Nash) Equilibrium} is a strategy $s^*$ that for any $G^\prime \in G$, $S^*|G^{\prime}$ is an NE for $G^\prime$. 
    The SPNE can remove noncredible threats. Every finite extensive form game $G$ has an SPNE and every finite perfect information extensive form game 
    has a pure strategy SPNE.
    \re{
        How to find SPNE:
        \tab{
            \item In finite-horizon games: backward induction;
            \item For infinite-horizon games (with continuity at infinity): one-stage deviation principle.
        }
    }
}
\sep{Incomplete Information}
First, we define \tbf{state}, each state $w\in \Omega$. States are exhaustive and mutually exclusive. Information function $P_i(w)$ is the set of states that $i$ thinks he may be in, 
given that the true state is $w$ (analogous to 'information set').
\ass{Information Function}{
    \lis{
        \item \tit{Reflexitivity or Non-Deludedness}: $\omega \in P_i(\omega)$;
        \item Consistency: $\forall\omega,\omega^{\prime}\in\Omega,\omega^{\prime}\in P_i(\omega)\Rightarrow P_i(\omega^{\prime})=P_i(\omega)$.
    }
}
\defi{Knowledge Operator}{
    We say that $i$ knows event $A\subset\Omega$ at $\omega$ if $P_i(\omega)\subset A$. Thus the event that $i$ knows A is:
    \eq{
        K_i(A):=\{\omega\in\Omega:P_i(\omega)\subset A\}
    }
    \ex{
        Consider the game between $A$ and $B$, $A$ may feel good or sick, $B$ can see whether has received a message of `$A$ is sick'. $\Omega=\{a,b,c\}$:
        \lis{
            \item a: $A$ is good;
            \item b: $A$ is sick, $B$ receive the message;
            \item c: $A$ is sick, $B$ receive no message;
        } 
        We can conclude that $P_A=\{\{a\},\{b,c\}\},P_B=\{\{a,b\},\{c\}\}$.
    }{
        \tab{
            \item $B$ know that $A$ is sick: $K_B(\{b,c\})=\{\omega:P_B(\omega)\subset\{b,c\}\}=\{c\}$;
            \item $A$ doesn't know that $B$ know $A$ is sick: $K_A(K_B(\{b,c\}))=\{\omega:P_A(\omega)\subset\{c\}\}=\varnothing $.
        }
    }
    Properties of the knowledge operator:
    \lis{
        \item $K_i(\Omega)=\Omega $;
        \item $E\subset F\Rightarrow K_i(E)\subset K_i(F)$;
        \item $K_i(A)\cap K_i(B)=K_i(A\cap B)$;
        \item $K_i(A)\subset A$;
        \item $K_i(A) = K_i(K_i(A))$；
        \item $\Omega\setminus K_i(A)= K_i(\Omega\setminus K_i(A))$。
    }
    \re{
        An event $A\subset\Omega $ is \tbf{self-evident} if $P_i(\omega) \subset A$ for any $\omega \in A, i$. An event $E\subset\Omega$ is 
        \tbf{common knowledge} at state $\omega$ if exists a self-evident $A$ s.t. $w\in A\subset E$ for all players (or $P_i(\omega)\subset E$ for all $i$).
    }
}
In games with incomplete information, one agent is unsure about the payoffs of others. One solution is to suppose that players have \tbf{common} prior on the distribution 
of possible games, this can create common knowledge by making the original game into a `bigger' one (incomplete information to imperfect information).
\defi{Bayesian Games}{
    Compared to normal games, the Bayesian games have two more elements:
    \lis{
        \item A set of types for each player $i$: $\theta_i\in\Theta_i$, different type can bring different payoff functions;
        \item A (joint) distribution $p(\theta_1,\ldots,\theta_1)$ over the types.
    }
    Under this definition, a strategy for player $i$ is a map $s_i:\Theta_i\to S_i$.\\
    \emocool Why it's called \tit{Bayesian game}? Given $p(\theta_1,\ldots,\theta_n)$, each player can calculate $p(\theta_{-i}\mid\theta_i)$ using Bayes rule. Thus, 
    the expected utility for player $i$ is given as:
    \eq{
        U(s_i^{\prime},s_{-i},\theta_i)=\sum_{\theta_{-i}}p(\theta_{-i}\mid\theta_i)u_i(s_i^{\prime},s_{-i}(\theta_{-i});\theta_i,\theta_{-i})
    }
}
\thm{Bayesian Nash Equilibrium (BNE)}{
    A strategy profile $s(\cdot)$ is a BNE if for all $i,\theta_i$, we have that:
    \eq{
        s_i(\theta_i)\in\arg\max_{s_i^{\prime}\in\mathcal{S}_i}\sum_{\theta_{-i}}p(\theta_{-i}\mid\theta_i)u_i(s_i^{\prime},s_{-i}(\theta_{-i});\theta_i,\theta_{-i}),
    }
    or in the infinite-type case:
    \eq{
        s_i(\theta_i)\in\arg\max_{s_i^{\prime}\in S_i}\int u_i(s_i^{\prime},s_{-i}(\theta_{-i});\theta_i,\theta_{-i})dP(\theta_{-i}\mid\theta_i).
    }
    \emotree For any finite Bayesian game, there exists a mixed-strategy BNE.
}
\sep{Dynamic Games of Incomplete Information}
In this game, there is an analog of BNE, called perfect Bayesian equilibrium (PBE), which strengthens subgame perfection by:
\lis{
    \item A complete strategy (a mapping from information set to strategies);
    \item Beliefs for each player $(P_i(\nu|h)$ (a conditional distribution over everything player $i$ \tbf{does not know}, given everything that player $i$ \tbf{does know}).
}
In a dynamic game of complete and perfect information, PBE is equivalent to SPNE because all information sets are singletons, so beliefs are trivial. 
\ex{
    Ante Game: \\
    there are two players. Player $i$ can observe $t_i\sim uniform(0,1)$ independently. Player $1$ can `showdown'(both players show $t_i$, the higher $i$ wins), or he can `raise': 
    under this case, player $2$ can `fold' (lose the game) or `match' (force a showdown).\\
    In this game, the information sets for player $1$ is given as $(t_1)$, and $(t_2,a_1)$ for player $2$. The beliefs for player $1$ is $p_1(t_2|t_1)=t_2$ and $p_2(t_1|t_2,a_1)=p_2(t_1|a_1)$ for player $2$.
}{
    Find the PBE for the game.
}
\sep{The signaling game}
Player $1$ has some private information $t_1$ and first move $A_1$, player $2$ sees $A_1$ and make move $A_2$. In this case, when player $2$ moves first, it's called a \tbf{screening game}. 
An example is the job market game (\cite{Spence1973}): player $1$ is the worker, who has the intrinsic ability $t_1$ and education decision $a_1$. Player $2$ is the firm that can decide the wage offered $a_2$.
\fig{The stylised signaling game}{The stylised signaling game}{0.5}
Consider a stylized signaling game: a seller has an item with quality of either $H$ (with probability $p$) or $L$ (probability $1-p$), and the seller can advertise either $H$ or $L$. In this case, the game only has a 
\tbf{pooling} equilibrium (the player $2$'s belief is independent of the seller's action). But suppose there is a cost $c$ ($H-c<L$) when the seller lies, this game has an \tbf{separating} equilibrium: the seller 
always tells the truth and the player $2$ always believes the advertisement.

\subsection{Repeated Games}

In \tbf{repeated games}, we refer to a situation in which the same \tbf{stage game} is played for $T$ times with a discount factor $\delta\in[0,r)$. 
We assume \tit{perfect monitoring}: the outcomes of all past periods are observed by all players. Notation:
\tab{
    \item $\mathbf{a}=\{a^t\}_{t=0}^T$：the sequence of actions;
    \item $\alpha=\{\alpha^t\}_{t=0}^T$: the sequence of mixed strategies;
    \item $u_i(\mathbf{a})=\sum_{t=0}^T\delta^tg_i(a_i^t,a_{-i}^t)$: payoff.
}
In any finitely repeated prisoner's dilemma game, the unique SPE is always to defect. If the game has multiple NEs, the SPE is to select one NE in each period. 
In infinitely repeated games, we can consider \rt{trigger strategies}: both cooperate if no defect in the past, otherwise always defect. This is a SPE when $\delta$ is large enough. 
\fig{Sustaining Cooperation}{Sustaining Cooperation}{0.5}
In the examples above, the game has two NEs. Now consider the following strategies:
\lis{
    \item In period 1, play (A,A);
    \item In period $2,\cdots,t^*$, $t^*\in(1,T)$: if (A,A), then continue; otherwise (C,C);
    \item In period $t^*+1,\cdots,T$: if all (A,A) in stage 2 and (B,B) after stage 2: (B,B); otherwise (C,C).
}
By \tbf{one-shot deviation principle}, this is an SPE for appropriate $t^*$. For the largest available $t^*$:
\eq{
    1 \leq \sum_n^{T-t^*}\delta^n
}
When both $T$ and $\delta$ are sufficiency large, we can make the actions (A,A) be played for arbitrarily many periods (infinite game). 
\thm{Nash Folk Theorem}{
    If $(v_1,\cdots, v_n)$ is feasible and higher than the minmax payoff lower bounds $\underline{v}_i$ for all $i$, then there exists $1>\delta\ge\underbar{\delta}$, that 
    for all game $G^{\infty}(\delta)$, there is a equilibrium with payoffs $(v_1,\cdots, v_n)$. 
}

\subsection{Cooperative Game}

A \tbf{cooperative game} (coalitional game) is a model of interacting decision-makers that focuses on the behavior of groups of players ($N$). 
A \tbf{coalition} is a subset $S\subset N$, we call $N$ the \tbf{grand coalition}. 
Every coalition $S$ has a set of available actions $A_S$. 
An \tbf{outcome} consists of a CS (\tbf{coalition structure}) and one action associated with the CS:
\[
    (S_k,a_k)_{k=1,...,\bar{k}}\text{with}S_j\cap S_k=\emptyset,\forall j\neq k,\cup_kS_k=N,a_k\in A_{S_k}.
\]
Each agent has a utility function $u_i$ without externalities. 
\defi{Transferable Payoffs}{
    A game with transferable payoffs associates any coalition $S$ to a real number $v(S)$, and assumes $v(\emptyset)=0$. 
    The set of actions available for coalition $S$ is all possible divisions $(x_i)_{i\in S}$ such that $\sum_{i\in S}x_i=v(S)$.
}
A coalition $S$ can \tbf{block} an action $a_N$ of the grand coalition if $v(S)>\sum_{i\in S}x_i, x_i=a_{N_i}$. 
We call a game \tbf{cohesive} if there exists some action $a_N$ in the grand coalition that no coalition can block. 
\defi{Core}{
    The core of a coalitional game is the set of actions $a_N$ of the grand coalition $N$ that are not blocked by any coalition.
}
\thm{Bondareva \$ Shapley}{
    A non-negative vector $\lambda_S$ with dimension $2^N$ is a \tbf{balanced} weight if:
    \[
        \sum_{\{S\subset N|i\in S\}}\lambda_S=1,\forall i\in N.
    \]
    A payoff function is \tbf{balanced} if for every balanced weight $\lambda$:
    \eq{
        \sum_{S\subset N}\lambda_Sv(S)\leq v(N)
    }
    There is an intuitive way to understand the balanced weight, $\lmabda_S$ can be viewed as the proportion of time for player $i$ distributing on the coalition $S$.
    \begin{center}
        A coalitional game with transferable payoffs has a non-empty core iff it is balanced.
    \end{center}
    \proo{}{
        The proof needs the duality theorem. 
        The core can be solved by solving the LP:
        \eq{
            \min\sum_{i\in N}x_i\mathrm{~s.t.~}\sum_{i\in S}x_i\geq v(S),\forall S\subset N.
        }
        The core is non-empty iff $\sum_{i\in N}x_i^* \le v(N)$, and the dual of the LP is given by:
        \eq{
            \max\sum_{S\in2^N}\lambda_Sv(S)\text{ s.t. }\lambda_S\geq0,\forall S\subset N\&\sum_{S\ni i}\lambda_S\leq1,\forall i\in N.
        }
        By the definition of LP, $\sum_{i\in N}x_i^*=\sum_{S\in2^N}\lambda_S^*v(S)$. 
        So if $LHS\le v(N)$, then $RHS\le v(N)$. 
    }
}
\defi{Convex Cooperative Game}{
    A game with transferable playoffs $v$ is \tit{convex} (sometimes referred to as super-modularity) if for any two colations $S$ and $T$:
    \eq{
        v(S\cup T)+v(S\cap T)\geq v(S)+v(T),
    }
    which implies that the marginal contribution of an individual $i$ to a coalition is (weekly) increasing as the coalition gets larger because for any $S\subset T$ and $i\notin T$:
    \[
        S\subset T\And i\notin T\implies v(T\cup\{i\})-v(T)\geq v(S\cup\{i\})-v(S).
    \]
    To see this, we consider the interaction between $S\cup \{i\}$ and $T$ and apply the above definition:
    \eq{
        v((S\cup\{i\})\cup T)+v((S\cup\{i\})\cap T)\geq v(S\cup\{i\})+v(T).
    }
    \re{
        There are some similar definitions in the game:
        \lis{
            \item \tbf{Superaddictivity}: for any two disjoint $S,T$: $v(S\cup T)\geq v(S)+v(T).$
            \item \tbf{Monotone}: for any coalitions $S\subset T$: $v(S)\le v(T)$.
        }
        Convexity implies superadditivity, and superadditivity implies monotonicity.
    }
}
\pro{}{
    Any convex game with transferable payoffs has a non-empty core.
}

\subsubsection{The Core and Competitive Equilibria}
Consider an exchange economy with a set of consumers $N$ and a set of goods $G$. 
A \tit{consumption bundle} is an element $x \in \RR_+^G$. 
Consumer $i$ enjoys utility $u_i(x_i)$ from a bundle $x_i$. 
Each consumer starts with an endowment $\omega_i\in \RR^G_+$. 
An \tit{allocation} $\{x_i\}_{i\in N}$ with $x \in \RR_+$ is feasible if
\[
    \sum_{i\in N}x_{i}=\sum_{i\in N}\omega_{i}
\]
\defi{Competitive Equilibria}{
    A competitive equilibria is a price vector $(p_g^*)_{g\in G}$ and a feasible allocation $x^*=(x_i^*)_{i\in N}$ such that:
    \eq{
        p^*\cdot x_i\leq p^*\cdot\omega_i\implies u_i(x_i^*)\geq u_i(x_i).
    }
    Intuitive understanding: $x_i^*$ is the consumption bundle that maximizes consumer $i$'s utility among bundles he can afford given the price $p^*$. 
}
We can view the market as a cooperative game (without considering prices), and for any coalition $S$ in the game, the distribution need to satisfy:
\eq{
    A_S=\left\{(x_i)_{i\in S}\mid\sum_{i\in S}x_i=\sum_{i\in S}\omega_i\right\}.
}
Note that in this game there is no transferable utility function. 
\thm{}{
    \begin{center}
        Any competitive equilibrium is in the core.
    \end{center}
    \proo{}{
        Let $x^*$ be a competitive equilibrium allocation corresponding to a price vector $p$. 
        Suppose that a coalition $S$ can block $x^*$, then there exists $(x_i)_{i\in S}$ such that $\sum_{i\in S}x_i=\sum_{i\in S}\omega_i$ and $u_i(x_i)>u_i(x_i^*)\forall i\in S$. 
        By definition of equilibrium, we can imply:
        \[
            p\cdot x_i>p\cdot \omega_i, \forall i\in S
        \]
        Adding up these conditions we obtain
        \eq{
            p\cdot\sum_{i\in S}x_i>p\cdot\sum_{i\in S}\omega_i.
        }
        Contradiction. \emotree
    }
}

\subsubsection{Shapley Value}
An outcome in the core may be unfair, a fair payment scheme rewards each agent according to his marginal contribution. 
However, this scheme is dependent on the order, we can permutate on all the orderings, which is the Shapley value.

\defi{Shapley Value}{
    The Shapley value is given by:
    \eq{
        \varphi_i(v)&=\sum_{S\subset N\setminus\{i\}}\frac{|S|!(|N|-|S|-1)!}{|N|!}(v(S\cup\{i\})-v(S)).
    }
    Intuitive understanding: $\varphi_i(v)$ represents the expected value of player $i$'s contribution to the coalition formed by the players preceding him in line. 
}
\ax{}{
    \tab{
        \item \tbf{Symmetry}: if $i$ and $j$ are interchangable ($v(S\cup\{i\})=v(S\cup\{j\})$ for all $S$ disjoint from $\{i,j\}$) in $v$, then $\varphi_{i}(v)=\varphi_{j}(v)$;
        \item \tbf{Dummy}: if $i$ is dummy ($v(S\cup\{i\})=v(S)$ for all $S$) in $v$, then $\varphi_{i}(v)=0$;
        \item \tbf{Addictivity}: For any two games $v$ and $w$, we have $\varphi(v+w)=\varphi(v)+\varphi(w)$.
    }
}
\thm{}{
    \begin{center}
        A value satisfies the three axioms above iff it is the Shapley value.
    \end{center}
    \proo{}{
        First, we prove the `if' part. The only not-so-straightforward part is the proof of symmetry:
        \eq{
            \begin{aligned}
                \varphi_{i}(v)& =\sum_{S\subset N\setminus\{i\}}\frac{|S|!(|N|-|S|-1)!}{|N|!}(v(S\cup\{i\})-v(S))  \\
                &=\sum_{S\subset N\setminus\{i,j\}}\frac{|S|!(|N|-|S|-1)!}{|N|!}(v(S\cup\{i\})-v(S)) \\
                &+\sum_{S\subset N\setminus\{i,j\}}\frac{(|S|+1)!(|N|-(|S|+1)-1)!}{|N|!}(v(S\cup\{i,j\})-v(S\cup\{j\})) \\
                &=\sum_{S\subset N\setminus\{i,j\}}\frac{|S|!(|N|-|S|-1)!}{|N|!}(v(S\cup\{j\})-v(S)) \\
                &+\sum_{S\subset N\setminus\{i,j\}}\frac{(|S|+1)!(|N|-(|S|+1)-1)!}{|N|!}(v(S\cup\{i,j\})-v(S\cup\{i\})) \\
                &=\quad\varphi_j(v).
            \end{aligned}
        }
        Next, we need to prove the `only if' part, suppose $\psi$ is the value that satisfies the above axioms, we need to prove $\psi=\varphi$. 
    }
}

\subsubsection{The Bargaining Model}
\cite{Rubinstein1982} studies another equilibrium in the game. In the model, 

\subsection{Mechanism Design}

Mechanism design is a reverse engineering of game theory, it focuses on the problems associated with incentives and private information (selling, matching and auctions). Redefine an incomplete information game: $\Gamma=\langle[n],\{A_i\}_{i\in\mathbb{n}},\{T_i\}_{i\in\mathbb{n}},\Omega,g,\{u_i\}_{i\in\mathbb{n}}\rangle$: $n$ is the player, $A_i$ is the set of actions, $T_i$ is the player's type, $g$ is the outcome mapping the action to the outcome space $\Omega$, and $u$ is the utility function. There is a prior distribution on $T$: $F$. 
\defi{Social Choice Function}{
    A social choice function is $f:T\to\Omega$. A successful mechanism needs to satisfy:
    \[
        g(\mathbf{s}^*(\mathbf{t}))\in f(\mathbf{t})
    \]
    The \tbf{social welfare} can be defined as $\mathbb{E}_{\mathbf{t}\sim F}\left[W(g(\mathbf{s}^*(\mathbf{t}))\right]$.
}
A \tit{direct mechanism} is one in which the space of possible actions is equal to the space of possible types. All other mechanisms are called \tit{indirect}.
\defi{Incentive Compatibility}{
    A (direct) mechanism is BIC (\tbf{bayesian incentive compatibility}) iff truthtelling is a BayesNash equilibrium: i.e.,
    \eq{
        \underset{\mathbf{t}\sim F}{\operatorname*{E}}\left[u_i(g(t_i,\mathbf{t}_{-i});\mathbf{t})\right]\geq\underset{\mathbf{t}\sim F}{\operatorname*{E}}\left[u_i(g(t_i^{\prime},\mathbf{t}_{-i});\mathbf{t})\right],\quad\forall i\in[n],\forall t_i^{\prime}\in T_i.
    }
    Similarly, a mechanism is EPIC (\tbf{ex-post incentive compatibility}) iff truthtelling is a Nash equilibrium: i.e.,
    \eq{
        u_i(g(t_i,\mathbf{t}_{-i});\mathbf{t})\geq u_i(g(t_i^{\prime},\mathbf{t}_{-i});\mathbf{t}),\quad\forall i\in[n],\forall t_i^{\prime}\in T_i.
    }
    A mechanism is DSIC (\tbf{dominant strategy incentive compatibility}) iff:
    \eq{
        u_i(g(t_i,\mathbf{t}_{-i});\mathbf{t})\geq u_i(g(t_i^{\prime},\mathbf{t}_{-i});\mathbf{t}),\quad\forall i\in[n],\forall t_i^{\prime}\in T_i,\forall\mathbf{t}_{-i}\in T_{-i}.
    }
    \re{
        In a direct mechanism, EPIC is equivalent to DSIC.
    }
}
\thm{The Revelation Principle}{
    Given a mechanism $M$ for which there exists an equilibrium strategy profile $s$, we can construct an equivalent direct mechanism $M^∗$ in which truthtelling is likewise an equilibrium.
}

\clearpage
\section{Development Economics}

\subsection{Models of Development Economics}

\subsection{Clan Culture}

\defi{Clan Culture}{
    A clan is a consolidated kin group made up of component
    families that trace their patrilineal descent from a common ancestor.
}
\sep{History of Clans}
"Modern" clan originated in the Song Dynasty (860-1279 CE). At that time Neo-Confucian ideology was formed,
which provided the theoretical basis as well as clan organization structure design. The characteristics
of "modern" clan culture:
\lis{
    \item The families of a clan lived in the same or several nearby communities;
    \item Common properties and organized routine group activities, resource pooling;
    \item Compilation of genealogies;
    \item Own internal governance structures.
}
Currently, although China has been transitioning for a long time from a traditional society to a modern society, clan culture is still
prevalent and has a broad impact on the lives of Chinese people, especially in rural areas.

\cite{bertrand2006role} shows the positive correlation between the fraction of family control among listed firms and family
ties using cross-country level data.
\cite{cheng2021clan} uses IV (the minimum distance to two prominent neo-Confucian academies,
the Kaoting Academy (Kaoting Shuyuan) and the Xiangshan Academy (Xiangshan Shuyuan)) to identify that
clan culture causes higher firm ownership concentration.
\fig{clan indensity}{Clan Culture Intensity}{}
Potential reasons that culture affects the concentration of family ownership:
\lis{
    \item Clan culture fosters high trust within the family and low trust in outsiders(\textbf{short-radius trust attitude}).
    According to agency-cost-based theories,
    family ownership can be concentrated in such a situation.
    \item \textit{Resources Pooling}: commom property ownership.
    \item \textit{Amenity Potential}: other things constant, owners subject to stronger influences of clan culture
    could have a higher utility.
}
\cite{zhang2020clans} estimates the effect of the clan on entrepreneurship. He finds that clan leads to a higher
occurrence of entrepreneurship by helping overcome financing constraints and escape from local governments' "grabbing hand."
\cite{zhang2019family} investigates the relationship between the low take-up rate of social pensions and the clan culture intensity. In
his article, dummy variable $temple_c$ (whether community $c$ has ancestral temple) is constructed as the proxy variable for the strength of clan
culture. Some interesting insights are obtained:
\lis{
    \item Clan culture is positively related to adults raising children for support in their old age;
    \item Clan culture is associated with a larger number of children being born and a higher probability of having sons;
    \item Clan culture is associated with a higher coresidence rate between old parents and adult sons;
    \item Clan culture is associated with a higher likelihood of receiving financial transfers from non-coresident children;
    \item Clan culture is associated with a lower likelihood of participating in rural pension programs.
}
\cite{cao2022clans}
There is much research about clan culture outside of Mainland China. \cite{yang2019family} found the concave relationship the between the \textbf{heterogeneity}
of clan family and the provision of public goods. This finding implies that group homogeneity yields not only benefits, but also some possible costs.
re{Common Control Variables in Clan Research Identification(Individual Level):
        \lis{
            \item \textit{Hukou} Status;
        }
        Regional Level:
        \lis{
            \item Distance to the sea;
        }}
\sep{Data resources in Clan Research}
\re{\tab{
        \item \textit{Comprehensive Catalogue of the Chinese Genealogy} can be used to construct the strength of clan culture;
        \item
    }}

\clearpage
\section{Reduced-Form Identification}
The main contents of this chapter are the notes of \cite{angrist2014mastering}, \cite{angrist2009mostly}.

\subsection{Counterfactual Framework \& Identification}

\key{ATT, ATE, Counter-factual World, Potential Outcome}
In the theory of the Neyman-Rubin Causal model, the outcome is $Y_i$, the potential outcome is $Y_{1i},Y_{0i}$:
\eq{
    Y_i=\begin{cases}Y_{1i}\quad&ifD_i=1
    \\Y_{0i}\quad&ifD_i=0\end{cases}
}
\eq{
    Y_i=Y_{0i}+(Y_{1i}-Y_{0i})D_i
}
\textit{Naive Comparison}:
\eq{
    \begin{aligned}
        \{\mathbf{E}[Y_i|D_i=1]-\mathbf{E}[Y_i|D_i=0]\} & =\{\mathbf{E}[Y_{1i}|D_i=1] - \mathbf{E}[Y_{0i}|D_i=1]\}                                                     
        \\&+\{\mathbf{E}[Y_{0i}|D_i=1]-\mathbf{E}[Y_{0i}|D_i=0]\}
    \end{aligned}
}
\tab{
    \item observed difference: $\mathbf{E}[Y_i|D_i=1]-\mathbf{E}[Y_i|D_i=0]$;
    \item ATE: $\mathbf{E}[Y_{1i}]-\mathbf{E}[Y_{0i}]$;
    \item ATT: $\mathbf{E}[Y_{1i}|D_i=1]-\mathbf{E}[Y_{0i}|D_i=1]$;
    \item selection bias: $\mathbf{E}[Y_{0i}|D_i=1]-\mathbf{E}[Y_{0i}|D_i=0]$.
}
The conditional average treatment effect (CATE) is defined as:
\eq{
    \tau_{CATE}(x)=\mathbb{E}(\tau_i|X_i=x)=\mathbb{E}(Y_{1i}-Y_{0i}|X_i=x),
} 
where $X_i$ is some additional characteristic. Note that the definition of ATE is on the whole population, including those who don't receive the treatment. The existence of the selection bias is due to the dependence between $D_i$ and the \textbf{potential} outcome $Y_{1i}, Y_{0i}$. The relationship between CATE and ATE is:
\meq{
    \tau_{ATE}=\int\tau_{\mathsf{C}ATE}(x)f(x)dx, \\
    \tau_{ATE}=\sum_{x\in\mathcal{X}}\tau_{CATE}(x)Pr(X_i=x).
}
\ass{\tbf{SUTVA} (Stable Unit Treatment Value Assumption)}{
    For two treatment vector $\tbf{D}$ and $\tbf{D}^\prime$, if $\tbf{D}_i=\tbf{D}^\prime_i$, then:
    \eq{
        Y_{i}(\mathbf{D})=Y_{i}(\mathbf{D}^{\prime}).
    }
    It means that the outcome is only affected by own treatment status, and not the treatment status of others. SUTVA can be relaxed in many ways.
}
\re{
    \tab{
        \item \tbf{Estimand}: the quantity to be estimated;
        \item \tbf{Estimate}: the approximation of the estimand using a finite data sample;
        \item \tbf{Estimator}: the method or formula for arriving at the estimate for an estimand.
    }
}
\defi{Identification}{
    \emocool Estimate the estimand from the data we observe.
}

\subsubsection{Randomization}
Randomization can make $D_i \perp Y_{1i},Y_{0i}$:
\meq{
    E[Y_{1i}|D_i=1]=E[Y_{1i}|D_i=0]=E[Y_{1i}]\\
    E[Y_{0i}|D_i=1]=E[Y_{0i}|D_i=0]=E[Y_{0i}]
}
\no So selection $=\mathbf{E}[Y_{0i}|D_i=1]-\mathbf{E}[Y_{0i}|D_i=0]=0$. \emocool Besides, by randomization, $ATT=ATE$.
Randomization example:
\tab{
    \item Rand HIE experiment: whether the insurance program makes people healthier;
    \item STAR: the effects of class size on education;
    \item OHP: this group experiment is not perfect because the group is not a determinant of whether to receive the treatment, but the treatment group does have
    a higher probability to get the treatment (\textbf{Instrumental Variable} can handle this situation);
}
\re{\tab{
        \item By randomization, the individual differences still exist;
        \item Checking for balance is an important step in randomization;
        \item The most critical idea of randomization is \textbf{Other Things Equal}(\textit{ceteris paribus});
        \item Randomization was invented by \textit{Ronald Aylmer Fisher} in 1925.
    }}

\subsection{Regression and Matching}
\key{
    \tab{
        \item CEF, CEF decomposition, ANOVA;
        \item regression justification,
    }
}
\defi{Strong Ignorability}{
    We say that $D_i$ is strongly ignorable conditional on a vector $\mathbf{X}_i$ if:
    \lis{
        \item $Y_i(0),Y_i(1)\perp D_i|\mathbf{X}_i$;
        \item \exists\varepsilon>0\text{such that }\varepsilon<Pr(D_i=1|\mathbf{X}_i)<1-\varepsilon.
    } 
}
The first part of the definition is sometimes referred to \tit{unconfoundeness} or \tit{exogeneous}. This is a powerful tool, note that under the strong ignorability:
\eq{
    \mathbb{E}(Y_i(0)|\boldsymbol{X}_i)=\mathbb{E}(Y_i(0)|D_i=0,\boldsymbol{X}_i)=\mathbb{E}(Y_i|D_i=0,\boldsymbol{X}_i).
}
This means that \tbf{we can interchange counterfactuals and realized data in conditionals}. 
\thm{Identification of the ATE}{
    If $D_i$ is strongly ignorable conditional on $\mathbf{X}_i$, then
    \eq{
        \operatorname{E}(\tau_i)=\sum_{x\in\operatorname{Supp}X_i}\left(\operatorname{E}(Y_i|D_i=1,\mathbf{X}_i=x)-\operatorname{E}(Y_i|D_i=0,\mathbf{X}_i=x)\right)Pr(\mathbf{X}_i=x)
    }  
}
\no Conditional Expectation Function (CEF) is a \hl{population} concept:
\eq{\begin{gathered}
        E[Y_i|X_i=x]=\int tf_y(t|X_i=x)dt \\
        E[Y_{i}|X_{i}=x]=\sum_{t}tP(Y_{i}=t|X_{i}=x)
    \end{gathered}}
\lem{The law of iterated expectations}{
    \eq{E\left[\mathrm{y}_i|\mathrm{X}_i=x\right]=\int tf_{\boldsymbol{y}}\left(t|\mathrm{X}_i=x\right)dt.}
    \proo{}{
        \eq{\begin{aligned}
                E\{E\left[\mathrm{y}_{i}|\mathrm{X}_{i}\right]\} & =\quad\int E\left[\mathrm{y}_{i}|\mathrm{X}_{i}=u\right]g_{x}(u)du                                                                                                                    \\
                                                                    & =\quad\int\left[\int tf_{\boldsymbol{y}}\left(t|\mathrm{X}_{i}=u\right)dt\right]g_{\boldsymbol{x}}(u)du                                                                               \\
                                                                    & \text{=} =\int\int tf_{y}\left(t|\mathrm{X}_{i}=u\right)g_{x}(u)dudt                                                                                                                  \\
                                                                    & =\quad\int t\left[\int f_{\boldsymbol{y}}\left(t|\mathrm{X}_{i}=u\right)g_{\boldsymbol{x}}(u)du\right]dt=\int t\left[\int f_{\boldsymbol{x}\boldsymbol{y}}\left(u,t\right)du\right]dt \\
                                                                    & =\quad\int tg_{y}(t)dt.
            \end{aligned}}
    }
}
\no \emoheart \hl{3 important property of CEF}:
\thm{CEF Decompostion Property}{
\eq{Y_i=E[Y_i|X_i]+\epsilon_i}
where $\epsilon_i$ is mean independent of $X_i$, and $X_i$ is uncorrelated with any function of $X_i$.
\proo{}{
Take the expectation of $X_i$ at both sides:
\eq{
    \begin{align}
        E[Y_i|X_i]        & = E[E[Y_i|X_i]|X_i]+E[\epsilon_i|X_i] \\
        E[\epsilon_i|X_i] & = E[Y_i|X_i] - E[Y_i|X_i]  = 0
    \end{align}
}
\eq{E[\epsilon_i]=\int_{X_i}f_x(t)E[\epsilon_i|X_i]dt=\int_{X_i}0dt=0=E[epsilon_i|X_i]}
}
}
\no \hl{\textbf{This means that $Y_i$ can be decomposed into 2 parts: explaind by $X_i$ and terms uncorrelated with $X_i$.}}
\thm{CEF Prediction Property}{
    CEF is the best estimator of $Y_i$ in the MMSE sense, which means:
    \eq{E\left[\mathrm{y}_i|\mathrm{X}_i\right]=\arg\min_{m(\mathrm{X}_i)}
        E\left[\left(\mathrm{Y}_i-m\left(\mathrm{X}_i\right)\right)^2\right]}
    \proo{}{
        \eq{
            \begin{array}{rcl}\left(\mathrm{Y}_i-m\left(\mathrm{X}_i\right)\right)^2&=&\left(\left(\mathrm{Y}_i-E\left[\mathrm{Y}_i|\mathrm{X}_i\right]\right)+\left(E\left[\mathrm{Y}_i|\mathrm{X}_i\right]-m\left(\mathrm{X}_i\right)\right)\right)^2\\&=&\left(\mathrm{Y}_i-E\left[\mathrm{Y}_i|\mathrm{X}_i\right]\right)^2+2\left(E\left[\mathrm{Y}_i|\mathrm{X}_i\right]-m\left(\mathrm{X}_i\right)\right)\left(\mathrm{Y}_i-E\left[\mathrm{Y}_i|\mathrm{X}_i\right]\right)\\&&+\left(E\left[\mathrm{Y}_i|\mathrm{X}_i\right]-m\left(\mathrm{X}_i\right)\right)^2\end{array}
        }
        The formula has the lowest constant value by setting $m\left(\mathrm{X}_i\right)=$ CEF.
    }}
\thm{ANOVA Theorem}{
\eq{\operatorname{V}(Y_i)=E[\operatorname{V}(Y_i|X_i)]+\operatorname{V}(E[Y_i|X_i])}
}
\no This indicates that the variance of $Y_i$ can be decomposed into two parts:
\lis{
    \item the variance of the CEF;
    \item the variance of the residual;
}
\re{\tab{
        \item The CEF property \hl{dosen't rely on any assumption}! It has nothing to do with regression right now;
        \item If $X_i$ is not mean independent of $Y_i$, then by ANOVA theorem, the variance of the outcome variable controlled
        by $X_i$ could be smaller;
    }}
\sep{CEF and (Population) Regression}
\eq{\begin{aligned}\beta&=\arg\min_bE[(Y_i-X_i'b)^2]\\1storder&:E[X_i(Y_i-X_i'b)]=0\\solution&:\beta=E[X_iX_i']^{-1}E[X_iY_i]\end{aligned}}
\thm{Regression Anatomy}{
\eq{\beta_k = \frac{Cov(Y_i,\tilde{X_{ki}})}{V(\tilde{X_{ki}})}}
\co{Bivariate Case}{
    \eq{\beta = \frac{Cov(Y_i,\tilde{X_i})}{V(\tilde{X_{i}})}}
}
\proo{}{Substitute\eq{
    Y_i=\alpha+\beta_1x_{1i}+\cdots+\beta_kx_{ki}+e_i
}
$\tilde{x}_{ki}$ is uncorrelated with $e_i$ and other covariates by construction, thus $Cov(\tilde{x}_{ki},x_{ki})=Var(\tilde{x}_{ki})$,
thus $Cov(Y_i,\tilde{x_{ki}}=\beta_k x_{ki})$.
}
\re{The regression anatomy shows that each $\beta_k$ in multi-regression is the bivariate slope after "partialing out"
    all the other regressors.}
}
\emogood Why the population regression coefficient is what we are interested in (Link with CEF):
\thm{Regression Justification}{\lis{
        \item Suppose the CEF is linear, then the population regression function is it;
        \item In any condition, $X^'_i\beta$ is the best predictor of $Y_i$ in a MMSE sense;
        \item The function $X^'_i\beta$ provides the MMSE linear approximation to $E[Y_i|X_i]$.\\
        \eq{\beta=\arg\min_bE\{(E[\mathrm{Y}_i|\mathrm{X}_i]-\mathrm{X}_i'b)^2\}}}
    \proo{}{Suppose $E[\mathrm{Y}_i|\mathrm{X}_i]=X_i^'\beta^{*}$. By regression decomposition theorem:
        \eq{\begin{align}
                E[X_i(Y_i-X_i'\beta^{*})]                             & = 0             \\
                \beta^{*}                  = E[X_iX_i']^{-1}E[X_iY_i] & = \tilde{\beta}
            \end{align}
        }
        \eq{
            \begin{aligned}
                \left(\mathrm{Y}_{i}-\mathrm{X}_{i}^{\prime}b\right)^{2} =\quad\{(\mathrm{y}_i-E[\mathrm{y}_i|\mathrm{X}_i])+(E[\mathrm{y}_i|\mathrm{X}_i]-\mathrm{X}_i^{\prime}b)\}^2 \\
                =\quad(\mathrm{y}_i-E[\mathrm{y}_i|\mathrm{X}_i])^2+(E[\mathrm{y}_i|\mathrm{X}_i]-\mathrm{X}_i^{\prime}b)^2                                                            \\
                +2(\mathrm{y}_i-E[\mathrm{y}_i|\mathrm{X}_i])(E[\mathrm{y}_i|\mathrm{X}_i]-\mathrm{X}_i^{\prime}b).
            \end{aligned}
        }}
    \co{}{\tab{
            \item For the saturated model, the population linear regression is the CEF;
            \item For the single dummy variable, the coefficient is the mean probability of receiving treatment;
        }

    }
}

\sep{From Regression to Causality}

\subsection{Instrumental Variables}

Consider the demand equilibrium model:
\eq{
    \begin{cases}
        q_t^d=\alpha_0+\alpha_1p_t+u_t&\text{(Demand)}\\
        q_t^s=\beta_0+\beta_1p_t+v_t&\text{(Supply)}\\
        q_t^d=q_t^s&\text{(Equilibrium)}
   \end{cases}
}

\subsubsection{Local Average Treatment Effect}
\lis{
    \item the \tit{instrument}: $Z_i$;
    \item the \tit{treatment}: $D_i$;
    \item the \tit{outcome}: $Y_i$.
}
The IV chain can lead us to the definition of LATE: the \tbf{first-stage} $\phi=E[D_i|Z_i=1]-E[D_i|Z_i=0]$; the \tbf{reduced-form} $\rho=E[Y_i|Z_i=1]-E[Y_i|Z_i=0]$. 
The LATE is defined as $\lambda=\frac{\rho}{\phi}=\frac{E[Y_i|Z_i=1]-E[Y_i|Z_i=0]}{E[D_i|Z_i=1]-E[D_i|Z_i=0]}$, which is the ratio of the reduced form to the first stage. 
\begin{center}
    LATE is the average causal effect of $D$ on $Y$ for those whose status $D$ is determined \tbf{solely} by $Z$ (\tit{compliers}), but only under the monotonicity assumption: there is no \tit{defier} in the sample.
\end{center}
\eq{
    \lambda = \frac{\rho}{\phi} = E[Y_{1i}-Y_{0i}|C_i=1]
}
To calculate the standard error of the 2SLS parameter, one can regress 
\[
    \eta_i=Y_i-\alpha_2-\lambda_{2SLS}D_i-\gamma_2A_i.    
\]
The standard error is given by:
\eq{
    SE(\hat{\lambda}_{2SLS})=\frac{\sigma_n}{\sqrt{n}}\times\frac{1}{\sigma_{\hat{D}}}
}

\subsubsection{2SLS}
Compared to computing LATE by hand, 2SLS (2-stage-least-squares) has two advantages:
\lis{
    \item 2SLS can use multiple IVs simultaneously;
    \item 2SLS can control for covariates.
}
\eq{
    \begin{cases}
        \hat{D}_i = \alpha_1+\phi Z_i+\gamma_1 A_i & \text{first-stage} \\
        Y_i=\alpha_2+\lambda_{2SLS}\hat{D}_i+\gamma_2A_i+e_{2i} & \text{second-stage}
    \end{cases}
}

\subsubsection{Validity of IV}
\begin{center}
    \tit{If you can not see it in the reduced form, it ain't there.}
\end{center}

\subsection{Asymptotic Analysis}

\subsection{Empirical Classics}

\subsubsection{Returns to Schooling}
It's been a long history to study the effects of education on the salary. Counterfactuals here are multi-faceted: instead of the
single contrast $Y_{1i}-Y{0i}$, it is more often to study every possible schooling choice $Y_{n,i}-Y_{m,i}$, where $n,m$ are the
years of education.
\cite{mincer1974schooling} uses U.S. census data to do the following regression:
\eq{
    \ln Y_i = \alpha+\rho S_i+\beta_1 X_i +\beta_2 X_i^2+e_i,
}
where $X_i=Age_i-S_i-6$, defined as the \tit{potential experience}, and $S_i$ is the year of education. The result shows that one-year more eduction would bring 10.7\% more earnings.\\
This result may be biased because it doesn't consider an individual's ability, which is correlated both with earnings and education.\\
To handle the ability bias, \cite{griliches1977estimating} includes IQ as a control variable to eliminate the influence of personal
ability, the new $\rho$ estimated is 5.9\%, which is smaller than expected.\\
In the process of handling ability bias, be aware of the bad control: which is controlling the occupation. This is because the occupation
is a variable \tit{after} the education years, and some effect of the education is through occupation, controlling occupation would
underestimate the effect. \\
The above regressions don't consider other variables, one approach to handle this is by comparing the twins
(\cite{ashenfelter1994estimates}, \cite{ashenfelter1998income}), since each person of the twins is almost identical in IQ,
family background, etc. The long regression is written as:
\eq{
    \ln Y_{i,f} = \alpha+\rho S_{i,f}+\lambda A_{i,f}+e_{i,f},
}
where subscript $i,f$ stand for individual and family respectively, $i=1,2$ indexes twin siblings. If we consider the regression within a family:
\eq{
    \ln Y_{1,f} = \alpha + \rho S_{1,f} +\lambda A_f +e_{1,f}\\
    \ln Y_{2,f} = \alpha + \rho S_{2,f} +\lambda A_f +e_{2,f}
}
Subtracting them would lead to:
\eq{
    \ln Y_{1,f}-\ln Y_{2,f}=\rho (S_{1,f}-S_{2,f}) + e_{1,f}-e_{2,f}
}
This regression gives an estimation of 6.2\% for $\rho$. But in practice, the collected data encounters the misreported issue, which can bring the \tbf{Attenuation bias}.
\proo{Attenuation Bias}{
    $C(\cdot)$ denotes the covariance among two variables.
    Consider the regression:
    \eq{
        Y_i=\alpha+\beta S^\star_i + e_i
    }
    But the data for $S^\star_i$ is misreported, the observed $S_i=S^\star_i+m_i$, where we assume that $\bE[m_i]=0,C(S^\star_i,m_i)=0$.
    We assume the real parameter $\beta=\frac{C(Y_i,S^\star_i)}{V(S^\star_i)}$, and the biased parameter is $\beta_b=\frac{C(Y_i,S_i)}{V(S_i)}$:
    \eq{
        \begin{aligned}
            \beta_b & = \frac{C(Y_i,S_i)}{V(S_i)}                                           \\
                    & = \frac{C(\alpha+\beta S^\star_i+e_i, S_i^\star+m_i)}{V(S_i)}         \\
                    & = \frac{C(Y_i, S_i^\star)}{V(S_i)}=\beta \frac{V(S^\star_i)}{V(S_i)}.
        \end{aligned}
    }
    Because $V(S_i)=V(S^\star_i)+V(m_i)$, which means $\beta_b=r\beta$, where
    \eq{
        r(V(S^\star_i))=\frac{V(S^\star_i)}{V(S^\star_i)+V(m_i)}<1.
    }
    \co{Adding Covariates Can Exacerbate Attenuation Bias}{
        When adding covariates $X_i$, we would get:
        \eq{
            \beta_b=\frac{V(\tilde{S_i^\star})}{V(\tilde{S_i^\star})+V(m_i)}\beta=r(V(\tilde{S_i^\star}))\beta
        }
        Be aware that $V(\tilde{S_i^\star})<V(S^\star_i)$ because $V(S^\star_i)=V(\gamma X_i + \tilde{S_i^\star})$. And $r(\cdot)$ is a decreasing function, so adding
        covariates would exacerbate attenuation bias.
    }
}

\subsection{Structural Equations}

Consider the following economic model:
\eq{
    Y_{i} = \alpha + \beta D_{i} + \varepsilon_{i}.
}
Using the potential outcome framework, we can obtain:
\eq{
    \begin{align*}
        Y_{i} &= Y_{i}(0)(1 - D_{i}) + Y_{i}(1)D_{i}\\
        &= Y_{i}(0) + \tau_{i}D_{i}\\
        &= Y_{i}(0) + \tau D_{i} + (\tau_{i} - \tau)D_{i}\\
        &= \underbrace{E(Y_{i}(0) | D_{i} = 0)}_{\alpha} + \underbrace{\tau}_{\beta} D_{i} + \underbrace{(\tau_{i} - \tau)D_{i} + (Y_{i}(0)-E(Y_{i}(0) | D_{i} = 0))}_{\varepsilon_{i}}
    \end{align*}
}
We can observe $E(Y_i|D_i)$, which means we can recover:
\tab{
    \item $E(Y_{i} | D_{i} = 1) = \alpha + \tau + E(\varepsilon_{i} | D_{i} = 1)$, where $E(\varepsilon_{i} | D_{i} = 1) = (E(\tau_{i} | D_{i}=1) - \tau)  + E(Y_{i}(0) | D_{i} = 1) -  E(Y_{i}(0) | D_{i} = 0)$;
    \item $E(Y_{i} | D_{i} = 0) = \alpha + E(\varepsilon_{i} | D_{i} = 0)$, where $E(\varepsilon_{i} | D_{i} = 0) = 0$
}
We can obtain an estimation on $\beta$ if $D_i$ is strongly ignorable: $E(\varepsilon_{i} | D_{i} = 1)-E(\varepsilon_{i} | D_{i} = 0)=0$. 
\begin{center}
    There are one-to-one mappings between the potential outcome framework and structural regressions.
\end{center}

\clearpage
\chapter{Data Science}

\section{Machine Learning}

\subsection{Imitation Learning}

\sep{DAGGER}
Imitation learning, which learns from the demonstration, is a kind of supervised learning. Generally, experts provide a set of demonstration trajectories, which are sequences of states
and actions. Assume we know:
\tab{
    \item State space and action space;
    \item Access to transition oracle $\mathbb{P}(s^{\prime}\mid s,a)$;
    \item Set of one or more teacher demonstrations $(s_0,a_0,s_1,a_1,\ldots)$ where actions are drawn from the teacher's policy $\pi^\star$.
}
\fig{DAGGER}{DAGGER}{0.7}
The $\pi_i$ (suboptimal policy) trick is designed to bootstrap more data, generalizing the learning outcome.

\clearpage
\section{Deep Learning}

\subsection{Neural Networks}
Non-linear classifiers: $f(\sum_i w_ix_i+b)$, the $f$ is called the \tbf{activation function}. Each non-linear classifier 
(with $n$ inputs and $1$ output) is a \tbf{neuron}.
\lis{
    \item Sigmoid(Logistic): $\sigma(z)=\frac1{1+\exp(-z)}$;
    \item Tanh: $\tanh(z)=\frac{\exp(z)-\exp(-z)}{\exp(z)+\exp(-z)}$;
    \item ReLU: $\mathrm{ReLU}(z)=\max(0,z)$.
}
\re{
    The properties of the tanh activation function:
    \tab{
        \item derivative: $\frac d{dx}\tanh(x)=1-\tanh^2(x)$;
        \item Output between -1 and 1, derivative output between 0 and 1 (larger than sigmoid, accelerate GD).
        \item For sigmoid: $\frac d{dx}\sigma(x)=\sigma(x)(1-\sigma(x))$, this function has an output range within $(0,\frac{1}{4}]$.
    }
}
The simplest neural network is a multi-layer perceptron with one hidden layer. A neural network with at least one hidden layer is a universal approximator
(can represent any continuous function). Shallow network can represent any function, but using deep structure is more effective: the hidden layer is a kind of 
modularization: and the modularization is automatically learned from data.

\subsubsection{Forward Pass: Inference}
\fig{Example of Forward Pass in Python}{Example of Forward Pass in Python}{0.7}
$W_1$ is a $4\times 3$ matrix and $W_2$ is a $4\times 4$ matrix.
\subsubsection{Back Propagation: Learning and Optimization}
The first step is to set an appropriate loss function: $L=\cL(X,Y,\theta)$, where $X$ and $Y$ are known from the samples and $\theta$ is the parameter to learn. 
Common loss functions include cross-entropy and MSE loss.
\fig{2D Neuron Backpropagation}{2D Neuron Backpropagation}{0.5}
\subsubsection{Initialization and Regularization}
\sep{Data Preprocessing}
Setting all weights of an NN to the same constant can not work, because, for all neurons at the same layer, the gradient is the same, thus the update is the same, and it can not 
learn different features. For small networks, it is OK to set $W_0=0.01*random.rand()$. XAvier initialization aims to make the variance across every layer is the same, 
preventing gradient explosion or diminishing:
\[w_{1,i}\overset{\mathrm{iid}}{\operatorname*{\sim}}N\left(0,\frac1d\right)\]
where $d$ is the number of inputs for each neuron. Another initialization note is to scale the features.\\
\tbf{early stopping} is a regularization technique, using a validation set to validate the error each time after the update when the validation error doesn't improve, return the stored weights. 
\tbf{Dropout} keeps a neuron active with probability $\alpha$, or setting it to zero otherwise. 

\clearpage
\section{Causal Inference with Machine Learning}

\subsection{Foundations of Causal Inference}

\cite{peters2017elements} provide an overview of causal inference from a fundamental science perspective. \cite{spirtes2010introduction} summarize the development of causal inference and machine learning interface from a computer science perspective.\\
There are two main causal frameworks: \tbf{Structural Causal Model} and \tbf{Potential Outcome Framework}.

\subsubsection{Directed Acyclic Graphs (DAG)}
We can encode the relationship between $D$ and $Y$ using an \tit{arrow} in a graph.  
\begin{figure}[H]
    \begin{center}
        \begin{tikzpicture}
            \node[text centered] (t) {$D$};
            \node[right=1.5 of t, text centered] (y) {$Y$};
            \draw [->, line width= 1] (t) -- (y);
        \end{tikzpicture}
        \caption{$D$ has a causal effect on $Y$}
    \end{center}
\end{figure}
When $D$ and $Y$ are both caused by some \tbf{confounder} $U$, the number of paths between $D$ and $Y$ has more than one:
\lis{
    \item The standard direct effect $D \rightarrow Y$;
    \item The \tbf{back door} path $D \leftarrow U \rightarrow Y$.
}
\begin{figure}[H]
    \begin{center}
        \begin{tikzpicture}
        \node[text centered] (t) {$D$};
        \node[right=1.5 of t, text centered] (y) {$Y$};
        \node[draw, rectangle, dashed, above = 1 of t, text centered] (u) {$U$};
        \draw [->, line width= 1] (t) -- (y);
        \draw[->,line width= 1] (u) --(t);
        \draw[->,line width= 1] (u) -- (y);
        \end{tikzpicture}
    \end{center}
    \caption{$D$'s effect on $Y$ is confounded by $U$}
\end{figure}
When $U$ is a control variable $X$ rather than a confunder, controlling $X$ can block the backdoor path.
\fig{Structural Causal Model}{Structural Causal Model}{0.7}
\fig{Front-Door Criteria}{Front-Door Criteria}{0.6}

\clearpage
\section{Reinforcement Learning}

This section is mainly the notes of \cite{thrun2000reinforcement} and \cite{agarwal2019reinforcement}.
\fig{A Taxonomy of RL Algorithms}{A Taxonomy of RL Algorithms}{1}

\subsection{Introduction and MDP}

\key{\tab{
        \item Exploration, Exploitation, Reward, Policy, Value Function;
        \item
    }}

\no The four elements of reinforcement learning:
\lis{
    \item \textit{Policy}: agent's way to interact with the environment;
    \item \textit{Reward Signal}: on each time step, the environment sends to the agent a single number;
    \item \textit{Value Function}: specify what is good in the long run, which is the discount value of the rewards;
    \item \textit{Model}: mimics behaviors of the environment;
}

\re{"Deadly Trials"\lis{
        \item The balance between \textbf{Exploration} and \textbf{Exploitation};
        \item Reinforcement learning is difficult to generalize;
        \item Delayed consequences may cause RL algorithm to perform poorly.
    }}

\no \textbf{MDP} is a mathematical framework to model \tit{discrete-time} sequential decision process, denoted by the tuple
$(\cS,\cA,\cT,\cR,\rho_0,\gamma)$:
\tab{
    \item $\cS$: the state space, which is the states for the \tbf{entire} environment(MOBA games may can not directly be modeled by MDP);
    \item $\cA$: the action space. $\cA$ can depend on $s \in \cS$;
    \item $\cT:\cS\times\cA\to\Delta(\cS)$:the environment transition probability function;
    \item $\cR:\cS\times\cA\to\Delta(\RR)$: the reward function;
    \item $\rho_0\in\Delta(\cS)$: the initial state distribution;
    \item $\gamma \in [0,1]$ is the discount factor.
}
The goal is to optimize:
\eq{
    \mathbb{E}_{s_t,a_t,r_t,t\geq0}\left[R_0\right]=\mathbb{E}_{s_t,a_t,r_t,t\geq0}\big[\sum_{t=0}^{\infty}\gamma^tr_t\big]
}
\re{
    \tab{
        \item The $\Delta(\cdot)$ may not be deterministic, but some random distribution;
        \item Among the above tuple, $\cS,\cT,\cR,\rho_0$ can not be modified by the agent, to train a good policy, $\cA,\gamma$ is the key;
        \item RL is more like infants rather than adults;
        \item The reward function is the way of communicating with the agent \tit{what} to do, not \tit{how} to do;
        \item The \tit{trajectory} of the MDP sequence: $S_0,A_0,R_1,S_1,A_1,\cdots$.
    }
}
\fig{agent-environment interaction}{The agent-environment interaction in a Markov decision process}{0.3}
\thm{Dynamics of MDP}{\meq{
p(s',r|s,a)\doteq\Pr\{S_t=s',R_t=r\mid S_{t-1}=s,A_{t-1}=a\}\\
\sum_{s^{\prime}\in\mathcal{S}}\sum_{r\in\mathcal{R}}p(s^{\prime},r|s,a)=1,\text{ for all }s\in\mathcal{S},a\in\mathcal{A}(s)
}}
\co{Some formula derived from the dynamics theorem}{
\eq{
p(s'|s,a)\doteq\Pr\{S_t=s'\mid S_{t-1}=s,A_{t-1}=a\}=\sum_{r\in\mathbb{R}}p(s',r|s,a)
}
\eq{
r(s,a)\doteq\mathbb{E}[R_t\mid S_{t-1}=s,A_{t-1}=a]~=~\sum_{r\in\mathbb{R}}r\sum_{s'\in\mathcal{S}}p(s',r|s,a)
}
\eq{
r(s,a,s')~\doteq~\mathbb{E}[R_t\mid S_{t-1}=s,A_{t-1}=a,S_t=s']~=~\sum_{r\in\mathbb{R}}r\frac{p(s',r|s,a)}{p(s'|s,a)}
}
}
\defi{Some useful function:}{
    The act value function given policy $\pi$:
    \eq{Q^{\pi}(s,a)=\mathbb{E}_{s_t,a_t,r_t,t\geq0}\big[\sum_{t=0}^{\infty}\gamma^tr(s_t,a_t)\mid s_0=s,a_0=a\big]}
    The expected return at state $s$ given policy $\pi$:
    \eq{V^\pi(s)=\mathbb{E}_{a\sim\pi(s)}\left[Q^\pi(s,a)\right]}
    The \tbf{advantage function}:
    \eq{A^\pi(s,a)=Q^\pi(s,a)-V^\pi(s)}
    The temporal-difference error:
    \eq{\delta_t=r_t+\gamma V(s_{t+1})-V(s_t)}
}
If we denote $G_t=R_{t+1}+\gamma G_{t+1}$, then we have:
\thm{Bellman Equation}{
    \eq{
        \begin{aligned}
            v_{\pi}(s) & \doteq\mathbb{E}_\pi[G_t\mid S_t=s]                                                                                   \\
                        & =\mathbb{E}_\pi[R_{t+1}+\gamma G_{t+1}\mid S_t=s]                                                                     \\
                        & =\sum_a\pi(a|s)\sum_{s^{\prime}}\sum_rp(s^{\prime},r|s,a)\Big[r+\gamma\mathbb{E}_\pi[G_{t+1}|S_{t+1}=s^{\prime}]\Big] \\
                        & =\sum\pi(a|s)\sum p(s^{\prime},r|s,a)\Big[r+\gamma v_\pi(s^{\prime})\Big],\quad\text{for all }s\in\mathcal{S},
        \end{aligned}
    }
    If we consider a $n$-state game, assume the reward $r\in \RR^n$ is deterministic. Then we denote $P$ as the transition matrix for a corresponding strategy and $V\in\RR^n$ as the value function vector. Finally, we can get the Bellman equation in matrix form:
    eq{
            V=r+\gamma PV.
        }
}
A policy $\pi$ is better $\pi'$ if and only if $v_{\pi}(s)\le v_{\pi'}(S) \text{for all } s \in \cS$. There is always at list one \tit{optimal policy} denoted by $\pi_*$ (doesn't hold for partially observed MDP). They share the same state-value function, called the \tit{optimal state-value function}: $v_*(s)\doteq\max_\pi v_\pi(s)$. The optimal policy also shares the same \tit{optimal action-value function}: $q_*(s,a)\doteq\max_{\pi}q_\pi(s,a)=\mathbb{E}[R_{t+1}+\gamma v_*(S_{t+1})\mid S_t=s,A_t=a]$. Then we can get the \tbf{Bellman optimality function} in an action-value function sense:
\thm{Bellman optimality function}{
    \eq{
        \begin{gathered}
            q_{*}(s,a) \begin{array}{rl}{=}&{\mathbb{E}\Big[R_{t+1}+\gamma\max_{a^{\prime}}q_{*}(S_{t+1},a^{\prime})\Big|S_{t}=s,A_{t}=a\Big]}\end{array} \\
            =\quad\sum_{s^{\prime}.r}p(s^{\prime},r|s,a)\Big[r+\gamma\operatorname*{max}_{a^{\prime}}q_{*}(s^{\prime},a^{\prime})\Big].
        \end{gathered}
    }
}
\fig{different reinforcement learning agents}{Classification of different reinforcement learning agents}{0.25}
\defi{The optimal policy $\pi^*$}{A policy $\pi^*$ is an optimal policy if for every policy $\pi$ and every $s\in\cS$, we have:
    \eq{V^{\pi^*}(s)\geq V^{\pi}(s)}
    \re{
        \tab{
            \item If we have the full information of the game (MDP framework), then the optimal policy is always deterministic;
            \item The optimal policy is always stochastic when there are \tbf{minimax} structure (e.g. protection information);
            \item Whether the optimal policy is stochastic or deterministic has nothing to do with the stochasticity of the game.
        }
    }}

\subsection{Bandit Algorithms}

The regret for bandit games is defined as :
\eq{
    \overline{R}_t=\sum_{i=1}^m\mathbb{E}[N_{t,i}]\Delta_i
}
Where $N_{t,i}=\sum_{t^{\prime}=0}^{t}\mathbb{1}\{a_{t^{\prime}}=i\}$ and $\Delta_{i}=\mu^{*}-\mu_{i}$.\\
\sep{Greedy Algorithms}
\fig{the greedy algorithm}{The greedy algorithm}{}
This algorithm achieves a regret at most $O(T)$.\\
\sep{The $\epsilon$-greedy algorithm}
\fig{The epsilon greedy algorithm}{The epsilon greedy algorithm}{}
The lower bound of $\epsilon$ greedy: $\overline{R}_t\geq\frac1m(\Delta_2+\cdots+\Delta_m)\varepsilon(T-m)$, where $\epsilon\le \epsilon_t$ for all $t$;
\thm{The upper bound of $\epsilon$ greedy}{
    \overline{R}_T\leq C'\sum_{i\geq2}\left(\Delta_i+\frac{\Delta_i}{\Delta_{\mathrm{min}}^2}\log\max\left\{e,\frac{T\Delta_{\mathrm{min}}^2}m\right\}\right)
    \proo{}{
        This proof contains two parts: part 1 is about the cost of exploration, and part 2 is about the suboptimal condition during exploitation.\\
        First, the exploration: $\overline{R}_t=\frac1m(\Delta_2+\cdots+\Delta_n)\epsilon$, by setting $\epsilon_t$ = $\frac{1}{t}$: $\overline{R}_t=\frac1m(\Delta_2+\cdots+\Delta_m)O(\log T)$.\\
        Then prove the probability of selecting the suboptimal arms is very thin.
    }
}
\sep{The explore-then-commit algorithm (ETC)}
\fig{The ETC algorithm}{The ETC algorithm}{}
This algorithm has an upper bound of $O(\Delta^2 log T)$, or $O(T^{\frac{2}{3}})$.
\sep{The UCB algorithm}
\fig{UCB Algorithm}{UCB Algorithm}{0.5}
The UCB is based on the principle of optimism in the face of uncertainty, which states that:
\begin{center}
    \tbf{One should act as if the environment is as nice as plausibly possible.}
\end{center}
The UCB estimate for each arm is an over-estimate compared with the empirical mean $\hat{\mu}_{i,t-1}=\frac{1}{N_{i,t-1}}\sum_{t'\le t-1}r_{t'}\mathbb{1}\{a_{t'}=i\},$ recall the Chernoff-Hoeffding bound:
\eq{
    \mathbb{P}(\overline{X}-\mathbb{E}[\overline{X}]\leq z)\geq1-\exp(-nz^2/2).
}
Repalcing $z$ to $\sqrt{\frac{2\log(1/\delta)}{N_{i,t-1}}}$, we would get:
\eq{
    \mathbb{P}(\mu_i\geq\hat{\mu}_{i,t-1}+\sqrt{\frac{2\log(1/\delta)}{N_{i,t-1}}})\leq\delta.
}
\sep{Thompson Sampling Algorithm}
Assume that the reward distributions of different arms belong to the same family with respective parameters:
\eq{
    r(i)\sim p(x\mid\theta_i).
}
When estimating $\theta$, the posterior is:
\eq{
    p(\theta\mid x)=\frac{p(x\mid\theta)p(\theta)}{\int_{\theta'}p(x\mid\theta')p(\theta')d\theta'}.
}
If the posterior $p(\theta|x)$ is in the same probability distribution family as the prior $p(\theta)$, then they are called conjugate distributions.
The Bernoulli-Beta is important for Bernoulli Bandits, for $\theta=\{\alpha,\beta\}$:
\eq{
p(\theta)=\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}x^{\alpha-1}(1-x)^{\beta-1},
}
where $\Gamma(z)=\int_{0}^{\infty}x^{z-1}\exp(-x)dx$ is the Gamma function. So when $p(\theta)\sim\mathrm{Beta}(\alpha_{0},\beta_{0})$ and
observing $x_1,\ldots,x_{\alpha^{\prime}+\beta^{\prime}}\sim x\mathrm{~i.i.d.}$, with $\alpha^\prime$ ones and $\beta^\prime$ zeros.
\eq{
    \begin{aligned}
        p(\theta\mid x_{1},\ldots,x_{\alpha^{\prime}+\beta^{\prime}}) & =\frac{p(x_1,\ldots,x_{\alpha^{\prime}+\beta^{\prime}}\mid\theta)p(\theta)}{\int_{\theta^{\prime}}p(x_1,\ldots,x_{\alpha^{\prime}+\beta^{\prime}}\mid\theta^{\prime})p(\theta^{\prime})d\theta^{\prime}}                                                 \\
                                                                        & =\frac{\binom{\alpha'+\beta'}{\alpha'}\theta^{\alpha'}(1-\theta)^{\beta'}\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}\theta^{\alpha_0-1}(1-\theta)^{\beta_0-1}}{\int_{\theta'}p(x_1,\ldots,x_{\alpha'+\beta'}\mid\theta')p(\theta')d\theta'} \\
                                                                        & =\frac{\binom{\alpha'+\beta'}{\alpha'}\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}}{\int_{\theta'}p(x_1,\ldots,x_{\alpha'+\beta'}\mid\theta')p(\theta')d\theta'}\theta^{\alpha_0+\alpha'-1}(1-\theta)^{\beta_0+\alpha'-1}                    \\
                                                                        & \sim\mathrm{Beta}(\alpha_0+\alpha^{\prime},\beta_0+\beta^{\prime}).
    \end{aligned}
}
\fig{Thompson sampling for Bernoulli bandits}{Thompson sampling for Bernoulli bandits}{0.5}
TS algorithm can work on any conjugate distributions.
\fig{General Thompson Sampling}{General Thompson Sampling}{0.5}

\subsubsection{Analysis of Bandit Algorithms}
\sep{ETC Algorithm}
\thm{ETC Regret}{
    Assume $r(i)$ is \tit{1-sub-Gaussian} for each $i$, the regret satisfies:
    \eq{
        \overline{R}_T\leq k\sum_{i\in[m]}\Delta_i+(T-mk)\sum_{i\in[m]}\Delta_i\exp\left(-\frac{k\Delta_i^2}{4}\right).
    }
    When $m=2$, taking $k=\lceil\operatorname*{max}\begin{Bmatrix}1,4\Delta_{2}^{-2}\log(T\Delta_{2}^{2}/4)\end{Bmatrix}\rceil$ yields:
    \eq{
        k=\lceil\operatorname*{max}\begin{Bmatrix}1,4\Delta_{2}^{-2}\log(T\Delta_{2}^{2}/4)\end{Bmatrix}\rceil
    }
}
\proo{}{
    \eq{
        \begin{aligned}
            \mathbb{E}\left[N_{T,i}\right] & =k+(T-mk)\mathbb{P}\big(i=\underset{i'}{\operatorname*{\arg\max}}\hat{\mu}_{mk-1,i^{\prime}}\big)           \\
                                            & \leq k+(T-mk)\mathbb{P}(\hat{\mu}_{mk-1,i}\geq\hat{\mu}_{mk-1,1})                                           \\
                                            & =k+(T-mk)\mathbb{P}\big(\hat{\mu}_{mk-1,i}-\mu_{i}-\big(\hat{\mu}_{mk-1,1}-\mu_{1}\big)\geq\Delta_{i}\big).
        \end{aligned}
    }
    By the sub-Gaussian tail bound:
    \eq{
        \mathbb{P}\left(\hat{\mu}_{mk-1,i}-\mu_i-(\hat{\mu}_{mk-1,1}-\mu_1)\geq\Delta_i\right)\leq\exp\left(-\frac{k\Delta_i^2}{4}\right).
    }
    Therefore,
    \eq{
        \begin{aligned}
            \overline{R}_{T} & =\sum_{i=1}^{m}\mathbb{E}\left[N_{T,i}\right]\Delta_{i}                                                                                                                      \\
                                & \begin{aligned}&\leq\sum_{i=1}^{m}\Delta_{i}\left(k+(T-mk)\right.\mathbb{P}\big(\hat{\mu}_{mk-1,i}-\mu_{i}-(\hat{\mu}_{mk-1,1}-\mu_{1})\geq\Delta_{i}\big)\big)\end{aligned} \\
                                & \leq\sum_{i=1}^{m}\Delta_{i}\left(k+(T-mk)\exp\left(-\frac{k\Delta_{i}^{2}}{4}\right)\right).
        \end{aligned}
    }
    By setting $m=2$, we can derive:
    \eq{
        \begin{aligned}
            \overline{R}_{T} & \leq  \Delta_{2}\left(k+(T-mk)\exp\left(-\frac{k\Delta_{2}^{2}}{4}\right)\right) \\
                                & \leq \Delta_{2}\left(k+T\exp\left(-\frac{k\Delta_{2}^{2}}{4}\right)\right).
        \end{aligned}
    }
    To minimize the regret, take a derivative on $k$ and use the first-order condition. When $k=\lceil\operatorname*{max}\left\{1,4\Delta_{2}^{-2}\log(T\Delta_{2}^{2}/4)\right\}\rceil $:
    \eq{
        \begin{aligned}
            \overline{R}_{T} & \leq\Delta_{2}\left(k+T\exp\left(-\frac{k\Delta_{2}^{2}}{4}\right)\right)                                                                                                                                               \\
                                & \leq\Delta_{2}\left(k_{0}+1+T\exp\left(-\frac{k_{0}\Delta_{2}^{2}}{4}\right)\right)                                                                                                                                     \\
                                & \leq\Delta_{2}\left(\frac{4}{\Delta_{2}^{2}}\log\left(\frac{T\Delta_{2}^{2}}{4}\right)+1+T\exp\left(-\frac{\Delta_{2}^{2}}{4}\cdot\frac{4}{\Delta_{2}^{2}}\cdot\log\left(\frac{T\Delta_{2}^{2}}{4}\right)\right)\right) \\
                                & \leq\Delta_{2}\left(\frac{4}{\Delta_{2}^{2}}\log\left(\frac{T\Delta_{2}^{2}}{4}\right)+1+T\cdot\frac{4}{T\Delta_{2}^{2}}\right)                                                                                         \\
                                & \leq\Delta_2+\frac{4}{\Delta_2}+\frac{4}{\Delta_2}\log\left(\frac{T\Delta_2^2}{4}\right).
        \end{aligned}
    }
}
\sep{Recap of Different Algorithms}
\re{
For $\epsilon$-greedy, by choosing $\varepsilon_{t}=\min\{1,Ct^{-1}\Delta_{\mathrm{min}}^{-2}m\}$:
\eq{
    \overline{R}_T\le C'\sum_{i\ge2}\left(\Delta_i+\frac{\Delta_i}{\Delta_{\text{min}}^2}\log\max\left\{e,\frac{T\Delta_{\text{min}}^2}{m}\right\}\right)
}
For ETC under 2-armed bandits, when $T\geq4\sqrt{2\pi e}/\Delta^{2}$, by choosing $k=\lceil\frac{2}{\Delta^{2}}W(\frac{T^{2}\Delta^{4}}{32\pi})\rceil,$
\eq{
    \overline{R}_T\leq O(\frac{1}{\Delta}\log T\Delta^2)+o(\log T)+\Delta。
}
For the two algorithms listed above, to achieve the optimal regret. the information about $\Delta$ is necessary. UCB and TS don't require knowing $\Delta$.\\
For UCB algorithm, by setting $\delta=T^{-2}$,
\eq{
    \overline{R}_{T}\leq3\sum_{i=1}^{m}\Delta_{i}+\sum_{i:\Delta_{i}>0}\frac{16\log T}{\Delta_{i}}.
}
For TS (Bernoulli bandits):
\eq{
    \overline{R}_{T}\le\sum_{i:\Delta_{i}>0}\frac{\mu_{1}-\mu_{i}}{d_{\mathrm{KL}}(\mu_{1}\parallel\mu_{i})}\log T+o(\log T).
}
}

\subsection{Model-Based RL}

There are two streams of reinforcement learning algorithms, \tit{model-based} v.s. \tit{model-free} methods. Model-based algorithms require the knowledge on the
environment(transition matrix $P$ and reward function $r$), or need to estimate $P,r$ during the computations. Model-free algorithms don't rely the estimation
of the environment.
\tbf{Policy Evaluation (Prediction)}\\
\eq{
    \begin{aligned}v_{k+1}(s)\quad&\doteq\quad\mathbb{E}_\pi[R_{t+1}+\gamma v_k(S_{t+1})\mid S_t=s]\\&=\quad\sum_a\pi(a|s)\sum_{s',r}p(s',r|s,a)\Big[r+\gamma v_k(s')\Big]\end{aligned}
}
\fig{Iterative Policy Evaluation}{Iterative Policy Evaluation}{0.5}
The policy evaluation would terminate after finite steps. For any $\epsilon\in(0,\frac{1}{1-\gamma})$, after at least $N\ge\frac{\gamma}{(1-\gamma)^4}\frac{n^2m\log(cnm/\delta)}{\varepsilon^2}$
steps, with at least probability $1-\delta$:
\eq{
    \|P(\cdot\mid s,a)-\hat{P}(\cdot\mid s,a)\|_1\leq(1-\gamma)^2\varepsilon.
}
\thm{Policy Improvement Theorem}{
    For all $s\in\cS$, if:
    \eq{q_\pi(s,\pi^{\prime}(s))\geq v_\pi(s)}
    Then the policy $\pi^{\prime}$ must be better than policy $\pi$.
}
\tbf{Policy Improvement}
\eq{
    \begin{aligned}
        \pi^{\prime}(s) & \begin{aligned}\dot{=}\quad\arg\max_aq_\pi(s,a)\end{aligned}                                                                                        \\
                        & \begin{aligned}=\quad\underset{a}{\operatorname*{argmax}}\mathbb{E}[R_{t+1}+\gamma v_{\pi}(S_{t+1})\mid S_{t}=s,A_{t}=a]\end{aligned}              \\
                        & \begin{aligned}=\quad\arg\max_a\sum_{s^{\prime},r}p(s^{\prime},r|s,a)\Big[r+\gamma v_\pi(s^{\prime})\Big],\end{aligned}
    \end{aligned}
}
\sep{Policy Iteration and Value Iteration}
\fig{Policy Iteration}{Policy Iteration}{0.5}
\fig{Value Iteration}{Value Iteration}{0.5}
Differences between VI and PI:
\re{
    \tab{
        \item In each sweep, VI only updates one-step evaluation and one-step improvement, PI updates
        multiple-step evaluation and one-step improvement;
        \item PI takes fewer rounds but takes more time within each round.
    }
}
\fig{Generalized Policy Iteration}{Generalized Policy Iteration}{0.25}
\sep{Episodic Discrete MDP}
In the episodic setting, in every episode, the learner acts for $H$ steps, starting from a fixed starting state $s_0\sim\rhp_0$, and the process repeats for $K$ episodes.
The agent's goal is to minimize the expected regret:
\eq{
    \overline{R}_K=\mathbb{E}\left[KV^*(s_0)-\sum_{k=0}^{K-1}\sum_{h=0}^{H-1}r(s_h^k,a_h^k)\right].
}

\subsubsection{UCVI: Upper Confidence bound Value Iteration}
In MDP, the exploration is more difficult compared to bandit games, because we can only decide the action rather than navigate to a
particular state which is barely reached before. Using the ideas in UCB from bandit games, we can give an overestimation of value functions
for those states that were barely reached before. By value iteration and policy iteration, this overestimation can navigate the
agent to explore the strange states.
\fig{UCVI}{UCVI}{0.9}
UCVI is an extremely important algorithm. Taking confidence level $\delta=1/KH$, the regrest can achieve a performance of $\overline{R}_T\leq10\sqrt{n^2mH^4K\log(nmH^2K^2)}.$

\subsubsection{PSRL: Posterior Sampling for Reinforcement Learning}
\fig{PSRL}{PSRL}{0.4}
The regret of PSRL: $\overline{R}_T\leq\sqrt{30n^2mH^3K\log(nmHK)}.$

\subsection{Model-free RL}

The model-free framework is important in two types of domains:
\lis{
    \item When the MDP model is unknown, but we can sample trajectories from the MDP;
    \item When the MDP model is known but computing the value function via our model-based control methods is infeasible due to the size of the domain.
}
\subsubsection{Q-Learning}
Recall the model-based value iteration, the model appears in two places. One for the computation of the action value function,
and one for computing all the optimal policies $\max_{a\in A}\left[R(s,a)+\gamma\sum_{s'\in S}P(s'\mid s,a)V(s')\right]$. Both
places could remove the dependency on $P$ by using Q-function $Q(s,a)$ directly.\\
Another feature of Q-learning is the \tit{step size} $\alpha$. In Q-learning, instead of utilizing the Bellman optimality equation directly,
the update only takes $\alpha$ portion of the action value.
\fig{Q-learning}{Q-learning}{0.5}
Where $\delta_t=r+\gamma\max_{a\in\mathcal{A}}Q(s',a')-Q(s,a)$ is known as the \tbf{temporal difference} error (TD).\\
One problem with Q-learning is that the trajectory sampled is subject to the current policy, which lacks exploration. One simple approach is
using $\epsilon$ greedy exploration.
\fig{Q-learning with greedy exploration}{Q-learning with $\epsilon$-greedy exploration}{0.45}
In reinforcement learning algorithms, we tend to overestimate the value function of the states, this problem comes from the fact that we are using our estimate to both choose the
better coin and estimate its value. Consider the state $s$ with two possible actions $a_1, a_2$, and $Q(s,a_1),Q(s,a_2)=0$. Then we have:
\eq{
    \begin{aligned}
        \hat{V}(s) & =\mathbb{E}[\max(\hat{Q}(s,a_1),\hat{Q}(s,a_2))]                \\
                    & \geq\max(\mathbb{E}[\hat{Q}(s,a_1)],\mathbb{E}[\hat{Q}(s,a_2)]) \\
                    & =\max(0,0)                                                      \\
                    & =0=V^{*}(s)\:.
    \end{aligned}
}
Where the inequality is followed by Jensen's inequality.\\
In double Q-learning, we can maintain two independent unbiased estimates, $Q_1$ and $Q_2$ and when we use one to select the maximum, we can use the other to get an estimate of the value of this maximum.
\fig{Double Q-Learning}{Double Q-Learning}{0.5}

\subsubsection{Monte-Carlo Policy Evaluation}
The idea of Monte-Carlo is quite simple, run the game with the policy $\pi$ for many iterations, and average all the rewards to approximate
the value function.
\fig{Monte-Carlo policy evaluation}{Monte-Carlo policy evaluation}{0.5}
Some variations of Monte-Carlo policy evaluation:
\tab{
\item The body of the algorithm can remove $S$ by using $V^{\pi}(s_{j,t})\leftarrow V^{\pi}(s_{j,t})+\frac{1}{N(s_{j,t})}(G_{j,t}-V^{\pi}(s_{j,t})).$;
\item The algorithm only counts the first visit in every episode, there is another \tit{every-visit} version that doesn't use this condition;
\item Similar to Q-learning, there is another incremental updating scheme $V^{\pi}(s_{j,t})\leftarrow V^{\pi}(s)+\alpha(G_{j,t}-V^{\pi}(s))$.
}
\sep{Importance Sampling: Off-Policy Policy Evaluation}
The goal of importance sampling is to estimate the expected value of a function $f(x)$ when $x$ is drawn from the distribution $q$ using only the data $f(x_1)\cdots f(x_n)$,
where $x_i$ are drawn from a different distribution $p$. To estimate $\mathbb{E}_{x\sim q}[f(x)]$:
\eq{
    \begin{aligned}
        \mathbb{E}_{x\sim q}[f(x)] & \begin{aligned}=\int_xq(x)f(x)dx\end{aligned}                \\
                                    & =\int_xp(x)\left[\frac{q(x)}{p(x)}f(x)\right]dx              \\
                                    & =\mathbb{E}_{x\sim p}\left[\frac{q(x)}{p(x)}f(x)\right]      \\
                                    & \approx\sum_{i=1}^n\left[\frac{q(x_i)}{p(x_i)}f(x_i)\right].
    \end{aligned}
}
Now we bring the context back to reinforcement learning. To estimate $V^{\pi_1}(s)=\mathbb{E}[G_t\mid s_t=s]$,
using $n$ trajectories $h_1,\cdots,h_n$ generated by $\pi_2$, the importance sampling estimate can give us:
\eq{
    V^{\pi_1}\left(s\right)\approx\frac1n\sum_{j=1}^n\frac{\mathbb{P}(h_j\mid\pi_1,s)}{\mathbb{P}(h_j\mid\pi_2,s)}G(h_j),
}
where $G(h_j)=\sum_{t=1}^{L_j-1}\gamma^{t-1}r_{j,t}$ is the total discounted reward for each trajectory. We have:
\eq{
    \begin{aligned}
        \begin{aligned}\mathbb{P}(h_j\mid\pi,s=s_{j,1})\end{aligned} & \begin{aligned}&=\prod_{t=1}^{L_j-1}\mathbb{P}(a_{j,t}\mid s_{j,t})\mathbb{P}(r_{j,t}\mid s_{j,t},a_{j,t})\mathbb{P}(s_{j,t+1}\mid s_{j,t},a_{j,t})\end{aligned} \\
                                                                        & \begin{aligned}&=\prod_{t=1}^{L_j-1}\pi(a_{j,t}\mid s_{j,t})\mathbb{P}(r_{j,t}\mid s_{j,t},a_{j,t})\mathbb{P}(s_{j,t+1}\mid s_{j,t},a_{j,t}),\end{aligned}
    \end{aligned}
}
This equation needs information of the model ($P,r$), but by the operation of division, we have:
\eq{
    \begin{aligned}
        V^{\pi_1}(s) & \begin{aligned}&\approx\frac1n\sum_{j=1}^n\frac{\mathbb{P}(h_j\mid\pi_1,s)}{\mathbb{P}(h_j\mid\pi_2,s)}G(h_j)\end{aligned}                                                                                                                                     \\
                        & \begin{aligned}&=\frac1n\sum_{j=1}^n\frac{\prod_{t=1}^{L_j-1}\pi_1(a_{j,t}\mid s_{j,t})\mathbb{P}(r_{j,t}\mid s_{j,t},a_{j,t})\mathbb{P}(s_{j,t+1}\mid s_{j,t},a_{j,t})}{\prod_{t=1}^{L_j-1}\pi_2(a_{j,t}\mid s_{j,t})\mathbb{P}(r_{j,t}\mid s_{j,t},a_{j,t})\mathbb{P}(s_{j,t+1}\mid s_{j,t},a_{j,t})}G(h_j)\end{aligned} \\
                        & =\frac1n\sum_{j=1}^nG(h_j)\prod_{t=1}^{L_j-1}\frac{\pi_1(a_{j,t}\mid s_{j,t})}{\pi_2(a_{j,t}\mid s_{j,t})}.
    \end{aligned}
}
Which maintains the off-policy policy evaluation as a model-free technique.
\sep{Temporal Difference Learning}
Replace the $G_t$ in incremental Monte-Carlo policy evaluation with $r_t+\gamma V^{\pi}\left(s_{t+1}\right),$ and we can get:
\fig{TD Learning}{TD Learning}{0.45}
where the term $\delta_t=r_t+\gamma V^\pi(s_{t+1})-V^\pi(s_t)$ is called \tit{temporal difference}.

\subsubsection{Monte-Carlo Control}
\defi{GLIE: Greedy in the limit of infinite exploration}{
    A policy $\pi$ is \tit{greedy in the limit of infinite exploration} if:
    \lis{
        \item $\lim\limits_{k\to\infty}N_k(s,a)\to\infty\text{with probability}1$;
        \item $\lim\limits_{k\to\infty}\pi_k(a\mid s)=\arg\max\limits_aQ(s,a)\text{with probability}1.$
    }
    One example of GLIE is an $\epsilon$-greedy policy where $\epsilon\to 0$.
}
\fig{Online Monte-Carlo control}{Online Monte-Carlo control}{0.4}
\fig{SARSA}{SARSA}{0.4}
SARSA is short for \tit{state action reward state action}. The difference between SARSA and Q-learning is that SARSA is
an on-policy algorithm, updating the Q-function using the current strategy, while Q-learning is an off-policy learning algorithm.

\subsection{Value Function Approximation}

The methodologies above try to estimate the value function or Q-function for every state and action, but these methods might not apply to those
scenarios with very large state and action spaces. A popular approach for this issue is via \tbf{value function approximation}(VFA).
\eq{
    V^{\pi}(s)\approx\hat{V}(s,\mathbf{w})\quad\mathrm{or}\quad Q^{\pi}(s,a)\approx\hat{Q}(s,a,\mathbf{w}),
}
where $\mathbf{w}$ refers to parameter or wights. Some function approximations are listed below:
\lis{
    \item Linear combinations of features;
    \item Neural networks;
    \item Decision trees.
}

\subsubsection{Linear feature representations}
In linear function representations, we use a feature vector to represent a state:
\eq{
    x(s)=(x_1(s),x_2(s),\ldots,x_d(s))^T,
}
where $d$ is the dimensionality of the feature space, the approximation can be expressed by:
\eq{
    \hat{V}(s,\mathbf{w})=x(s)^T\mathbf{w}=\sum_{j=1}^dx_j(s)\mathbf{w}_j.
}
Then we can define the loss function:
\meq{
J(\mathbf{w})=\mathbb{E}_{s\sim\rho^{\pi}(s)}\left[(V^{\pi}(s)-\hat{V}(s,\mathbf{w}))^2\right];\\
\rho^\pi(s)=\lim_{T\to\infty}\frac{\sum_{t=0}^T\gamma^t\mathbb{P}(s_t=s\mid\pi)}{\sum_{t=0}^T\gamma^t}.
}
where $\rho^\pi(s)$ denotes the cumulative probability under the policy $\pi$. There are several ways to solve this optimization problem:
\lis{
    \item Gradient Descent (SGD);
    \item Policy evaluation with linear VFA;
}
\fig{Monte-Carlo policy evaluation with linear VFA}{Monte-Carlo policy evaluation with linear VFA}{0.45}
The updating formula $\mathbf{w}\leftarrow\mathbf{w}+\alpha(avg (R(s_t))-\hat{V}(s_t,\mathbf{w}))x(s_t)$ is derived by taking derivative
of $\mathbf{w}$ on $J(\mathbf{w})$:
\eq{
    \begin{aligned}
        \nabla J(\mathbf{w}) & = 2(V^{\pi}(s)-\hat{V}(s,\mathbf{w}))\frac{\partial (V^{\pi}(s)-\hat{V}(s,\mathbf{w}))}{\partial \mathbf{w}} \\
                                & = 2(V^{\pi}(s)-\hat{V}(s,\mathbf{w}))\cdot x(s).
    \end{aligned}
}
In different policy evaluation settings the term $V^{\pi}(s)-\hat{V}(s,\mathbf{w})$ could be replaced by different styles:
\tab{
\item Monte-Carlo: $\sum_{j=t}^{H_k}r_{k,j}-\hat{V}(s,\mathbf{w})$;
\item Temporal-difference: $r+\gamma\hat{V}^{\pi}(s^{\prime},\mathbf{w})-\hat{V}^{\pi}(s,\mathbf{w})$;
\item Q-learning (maximum TD(0)): $r+\gamma\operatorname*{max}_{a^{\prime}}\hat{Q}(s^{\prime},a^{\prime},\mathbf{w})-\hat{Q}(s,a,\mathbf{w})$.
}
One thing about VFA is that these algorithms may not converge or converge to a suboptimal solution.

\subsubsection{Deep Q-Learing}
This part would introduce Deep Q-learning (\cite{mnih2015human}), Double DQN (\cite{van2016deep}) and Dueling DQN (\cite{wang2016dueling}).\\
\sep{DQN}
The DQN is designed to estimate the state-action function $\hat{Q}(s,a,\mathbf{w})$.
\fig{DQN architecture}{DQN Architecture}{0.45}
The DQN architecture takes inputs consisting of an $84\times 84\times 4$ image, which denotes the state. But the original size of the
Atari game is $210\times160\times3$, so it needs some pre-processing:
\tab{
    \item \tit{Single frame encoding}:
    \item \tit{Dimensionality reduction}:
}
The loss function is given by:
\meq{
J(\mathbf{w})=\mathbb{E}_{(s_t,a_t,r_t,s_{t+1})}[(y_t^{DQN}-\hat{Q}(s_t,a_t,\mathbf{w}))^2],\\
y_t^{DQN}=r_t+\gamma\max_{a'}\hat{Q}(s_{t+1},a',\mathbf{w}^-).
}
Where $\mathbf{w}^-$ represents the parameters of the target network, and $\mathbf{w}$ is the online network after the update.\\
\tbf{Experience Replay}: The Q-network is updated by SGD with sampled gradients from minibatch data: $(s,a,r,s^{\prime})\sim\mathrm{Uniform}(D).$
$D$ is called the \tit{replay buffer}. This setting has some advantages and limitations:
\tab{
    \item \tit{Greater data efficiency}: Each step of experience can be potentially used for many updates, which improves data efficiency;
    \item \tit{Remove sample correlations}: Randomizing the transition experiences reduces the correlations between consecutive samples and therefore reduces the variance of updates
    and stabilizes the learning;
    \item \tit{Avoiding oscillations or divergence};
    \item \tit{Limitations}: Both the replay buffer and the uniform sampling don't t differentiate important transitions or informative transitions.
    A more sophisticated replay strategy is \tit{Prioritized Replay} by \cite{schaul2015prioritized}.
}
\tbf{Target Network} is used to improve the stability of learning and deal with the nonstationary learning targets.
For every $C=10000$ update steps the target network $\hat{Q}(s,a,\mathbf{w}^{-})$ is updated by copying the parameters' values $(\mathbf{w}^{-}=\mathbf{w})$
from the online network $\hat{Q}(s,a,\mathbf{w})$.
\fig{Deep Q-learning}{Deep Q-learning}{0.8}
\sep{DDQN}
Similar to the idea used in double Q-learning, DDQN uses two different networks to perform action selection and value function approximation.
However, DDQN doesn't maintain two independent networks but utilizes the target network by:
\eq{
y_t^{DDQN}=r_t+\gamma\hat{Q}(s_{t+1},\arg\max_{a'}\hat{Q}(s_{t+1},a',\mathbf{w}),\mathbf{w}^-).
}
\sep{Dueling DQN}
Recall the \tit{advantage function}:
\eq{
    A^\pi(s,a)=Q^\pi(s,a)-V^\pi(s).
}
We have $\mathbb{E}_{a\sim\pi(s)}[A^{\pi}(s,a)]=0.$ Like in DQN, the dueling network is also a neural network function approximator for
learning the Q-function. Differently, it approximates the Q function by decoupling the value function and the advantage function.
\fig{Dueling DQN v.s. DQN}{Dueling DQN v.s. DQN}{0.5}
Where the green layer conducts the following operations:
\meq{
    \hat{Q}(s,a,\mathbf{w},\mathbf{w_A},\mathbf{w_V})=\hat{V}(s,\mathbf{w},\mathbf{w_V})+\left(A(s,a,\mathbf{w},\mathbf{w_A})-\max_{a'\in A}A(s,a',\mathbf{w},\mathbf{w_A})\right);\\
    \hat{Q}(s,a,\mathbf{w},\mathbf{w_A},\mathbf{w_V})=\hat{V}(s,\mathbf{w},\mathbf{w_V})+\left(A(s,a,\mathbf{w},\mathbf{w_A})-\frac{1}{|\mathcal{A}|}\sum_{a'}A(s,a',\mathbf{w},\mathbf{w_A})\right).
}
in which $\mathbf{w_V}$ is the parameter of the FC layer to predict the value functions, and $\mathbf{w_A}$ is the parameter of the FC layer to predict the advantage functions.

\subsection{Policy Gradient Algorithms}

In most RL algorithms, the policy does not exist without the action value estimates $Q(s,a)$, these are called \tbf{value-based} algorithms.
These algorithms have several limitations:
\lis{
    \item Can only address discrete action spaces due to the $\arg\max_{a\in A}$ operation;
    \item Computing $Q(s,a)$ for all state-action pairs is costly;
    \item Can only improve the policy indirectly by improving the estimates of the value function.
}
\tbf{Policy-based} method directly parameterizes the policy function $\pi_\theta(s)$ without calculating the value functions.
A value function may still be used to learn the policy parameters, but it is not required for action selection. To implement the policy-based methods,
an important step is to approximate policies with parametrization. In discrete case, we can use softmax and \gt{state-action preferences}
$h(s,a,\theta)$ to parameterize a policy:
\eq{
    \pi(a\mid s,\boldsymbol{\theta})=\frac{\exp(h(a,s,\boldsymbol{\theta}))}{\sum_{a'}\exp(h(a',s,\boldsymbol{\theta}))}.
}
In continuous cases, we can use the following functions to parameterize a policy:
\meq{
\pi(a\mid s,\boldsymbol{\theta_\mu},\boldsymbol{\theta_\sigma})=\frac{1}{\sigma(s,\theta_{\sigma})\sqrt{2\pi}}\exp(-\frac{(a-\mu(s,\theta_{\mu}))^2}{2\sigma(s,\theta_{\sigma})^2});\\
\mu(s,\boldsymbol{\theta})=\boldsymbol{\theta}_{\boldsymbol{\mu}}^T\boldsymbol{x}_{\boldsymbol{\mu}}(s),\quad\sigma(s,\boldsymbol{\theta})=\exp(\boldsymbol{\theta}_{\boldsymbol{\sigma}}^T\boldsymbol{x}_{\boldsymbol{\sigma}}(s)),
}
In the settings where the episode terminates at some terminal state set, we can define the objective function as:
\eq{
\boldsymbol{J}(\boldsymbol{\theta})=V^{\pi_\theta}(s_0)=\sum_{s\in\mathcal{S}}\rho^{\pi_\theta}(s\mid s_0)r(s),
}
where $r(s)=\mathbb{E}_{a\operatorname*{\sim}_{T}}[\mathcal{R}(s,a)]$ and $\rho^{\pi_\theta}(s\mid s_0)=\frac{1}{T}\sum_{t=0}^T\mathbb{P}(s_t=s\mid s_0,\pi_\theta)$.
In the continuous setting where the process continues infinitely, we can define:
\eq{
    \begin{aligned}
        \boldsymbol{J}(\boldsymbol{\theta}) & =\lim_{T\to\infty}\frac1T\sum_{t=1}^T\mathbb{E}[r_t\mid s_0,\pi_\theta] \\
                                            & =\lim_{t\to\infty}\mathbb{E}[r_t\mid s_0,\pi_\theta]                    \\
                                            & =\sum_s\rho^{\pi_\theta}(s\mid s_0)r(s)                                 \\
                                            & =V^{\pi\theta}\left(s_0\right).
    \end{aligned}
}
\thm{Policy Gradient Theorem}{
    \eq{
        \begin{gathered}
            \nabla_\theta J(\theta) \propto\sum_{s\in\mathcal{S}}\rho^{\pi_\theta}(s\mid s_0)\sum_{a\in\mathcal{A}}Q^{\pi_\theta}(s,a)\nabla_\theta\pi_\theta(a\mid s) \\
            =\mathbb{E}_\pi\left[Q^{\pi_\theta}(s,a)\nabla_\theta\log\pi_\theta(a\mid s)\right].
        \end{gathered}
    }
}
\proo{Policy Gradient Theorem}{
    Here we only prove the episodic case.
    \eq{
        \begin{aligned}
            \nabla_\theta V^\pi(s)
                & =\nabla_\theta\Big(\sum_{a\in\mathcal{A}}\pi_\theta(a\mid s)Q^\pi(s,a)\Big)                                                                                                                                                                                    \\
                & =\sum_{a\in\mathcal{A}}\left(\nabla_\theta\pi_\theta(a\mid s)Q^\pi(s,a)+\pi_\theta(a\mid s)\nabla_\theta Q^\pi(s,a)\right)                                                                                                                                     \\
                & =\sum_{a\in\mathcal{A}}\left(\nabla_\theta\pi_\theta(a\mid s)Q^\pi(s,a)+\pi_\theta(a\mid s)\nabla_\theta\sum_{s^{\prime},r}\mathbb{P}(s^{\prime},r\mid s,a)(r+V^\pi(s^{\prime}))\right)                                                                        \\
                & =\sum_{a\in\mathcal{A}}\left(\nabla_\theta\pi_\theta(a\mid s)Q^\pi(s,a)+\pi_\theta(a\mid s)\sum_{s^{\prime},r}\mathbb{P}(s^{\prime},r\mid s,a)\nabla_\theta V^\pi(s^{\prime})\right)                                                                           \\
                & \begin{aligned}=\sum_{a\in\mathcal{A}}\left(\nabla_\theta\pi_\theta(a\mid s)Q^\pi(s,a)+\pi_\theta(a\mid s)\sum_{s'}\mathbb{P}(s'\mid s,a)\nabla_\theta V^\pi(s')\right)\end{aligned}
        \end{aligned}
    }
    This equation has a nice recursive form, to unroll the recursion of $\nabla_\theta V^\pi(s^{\prime})$, denote
    \meq{
        \phi(s)=\sum_{a\in\mathcal{A}}\nabla_\theta\pi_\theta(a\mid s)Q^\pi(s,a)\\
        \eta(s){=}\sum_{k=0}^\infty\rho^\pi(s_0{\to}s,k)
    }
    for simplicity, and denote $\rho^\pi(s\to s^\prime,k)$ as the probability of transitioning from $s$ to $s^\prime$ with policy $\pi$ after $k$ steps. We have:
    \eq{
        \begin{aligned}
            \nabla_\theta J(\theta) =\nabla_\theta V^\pi(s_0) & = \phi(s_0)+\sum_a\pi_\theta(a\mid s)\sum_{s'}\mathbb{P}(s'\mid s_0,a)\nabla_\theta V^\pi(s')                                      \\
                                                                & = \phi(s_0)+\sum\rho^{\pi}(s_0\rightarrow s^{\prime},1)\nabla_{\theta}V^{\pi}(s^{\prime})                                          \\
                                                                & = \phi(s_0)+\sum_{s'}\rho^{\pi}(s_0\rightarrow s',1)\phi(s')+\sum_{c''}\rho^{\pi}(s_0\rightarrow s'',2)\nabla_{\theta}V^{\pi}(s'') \\
                                                                & = \sum_s\sum_{k=0}^\infty\rho^\pi(s_0\to s,k)\phi(s) = (\sum_s\eta(s))\sum_s\eta(s)\phi(s)                                         \\
                                                                & \propto\sum_s\frac{\eta(s)}{\sum_s\eta(s)}\phi(s)=\sum_s\rho^\pi(s\mid s_0)\sum_a\nabla_\theta\pi_\theta(a|s)Q^\pi(s,a).
        \end{aligned}
    }
    Further, the policy gradient can be written as:
    \eq{
        \begin{aligned}
            \nabla_\theta J(\theta) & \propto\sum_{s\in\mathcal{S}}\rho^\pi(s\mid s_0)\sum_{a\in\mathcal{A}}Q^\pi(s,a)\nabla_\theta\pi_\theta(a\mid s)                                          \\
                                    & =\sum_{s\in\mathcal{S}}\rho^\pi(s\mid s_0)\sum_{a\in\mathcal{A}}Q^\pi(s,a)\pi_\theta(a\mid s)\frac{\nabla_\theta\pi_\theta(a\mid s)}{\pi_\theta(a\mid s)} \\
                                    & =\sum_{s\in\mathcal{S}}\rho^\pi(s\mid s_0)\sum_{a\in\mathcal{A}}\pi_\theta(a\mid s)Q^\pi(s,a)\frac{\nabla_\theta\pi_\theta(a\mid s)}{\pi_\theta(a\mid s)} \\
                                    & =\mathbb{E}_\pi[Q^\pi(s,a)\nabla_\theta\log\pi_\theta(a\mid s)],
        \end{aligned}
    }
}
\fig{REINFORCE}{REINFORCE}{0.7}
For $Q^\pi(s,a)$, we can use $G_t=\sum\gamma^tr_t$ to estimate. For $\nabla_\theta\log\pi_\theta(a\mid s)$, it depends on its form.
Policy gradient suffers from \rt{high variance}, one solution is by subtracting a baseline $b(s)$ independent of $\theta$:
\eq{
\nabla_\theta J(\theta)\propto\sum_{s\in\mathcal{S}}\rho^\pi(s\mid s_0)\sum_{a\in\mathcal{A}}(Q^\pi(s,a)-b(s))\nabla\pi_\theta(a\mid s).
}
We can see that:
\eq{
    \sum_ab(s)\nabla\pi(a\mid s,\theta)=b(s)\nabla\sum_a\pi(a\mid s,\theta)=b(s)\nabla1=0,
}
which means the expectation value does not change. A natural choice for the baseline is an estimate of the state value $\hat{V}(s,\boldsymbol{w})$.
\re{
    This equation can be generalized to:
    \lem{Stein's identity}{
        \eq{
            \mathbb{E}_\pi[\nabla_a\log\pi(a\mid s)\phi(s,a)+\nabla_a\phi(s,a)]=0,
        }
        where $\phi(s,a)$ is an arbitrary state-action function.
    }
}
\fig{REINFORCE with baseline}{REINFORCE with baseline}{0.7}
\re{
    Why the introduction of baseline reduces the variance:\\
    Assume $b(s)$ is an unbiased estimator of $V^\pi(s)$, then the term $R_t(\tau)-b(s_t)$ has a mean $0$. Thus,
    \eq{
        \begin{aligned}
            \mathrm{Var}\left(\sum_{t=0}^{T-1}\nabla_\theta\log\pi_\theta(a_t|s_t)(R_t(\tau)-b(s_t))\right) & \approx\sum_{t=0}^{T-1}\mathbb{E}\tau\left[\left(\nabla_\theta\log\pi_\theta(a_t|s_t)(R_t(\tau)-b(s_t))\right)^2\right]                                 \\
                                                                                                            & \approx \sum_{t=0}^{T-1}\mathbb{E}_\tau\left[\left(\nabla_\theta\log\pi_\theta(a_t|s_t)\right)^2\right]\mathbb{E}_\tau\left[(R_t(\tau)-b(s_t))^2\right]
        \end{aligned}
    }
    The above equation does not necessarily show that adding a baseline would reduce the variance, this only provides an intuitive understanding.
}
\re{
The general form of \tit{policy gradient}:
\eq{
    g=\mathbb{E}\left[\sum_{t=0}^{\infty}\Psi_t\nabla_\theta\log\pi_\theta\left(a_t\mid s_t\right)\right]
}
The $\Psi_t$ could be the following form:
\tab{
\item $\sum_{t=0}^\infty r_t$: : the total reward of the trajectory;
\item $Q^\pi(s_t,a_t)$: the action value function;
\item $\sum_{t^\prime}^\infty$: the reward following action $a_t$;
\item $\sum_{t'=t}^{\infty}r_{t'}-b\left(s_{t}\right)$:  the reward following action $a_t$ with a baseline;
\item $A^\pi(s_t,a_t)$: the advantage function;
\item $r_t+V^\pi(s_{t+1})-V^\pi(s_t)$: the TD residual.
}
}

\subsubsection{Actor-Critic}
In practice, most of the variance is from the Monte-Carlo estimation $G_t$ of $Q(s_t,a_t)$. If we use the parameterized temporal-difference method to
estimate the state-action function, then we can get \tbf{actor-critic} algorithm. Actor-critic methods consist of two models;
\lis{
    \item The critic updates the value function parameters $w$;
    \item The actor updates the policy parameters $\theta$ in the direction suggested by the critic.
}
\fig{One-step actor-critic}{One-step actor-critic}{0.7}
\sep{Deep Deterministic Policy Gradient: DDPG}
DDPG is an algorithm similar to DQN, it utilizes the trick of \tit{replay buffer} and \tit{target netwrok}. However,
DDPG maintains two networks at the same time: one to approximate the Q-function (critic), and another to choose the action (actor).
Because the approximated Q-function can be differentiable, DDPG can be applied for continuous action spaces.
\fig{DDPG}{Deep Deterministic Policy Gradient}{0.7}
where $d=1$ if it's the terminal state, otherwise $d=0$.
\sep{Soft Actor-Critic: SAC}
This algorithm is proposed by \cite{haarnoja2018soft}. SAC is an algorithm that optimizes a stochastic policy in an off-policy way, a key feature
of SAC is \tbf{entropy regularization}, which is designed to maximize the trade-off between the expected return and the randomness in the policy. (exploitation v.s. exploration)
The objective function is given by:
\eq{
\pi^{*}=\arg\max_{\pi}\mathop{\mathrm{E}}_{\tau\sim\pi}\left[\sum_{t=0}^{\infty}\gamma^{t}\bigg(R(s_{t},a_{t},s_{t+1})+\alpha H\left(\pi(\cdot|s_{t})\right)\bigg)\right],
}
where $H(P)=\operatorname*{E}_{x\sim P}\left[-\log P(x)\right]$ is the \tbf{entropy}, SAC concurrently learns a policy $\pi_\theta$ and two Q-functions $Q_{\phi_1},Q_{\phi_2}$:
\fig{SAC}{Soft Actor-Critic}{0.8}
\subsubsection{Trust Region Policy Optimization: TRPO}
If the noise of the gradient is large, choosing the best step size using the exact line search could be analogous to choosing the optimum among the random signals.
One way to solve this is the \tbf{trust region} method (\cite{schulman2015trust}). Let $c(s_t)$ be the cost function, and denote
\eq{
    \begin{aligned}&\eta(\pi)=\mathbb{E}_{s_0,a_0,...}\left[\sum_{t=0}^\infty\gamma^tc(s_t)\right],\text{ where}\\&s_0\sim\rho_0(s_0),a_t\sim\pi(a_t|s_t),s_{t+1}\sim P(s_{t+1}|s_t,a_t).\end{aligned}
}
Recall the advantage function $A_\pi(s,a)=Q_\pi(s,a)-V_\pi(s)$, so for another policy $\tilde{\pi}$, we have the improvement formula:
\eq{
    \eta(\tilde{\pi})=\eta(\pi)+\mathbb{E}_{s_0,a_0,s_1,a_1,...}\left[\sum_{t=0}^\infty\gamma^tA_\pi(s_t,a_t)\right]
}
This is accumulated over timesteps. If we denote $\rho_{\pi}(s)=(P(s_0=s)+\gamma P(s_1=s)+\gamma^2P(s_2=s)+\ldots)$, then we can rewrite this in accumulation over states:
\eq{
    \eta(\tilde{\pi})=\eta(\pi)+\sum_s\rho_{\tilde{\pi}}(s)\sum_a\tilde{\pi}(a|s)A_\pi(s,a).
}
Optimizing the right side directly is difficult since we can not take the derivative of $\rho_{\tilde{\pi}}(s)$. So we introduce a local approximation:
\eq{
    L_\pi(\tilde{\pi})=\eta(\pi)+\sum_s\rho_\pi(s)\sum_a\tilde{\pi}(a|s)A_\pi(s,a),
}
where we approximate $\rho_{\tilde{\pi}}(s)$ using $\rho_{\pi}(s)$. Now we can optimize the right side directly, it is proved that:
\lem{\cite{kakade2002approximately}}{
\eq{
    \eta(\pi_{\mathrm{new}})\leq L_{\pi_{\mathrm{old}}}(\pi_{\mathrm{new}})+\frac{2\epsilon\gamma}{(1-\gamma(1-\alpha))(1-\gamma)}\alpha^2,
}
where the term is given by:
\meq{
\pi^{\prime}=\arg\min_{\pi^{\prime}}L_{\pi_{\mathrm{old}}}(\pi^{\prime});\\
\pi_{\mathrm{new}}(a|s)=(1-\alpha)\pi_{\mathrm{old}}(a|s)+\alpha\pi'(a|s);\\
\epsilon=\max_s\lvert\mathbb{E}_{a\sim\pi^{\prime}(a|s)}\left[A_\pi(s,a)\right]\rvert.
}
Since $\alpha,\gamma\in[0,1]$, we can get a tighter bound:
\eq{
    \eta(\pi_{\mathrm{new}})\leq L_{\pi_{\mathrm{old}}}(\pi_{\mathrm{new}})+\frac{2\epsilon\gamma}{(1-\gamma)^2}\alpha^2.
}\label{eq:kakade2002}
}
Now we can generalize this lemma to:
\thm{Monotonic Improvement Guarantee for General Stochastic Policies}{
By replacing $\alpha$ with a distance measure between $\pi$ and $\tilde{\pi}$, we can generate a bound similar to equation \ref{eq:kakade2002}. More specifically,
if we let $\alpha=D_{\mathrm{TV}}^{\max}(\pi_{\mathrm{old}},\pi_{\mathrm{new}})=\operatorname*{max}_{s}D_{TV}(\pi(\cdot|s)\parallel\tilde{\pi}(\cdot|s))$, where $D_{TV}(p\parallel q)=\frac{1}{2}\sum_i\lvert p_i-q_i\rvert$
is the \tit{total variation divergence}, and $\epsilon=\max_s|\mathbb{E}_{a\sim\pi^{\prime}(a|s)}[A_\pi(s,a)]|$. Then equation \ref{eq:kakade2002} holds.\\
Note the relationship between the total variation divergence and the \tbf{KL divergence}: $D_{TV}(p\parallel q)^{2}\le D_{\mathrm{KL}}(p\parallel q)$, we can get the following bound:
\eq{
\eta(\tilde{\pi})\leq L_\pi(\tilde{\pi})+CD_{\mathrm{KL}}^{\max}(\pi,\tilde{\pi}), C=\frac{2\epsilon\gamma}{(1-\gamma)^2}.
}
Where $D_{\mathrm{KL}}^{\max}(\pi,\tilde{\pi})=\max_{s}D_{\mathrm{KL}}(\pi(\cdot|s)\parallel\tilde{\pi}(\cdot|s)).$.
}
From this theorem we can propose an algorithm:
\fig{TRPO Prior}{Approximate policy iteration algorithm guaranteeing non-increasing expected cost $\eta$}{0.8}
\tbf{Trust region policy optimization} is an approximation to the algorithm above. When we parameterize the policy $\pi$ using $\theta$, we can solve:
\eq{
\mathrm{minimize}\left[L_{\theta_{\mathrm{old}}}(\theta)+CD_{\mathrm{KL}}^{\mathrm{max}}(\theta_{\mathrm{old}},\theta)\right]
}
to improve the real $eta$. But if we use $C$ derived from theory, the step size would be very small. So we use a constraint on the KL divergence to
keep the optimal solution within a \tbf{trust region}:
\eq{
    D_{\mathrm{KL}}^{\max}(\theta_{\mathrm{old}},\theta)\leq\delta.
}
What's more, this new constraint constrains the KL divergence in every state. We relax this by transforming the optimization to:
\meq{
\min_\theta L_{\theta_{\mathrm{old}}}(\theta)\\
\text{subject to }\overline{D}_{\mathrm{KL}}^{\rho\theta_{old}}(\theta_{\mathrm{old}},\theta)\leq\delta.
}
where $\overline{D}_{\mathrm{KL}}^{\rho\theta_{old}}(\theta_{old},\theta):=\mathbb{E}_{s\sim\rho\theta_{old}}\left[D_{\mathrm{KL}}(\pi_{\theta_{old}}(\cdot|s)\parallel\pi_{\theta}(\cdot|s))\right].$
In practice, we can estimate the objective and constraint based on the sample:
\eq{
    \begin{aligned}
        \operatorname*{minimize}_{\theta}\mathbb{E}_{s\sim\rho_{\theta_{\mathrm{old}}}} & \left[\frac{\pi_{\theta}(a|s)}{\pi_{\theta_{old}}(a|s)}Q_{\theta_{\mathrm{old}}}(s,a)\right]     \\
        \mathrm{subject~to~\mathbb{E}_{s\sim\rho_{\theta_{\mathrm{old}}}}}              & [D_{\mathrm{KL}}(\pi_{\theta_{\mathrm{old}}}(\cdot|s)\parallel\pi_{\theta}(\cdot|s))]\leq\delta.
    \end{aligned}
}
\subsubsection{Proximal Policy Optimization: PPO}
PPO, designed by \cite{schulman2017proximal}, has some of the benefits of TRPO, but is much simpler to implement. TRPO algorithm can fluctuate greatly when
$\rho_t(\theta)~=~\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_\mathrm{old}}(a_t|s_t)}$ changes too quickly, so PPO proposes the \tit{clip surrogate loss}:
\eq{
    L^{CLIP}(\theta)=\hat{\mathbb{E}}_t\Big[\min(\rho_t(\theta)\hat{A}_t,\operatorname{clip}(\rho_t(\theta),1-\epsilon,1+\epsilon)\hat{A}_t)\Big]
}
There is another surrogate loss in the penalty style:
\eq{
    L^{KLPEN}(\theta)=\hat{\mathbb{E}}_t\bigg[\frac{\pi_\theta(a_t\mid s_t)}{\pi_{\theta_\mathrm{old}}(a_t\mid s_t)}\hat{A}_t-\beta\operatorname{KL}[\pi_{\theta_\mathrm{old}}(\cdot\mid s_t),\pi_\theta(\cdot\mid s_t)]\bigg]
}
In practice, there is a simple way to adjust the hyperparameter $\beta$, compute $d=\hat{\mathbb{E}}_t[\mathrm{KL}[\pi_{\theta_\mathrm{old}}(\cdot\mid s_t),\pi_\theta(\cdot\mid s_t)]]$:
\tab{
    \item If $d<d_{\mathrm{targ}}/1.5,\beta\leftarrow\beta/2$;
    \item $d>d_{\mathrm{targ}}\times1.5,\beta\leftarrow\beta\times2$.
}
\fig{PPOAC}{PPO, Actor-Critic Style}{0.8}

\clearpage
\chapter{Operations Management}

\section{Empirical Operations Management}

\cite{roth2007applications} describes the evolution of empirical OM from 1980 to 2007, the author selects 12 profounding papers
in this domain. \cite{brusco2017cluster} reviewed the clustering methods applied in 6 OM journals.\\
\cite{choi2016multi}, multi-methodological OM is advocated, which includes the empirical methodology.
\defi{Multi-methodological OM}{an approach for OM research in which at least two distinct OM research
    methods are employed nontrivially to meet the research goals.}
\cite{roth2022pioneering} classified 75 papers as empirical among the top 200 cited papers in \textit{POM}, these papers are mainly from 3 topical areas:
\lis{
    \item Responsibility Operations: covers environmental management, sustainability, and humanitarian efforts;
    \item Supply Chain Management: bullwhip effect, risk management, supply chain finance;
    \item Manufacturing Strategy and Quality Management.
}
Primary data (surveys, experiments, interviews) are used in these studies, followed by secondary data (public database, firm's data).
\no \hl{Roth's suggestions}:
\lis{
    \item For some topic which is very intuitive and not surprising, focus on the \textbf{size} of effect rather than sign;
    \item Avoid the confirmation bias and focus on consistency;
    \item Focus on endogeneity and causality, using common sense simultaneously.
}
\cite{kumar2022expanding} reviewed different domains of OM publications on \textit{POM}, within which a section about empirical OM is covered.\\
\cite{mithas2022causality} reviewed 411 empirical papers form 2016-2021 on \textit{POM}, with a
causal inference and counterfactual perspective.
\fig{identification strategies}{identification strategies from 2016 to 2021}{0.95}
\re{Two challenges in assessing causality:\\
    \eq{\begin{aligned}T & =ATE+\left[E(Y(0)|Z=1)-E(Y(0)|Z=0)\right] \\
                & +(1-\pi)*\left[(ATT-ATU)\right].\end{aligned}}
    \tab{
        \item $\left[E(Y(0)|Z=1)-E(Y(0)|Z=0)\right]$ is the \textbf{baseline bias}, coming from \textit{OVB} or
        \textit{simultaneity};
        \item $\left[(ATT-ATU)\right]$ is the \textbf{differential treatment bias}.
    }
}
\fig{identification techniques}{How identification techniques works}{}
\cite{fisher2022innovations} especially investigate the empirical research in retail operations from
traditional ones like forecasting and inventory planning, to new technologies, like radio frequency
identification (RFID) and e-commerce.

\subsection{Quantitative Marketing}

\subsubsection{Promotion Decompostion}
\cite{Leeflang2002} reviewed the development process of the SCAN*PRO (scanner and promotion) model, which was proposed by \cite{wittink1988scan}. The original version is given by:
\eq{\label{eq:scanpro}
    \begin{aligned}
        q_{kjt}&=\left[\prod_{r=1}^{n}\left[\frac{p_{krt}}{\overline{p}_{kr}}\right]^{\beta rj}\prod_{l=1}^{3}\gamma_{lrj}^{D_{lkn}}\right]\left[\prod_{t=1}^{T}\delta_{ji}^{X_{t}}\right]\left[\prod_{k=1}^{K}\lambda_{kj}^{Z_{k}}\right]e^{u_{kjt}}\\
        k&=1,\ldots,K,~t=1,\ldots,T
    \end{aligned}
}
Where: $q_{kjt}$ is unit sales in store $k$ for brand $j$ at week $t$, $p_{krt}$ is the unit price in store $k$ for brand $j$ at week $t$, \bar{p}_{kr} is the median price at store $k$ 
for brand $r$ in non-promoted weeks, and $D_{lkrt}$ is the dummy variable indicating whether promotion activity $l$ is launched in store $k$ for brand $r$ at time $t$. 
The terms $X_t$ and $Z_k$ can be interpreted as time and location fixed effects, and $u_{kjt}$ is the error term. The above equation looks complicated, but if we use $\log$ to transform it, we can get；
\eq{
    \log q_{kjt} = \sum_{i=1}^n(\log p_{krt}-\log \bar{p_{kr}})\cdot \beta_{rj}\cdot \sum_{l=1}^3\log \gamma_{lkj}D_{lkrt}+Fixed\_Effects + u_{kjt}
}
The $\beta_{rj}$ is the price discount elasticities (own-brand for $r=j$, cross-brand if $r\ne j$), the $\gamma_{jt}$ is the promotion multiplier. This model allows all brands 
to have unique own- and cross-brand effects for the marketing variables.\\
Equation \ref{eq:scanpro} does not account for dynamic promotion effects, which occur if promotions influence future demand. The \tbf{process function} of the own-brand discount elasticity can capture these effects:
\eq{
    \beta_{kjt}^{\prime}=\beta_{0j}^{\prime}+\beta_{1j}^{\prime}Dsum_{kjt}+\beta_{2j}^{\prime}CDsum_{kjt}+\beta_{3j}^{\prime}d_{\eta}\left(\frac{1}{PTime_{kjt}}\right)+\beta_{4j}^{\prime}d_{\eta}\left(\frac{1}{CPTime_{kjt}}\right)+u_{kjt}^{\prime},
}
where 
\tab{
    \item $Dsum_{kjt}$ is $\sum_{s=1}^{\omega}\eta^{s-1}\times(discount_{kj,t-s})$ in store $k$ for brand $j$ at week $t$, $\eta$ is the decline rate;
    \item $CDsum_{kjt}$ is $\sum_{\begin{array}{c}r=1\\r\neq j\end{array}}^n\sum_{s=1}^\omega\eta^{s-1}\times(discount_{kr,t-s})$;
    \item $d_\eta$ is a dummy variable equal to 1 if $\eta=1$ and 0 if $0<\eta<1$;
    \item $PTime_{kjt}$ is the number of weeks since the last own-brand promotion.
}
Besides decomposing the demand based on elasticity (gross effects)m, there is another decomposition scheme based on the unit sales (net effects). 
\cite{VanHeerde2004} is an example of the net effects decomposition. In the standard sales-based decomposition, the own-brand sales increase can be split into three parts: 
cross-brand, crossperiod, and category-expansion effects. Denote $S_{ijt}$ as the sales volume at store $i$ for brand $j$ at time $t$, and $CS_{i}$ as the average sales at store $i$. 
Define:
\[
    \begin{aligned}
        \mathrm{OBS}_{ijt}&=-\frac{S_{ijt}}{CS_i},\quad\mathrm{CBS}_{ijt}=\sum_{k=1}^{l}\frac{S_{ikt}}{CS_i},\\
        \mathrm{PPCS}_{it}&=\sum_{s=-T^*}^{l}\sum_{k=1}^{l}\frac{S_{ikt+s}}{CS_i},\quad\mathrm{TCS}_{it}=-\sum_{s=-T^*}^{l}\sum_{k=1}^{l}\frac{S_{ikt+s}}{CS_i}.
    \end{aligned}
\]
Where $OBS_{ijt}$ is the own-brand sales, $CBS_{ijt}$ is the cross-brand sales, $PPCS_{it}$ is the Pre- and Post category sales, and $TCS_{it}$ is the total category sales. By construction, the following equation holds:
\eq{
    \mathrm{OBS}_{ijt}=\mathrm{CBS}_{ijt}+\mathrm{PPCS}_{it}+\mathrm{TCS}_{it}.
}
For each variable given above, the firm can use a linear system to estimate the net effects:
\eq{
    \begin{aligned}
        \mathrm{X}_{ijt}& =\alpha_{j}^{\prime}+\sum_{l=1}^{4}\beta_{\mathrm{ob},lj}\mathrm{PI}_{ijlt}+\sum_{l=1}^{4}\gamma_{1,lj}^{\prime}\mathrm{CPI}_{ijlt}+\sum_{m=1}^{3}\gamma_{2,mj}^{\prime}\mathrm{D}_{ijmt}  \\
        &+\sum_{m=1}^{3}\gamma_{3,mj}^{\prime}\mathrm{CD}_{ijmt}+\gamma_{4j}^{\prime}\mathrm{RP}_{ijt}+\gamma_{5j}^{\prime}\mathrm{CRP}_{ijt} \\
        &+\sum_{\tau=T+T^{*}+1}^{T_{\max}-T-T^{*}}\gamma_{6,\tau j}^{\prime}W_{t}+\sum_{\tau=1}^{T_{+}T^{*}}\gamma_{7,\tau j}^{\prime}\mathrm{PI}_{ijlt+\tau} \\
        &+\sum_{\tau=1}^{T+T^{*}}\gamma_{8,\tau j}^{\prime}\mathrm{PI}_{ijlt-\tau}+\sum_{\tau=1}^{T+T^{*}}\gamma_{9,\tau j}^{\prime}\mathrm{CPI}_{ijlt+\tau} \\
        &+\sum_{\tau=1}^{T+T^{*}}\gamma_{10,\tau j}^{\prime}\mathrm{CPI}_{ijlt-\tau}+u_{ijt}^{\prime}, 
    \end{aligned}
}
where $PI_{ijlt}$ and $CPI_{ijlt}$ is the own-brand price index and cross-brand price index among the same category considering support $l$, $D_{ijmt}$ is the non-price promotion 
considering type $m$, $RP_{ijt}$ is the regular price. To allow the interactions between different products, the semiparametric SCAN*PRO-model was proposed:
\eq{
    \begin{aligned}
        \ln q_{kjt}& =m(\ln(PI_{k1t}),\ln(PI_{k2t}),...,\ln(PI_{knt})+\sum_{l=1}^{3}\sum_{r=1}^{n}\gamma_{lrj}^{\prime\prime}D_{lkrt}+\delta_{ji}^{\prime\prime}X_{t}+\lambda_{kj}^{\prime\prime}Z_{k}+u_{kjt}^{\prime\prime}  \\
        &t=1,\ldots,T\mathrm{~and~}k=1,\ldots,K
    \end{aligned}
}
where $m(\cdot)$ is a nonparametric function.

\subsubsection{Online Advertising}
\cite{Dinner2014} studied the croos-channel effects between online and offline advertising. They verified that the cross-channel effect exists, but it could be 
diminished by an indirect effect manifested by its impact on paid search impressions and click-through rate. 

\clearpage
\section{Supply Chain}

\subsection{Manufacturing}
\cite{chen2023quick} investigates the impact of ``quick response'' of the fashion product on the supply chain under
different demand stochasticity. They assume there are two periods, and each period has an identical demand function $d_i=A_i-p_i(i=1,2)$, where $A_i$ is the market size:
\eq{A_i=\begin{cases}H=1+\sigma&\text{ with probability }0.5,\\L=1-\sigma&\text{ with probability }0.5.&\end{cases}}
$\sigma$ is the standard deviation of $A_i$ and a measure on the demand uncertainty. It is assumed that $A_1$ and $A_2$ are positively and perfectly correlated. There are three scenarios for
the manufacturer to decide:
\lis{
    \item \tbf{N}: no quick response;
    \item \tbf{D}: quick response with dynamic wholesale pricing;
    \item \tbf{C}: quick response with committed wholesale pricing.
}
\fig{chen2023quick}{Sequence of Events}{0.5}
To analyze the effect of ``quick response'' on the supply chain (compare scenarios N and D), the authors give 3 intermediate scenarios $\alpha,\beta,\gamma$ and extract 4 effects.
The results show that ``quick response'' may harm the retailer when demand uncertainty is in a medium range.\\
\cite{dong2022food} studies the effects of the auditing systems mode (decentralized v.s. centralized) in the food supply chain
on the food safety outcome and economical payoff. In the decentralized mode, there are four parties in the game:
\lis{
    \item At stage 1, the upstream agency decides on its auditing action $\mathcal{A}_1\in\{\mathcal{Y},\mathcal{N}\}$, there is an auditing cost $c_1$ if $\mathcal{A}_1\ = \mathcal{Y}$;
    \item At stage 2, the upstream supplier decides on the safety level of the raw material $\mathcal{A}_2\in\{\mathcal{S},\mathcal{R}\}$, there is an extra cost $\Delta c_2$ if $\mathcal{A}_2 = \mathcal{S}$;
    \item At stage 3, the downstream agency decides on its auditing action $\mathcal{A}_3\in\{\mathcal{Y},\mathcal{N}\}$, there is an auditing cost $c_3$ if $\mathcal{A}_3\ = \mathcal{Y}$;
    \item At stage 4, the downstream producer decides on the safety level of the food $\mathcal{A}_4\in\{\mathcal{S},\mathcal{R}\}$, there is an extra cost $\Delta c_4$ if $\mathcal{A}_4 = \mathcal{S}$.
}
Each time the agency chooses to audit, there would be a probability $p_1,p_3$ of the TP rate. When the unsafe food enters the market, four agents would face penalties $k_1,k_2,k_3$ and $k_4$.
The centralized mode follows a similar modeling process. The authors deduce the SPME under different parameter settings,
the paper identifies the driving forces behind equilibrium decisions:
\tab{
    \item The \tit{penalty-shield} effect of the downstream agency may lead to risky behavior by the upstream supplier.
    \item A \tit{free-riding} effect between the two agencies may lead to less auditing.
}
The results show that the centralized auditing system may generate inferior performance.\\
\cite{dong20223d} investigate the assortment strategy under the usage of 3D printing. The model assumes that the assortments can be selected from two sets:
the generic set $\cG$ and the 3D-specific set $\cS$. The manufacturer decides on three production sets: the dedicated
technology set $\cD\subset\cG$, the traditional flexible set $\cT \subset \cG$, and the 3D printing set $\cP \subset \cG\cup\cS$.
Then the authors use multinomial-logit model to characterize the market of each product and separate the investment cost to \tit{adoption cost}, \tit{development cost}, and \tit{capacity cost}.
The paper shows the assortment structure: based on the popularity of the products. The Numerical study shows that 3D printing tends to be more valuable when popularities of the generic designs are distributed
more evenly and when popularities of the 3D-specific designs are distributed less evenly.

\subsection{Distribution Channel}

\cite{kouvelis2020should} analytically study the game consisting of three independent parties: a manufacturer, a retailer, and a sales agent. They
give the equilibrium solution for the parties' decision to choose the mode of the value chain under different parameter settings. Key modeling framework:
The manufacturer has a unit cost $c\ge 0$, and consumers have two valuations on the product: $r(r>c)$ or $0$. The efforts of the sales agent can make the probability
of the consumers' non-negative valuation higher.

\clearpage
\section{Revenue Management}

Revenue management is a data-driven system to price perishable assets tactically
at the micro-market level to maximize expected revenue or profit. Some critical reviews
before 2009 can be found in the book \cite{Gallego2019}.
There are some extra summary papers like \cite{Strauss2018},
\cite{Klein2020}.

\subsection{Traditional RM}

\key{\tab{
        \item Protection Level, Booking Limit, Littlewood's Rule
    }}
\ass{What does \textbf{"traditional"} means in RM?}{
    \lis{
        \item The traditional RM system doesn't consider the choice model,
        in particular, it assumes the demands are independent random variables;
        \item Further assumption: the consumer will leave without purchasing if the preferred fare
        class is unavailable (holds when gaps in fares are large enough);
        \item The capacity is fixed, the capacity's marginal profit is zero(can be relaxed);
        \item All booked consumers would arrive (another circumstance see \ref{subsubsec:overbooking}).
    }
}

\subsubsection{Single Resource RM with the-worst-case Arrival Pattern}
\ass{Single Resource RM}{
    \lis{
        \item The units of capacity is $c$, pricing at multiple different level $p_n<\cdots<p_1$;
        \item Low-before-high fare class arrival order: $D_2$ before $D_1$ for example
        (this is the worst case for revenue);
        \item \textbf{Protection level} for customer $j$: leave $y \in \{0,1,\cdots,c\}$ for $D_{j-1},\codts,D_1$; $c-y$ is
        the \textbf{booking limit} which serves $D_j$;
    }
}
\emocool So the problem is to solve the optimal protection level given the current consumer level $j$.\\
Let $V_j(x)$ be the optimal revenue given $D_j$ coming in, $x$ units remained. $V_0(x)=0$ by design.
Let $y$ be the protection level for $D_{j-1},\cdots,D_1$: sales at $p_j$ = $\min\{x-y,D_j\}  $.
The remaining capacity for $D_{j-1},\cdots,D_1$ is $x-\min\{x-y,D_j\}=max\{y,x-D_j\}$.
Now let $W_j(y,x)$ be the optimal solution. We have:
\eq{W_j(y,x)=p_j\mathbb{E}\{\min\{x-y,D_j\}\}+\mathbb{E}\{V_{j-1}(\max\{y,x-D_j\})\}}
\eq{V_{j}(x)=\max_{y\in\{0,\ldots,x\}}W_{j}(y,x) =
\max_{y\in\{0,...,x\}}\left\{p_{j}\mathbb{E}\{\min\{x-y,D_{j}\}\}+
\mathbb{E}\{V_{j-1}(\max\{y,x-D_{j}\})\}\right\}}
\pro{Structure of the Optimal Policy}{
    \begin{equation}
        y_{j-1}^*=\max\{y\in\mathbb{N}_+:\Delta V_{j-1}(y)>p_j\}.
    \end{equation}
    The maximizer of $W_j(y,x)$ is given by $y_j^*,\codts, y_1^*$
}
\re{The optimal solution for $y_j $is independent of the distribution of $D_j$;}
\co{When $j=2$:}{
    \thm{Littlewood's rule}{
        \eq{y_1^*=\max\{y\in\mathbb{N}_+:\mathbb{P}\{D_1\geq y\}>r\}.}
    }
}
\re{The Littlewoods Rule:\lis{
\item The solution depends on the \textbf{fare ratio}: $r:=p_2/p_1$;
\item When the distribution of $D_2$ is continuous: $F_{1}(y)=\mathbb{P}\{D_{1}\leq y\}$.
The optimal protection level is $y_1^*=F_1^{-1}(1-r)=\mu_{1}+\sigma_{1}\notin^{-1}(1-r)$:\lis{
    \item if $r>\frac{1}{2}$, $y_1^*<\mu_1$ and $y_1^*$ decreases with $\sigma_1$;
    \item if $r<\frac{1}{2}$, $y_1^*<\mu_1$ and $y_1^*$ increases with $\sigma_1$;
    \item if $r=\frac{1}{2}$, $y_1^*=\mu_1$;}
\item Using the Littlewoods rule would result in some $D_1$ served by competitors (high spill rates).
Solution: add a penalty to save more seats for the high-fare consumers:
\eq{ y_1^*=\max\left\{y\in\mathbb{N}_+:\mathbb{P}\{D_1\geq y\}>\frac{p_2}{p_1+\rho}\right\}}
}}
\tbf{Multi-fare Heuristics}: The EMSR (expected marginal seat revenue) is a set of heuristic algorithms. The EMSR-a algorithm is based on the idea of
adding protection levels produced by applying Littlewood’s rule to each pair of fare classes. Suppose that we are at stage $j$ and we need to decide the
protection level for fare classes $j-1,j-2,\cdots,1$. We can apply the Littlewood's rule on fare class $k(k<j)$:
\eq{
    y_{kj}^*=\max\left\{y\in\mathbb{N}_+:\mathbb{P}\{D_k\geq y\}>\frac{p_j}{p_k}\right\}.
}
The overall protection level at stage $j$ is given by $y_{j-1}^a:=\sum_{k=1}^{j-1}y_{kj}^*$.\\
The EMSR-b algorithm simply aggregate the fare classes $j-1,\cdots,1$ into one class, the demand is denoted as $D[1,j-1]$, and the aggregated price is defined as:
\eq{
    \bar{p}_{j-1}=\sum_{k=1}^{j-1}p_k\frac{\mu_k}{\mu[1,j-1]}.
}
The algorithm uses the Littlewood's rule to solve the two-fare class problem:
\eq{
    y_{j-1}^b=\max\left\{y\in\mathbb{N}_+:\mathbb{P}\{D[1,j-1]\geq y\}>\frac{p_j}{\bar{p}_{j-1}}\right\}
}
\sep{The Upper Bound of $V_n(c)$}
Assume the demand vector $D=(D_{n},\ldots, D_{1})$ is known in advance, we can transform the problem into a knapsack problem:
\eq{
    \bar{V}(c\mid D):=\max\Bigg\{\sum_{j=1}^np_jx_j:\sum_{j=1}^nx_j\leq c,0\leq x_j\leq D_j\forall j=1,\ldots,n\Bigg\}.
}
This problem has an explicit solution, which is the upper bound for $V^U_n(c)$:
\eq{
\bar{V}(c\mid D)&=\sum_{j=1}^np_j\min\{D_j,(c-D[1,j-1])^+\},
}
where the term $(c-D[1,j-1])^+$ is the remaining capacity after the reserved capacity for the fare class $j-1,\cdots,1$. Taking the expectation on the capacity, we have:
\eq{
    \begin{aligned}
        V_{n}^{U}(c) & \begin{aligned}&=\sum_{j=1}^np_j\mathbb{E}\{\min\{D_j,(c-D[1,j-1])^+\}\}\end{aligned}        \\
                        & =\sum_{j=1}^np_j\left(\mathbb{E}\{\min\{D[1,j],c\}\}-\mathbb{E}\{\min\{D[1,j-1],c\}\}\right) \\
                        & =\sum_{j=1}^n(p_j-p_{j+1})\mathbb{E}\{\min\{D[1,j],c\}\},                                    \\
                        & =\sum_{j=1}^n(p_j-p_{j+1})\sum_{k=1}^c\mathbb{P}\{D[1,j]\geq k\}
    \end{aligned}
}
where we define $p_{n+1}\quad\equiv\quad0$. Recall that in $\bar{V}(c\mid D)$, $D$ are random variables. Since $\bar{V}(c\mid D)$ is concave, by applying Jensen's inequality, we can obtain a more tractable upper bound:
\eq{
    \begin{aligned}
        \bar{V_n}(c) & =\max\left\{\sum_{j=1}^np_jx_j:\sum_{j=1}^nx_j\le c,\quad0\le x_j\le\mu_j\forallj=1,\ldots,n\right\} \\
                        & =\sum_{j=1}^n(p_j-p_{j+1})\min\{\mu[1,j],c\}
    \end{aligned}
}
\sep{The Lower Bound of $V_n(c)$}
The lower bound of the revenue management system is simply applying zero protection level, at the worst case, the bound is given by:
\eq{
    \begin{aligned}
        V_{n}^{L}(c) & =\sum_{j=1}^np_j\mathbb{E}\{\min\{D_k,(c-D[j+1,n])^+\}\}                      \\
                        & =\sum_{j=1}^np_j(\mathbb{E}\min\{D[j,n],c\}\}-\mathbb{E}\min\{D[j+1,n],c\}\}) \\
                        & =\sum_{j=1}^n(p_j-p_{j-1})\operatorname{E}\{\min\{D[j,n],c\}\},
    \end{aligned}
}
where we define $p_{0}\quad\equiv\quad0$.
\pro{}{
    For the multiple fare class problem, we have:
    \eq{
        V_n^L(c)\leq V_n(c)\leq V_n^U(c)\leq\bar{V}_n(c).
    }
}

\subsubsection{Single Resource RM with General Arrival Pattern}
In this model, we consider the effects of time. Assume the horizon is $T$, and time $t$ represents the time left until the end.
Let $M_t$ denote the assortment at time $t$. Assume the fare class $j$ arrive according to a Poisson process with $\lambda_{jt}$, and let $N_{jt}$ denote
the number of consumers that arrive during the last $t$ units of time, which is Poisson with parameter:
\eq{
    \Lambda_{jt}:= \int_0^t\lambda_{js}1 (j \in M_s) ds.
}
Now, refine $V(x)$ to $V(t, x)$ as the maximum expected revenue given capacity $x$ and time $t$. In a short time window $\delta t$, the probability that there is one request for class $j$ is $\lambda_{jt}\delta t+o(\delta t)$,
and the probability that there are no requests is $1-\sum_{k\neq j}\lambda_{kt}\delta t+o(\delta t)$. Now we can have:
\eq{
    \begin{aligned}
        V(t,x) & \begin{aligned}=\sum_{j\in M_t}\lambda_{jt}\delta t\max\{p_j+V(t-\delta t,x-1),V(t-\delta t,x)\}\end{aligned} \\
                & +\left(1-\sum_{j\in M_t}\lambda_{j_t}\delta t\right)V(t-\delta t,x)+o(\delta t)                               \\
                & =V(t-\delta t,x)+\delta t\sum_{j\in M_t}\lambda_{jt}[p_j-\Delta V(t-\delta t,x)]^++o(\delta t),
    \end{aligned}
}
where $\begin{aligned}\Delta V(t,x)&=V(t,x)-V(t,x-1)\end{aligned}$. Subtracting $V(t-\delta t,x)$ from both sides and dividing by $\delta t$, and setting $\delta t\to 0$, we can get the HJB (Hamilton–Jacobi–Bellman) equation:
\eq{
    \begin{aligned}            & \frac{\partial V(t,x)}{\partial t}=\mathcal{R}_t(\Delta V(t,x)).  \\
                        & \mathcal{R}_t(z):=\sum_{j\in M_t}\lambda_{jt}\left[p_j-z\right]^+
    \end{aligned}
}
We can see that fare $j$ is accepted iff $p_j\geq\Delta V(t,x)$. So if we accept fare $j$ at state $(t, x)$, we should accept all fare classes $k<j$. We can define:
\eq{
    a(t,x):=\max\{j:p_j\geq\Delta V(t,x)\}
}
\thm{The structure of $V(t,x)$}{
    The value function $V(t,x)$ is increasing in $t$ and $x$. $\Delta V(t,x)$ is decreasing in $x$. If the arrival rate $\lambda_{jt}$ is stationary across time, $V(t,x)$
    is strictly increasing and concave in $t$.
}
\sep{Discrete-Time Formulation}
In the discrete setting, we can set $\delta t=1$, and drop the $\mathcal{o}(\delta t)$ term. By setting the interval density $k>1$, and letting $\lambda_{jt}\leftarrow\frac1k\lambda_{j,t/k}$, we can have:
\eq{
    \begin{aligned}
        V(t,x) & \begin{aligned}=\sum_{j\in M_t}\lambda_{jt}\max\{p_j+V(t-1,x-1),V(t-1,x)\}\end{aligned}   \\
                & +\left(1-\sum_{j\in M_t}\lambda_{jt}\right)V(t-1,x)                                     & \\
                & =V(t-1,x)+\sum_{j\in M_t}\lambda_{jt}[p_j-\Delta V(t-1,x)]^+                              \\
                & =V(t-1,x)+\mathcal{R}_t(\Delta V(t-1,x))
    \end{aligned}
}

\subsubsection{Network Revenue Management with Independent Demands}

In the airline RM vocabulary, the \tbf{flight leg} refers to a resource and the ODF (\tbf{origin-destination-fare}) refers to the product. In the traditional model, it is often assumed that
the set of possible ODFs is given and the demand for each ODF is independent of the demand for other ODFs.
\sep{Formulations of Dynamic Programming}
Assume there are $m$ resources (flight legs) in the network and let $c^2:=(c_1,\ldots,c_m)$ denote the capacity. We measure time backward (start at $t=T$ and end at $t=0$).
We index each ODF (itinerary) by $1,\cdots, K$, and the price levels for ODF $k$ by $p_{kj}$ for $j\in\{1,\ldots,n_k\}$. Let $\lambda_{tkj}$ denote the
arrival rate at time $t$ of customers interested in ODF $kj$, and $A_k$ be the resource vector (with dimension of $m$). We use the dummy $u_{kj}$ to denote the decision variable: whether we
accept a request for ODF $kj$, the feasible set is constrained by the remained capacity $x$: $A_k \cdot u_{kj} \le x$. Given a short enough period $\delta t$ such that $\sum_{k=1}^K\sum_{j=1}^{n_k}\lambda_{tkj}\delta t \ll 1$, we can write the DP as:
\eq{
    \begin{aligned}
        V(t,x) & =\sum_{k=1}^K\sum_{j=1}^{n_k}\lambda_{tkj}\delta t\max_{u_{kj}\in\{0,1\}}[p_{kj}u_{kj}+V(t-\delta t,x-A_ku_{kj})] \\
                & +\left\{1-\sum_{k=1}^K\sum_{j=1}^{n_k}\lambda_{tkj}\delta t\right\}V(t-\delta t,x)+o(\delta t),
    \end{aligned}
}
where $o(\delta t)$ comes from the property of Poisson process. Subtracting $V(t-\delta t,x)$ from both sides and dividing by $\delta t$, and using the notation
$\Delta_kV(t,x)=V(t,x)-V(t,x-A_k)$, we obtain the HJB equation:
\eq{\label{eq:rmodf}
    \frac{\partial V(t,x)}{\partial t}&=\sum_{k=1}^K\sum_{j=1}^{n_k}\lambda_{tkj}[p_{kj}-\Delta_kV(t,x)]^+,
}
where the term $[p_{kj}-\Delta V_k(t,x)]^+$ is equivalent to the maximum of $p_{kj}u_{kj}+V(t,x-A_ku_{kj})-V(t-\delta t,x)$. Let
\eq{
R_t(u,\Delta V(t,x)):=\sum_{k=1}^K\sum_{j=1}^{n_k}\lambda_{tkj}[p_{kj}-\Delta_kV(t,x)]u_{kj},
}
and consider the optimization problem:
\eq{
    \begin{aligned}
        \mathcal{R}_t(\Delta V(t,x)) & :=\max_{u}R_t(u,\Delta V(t,x))                                                                   \\
                                        & =\sum_{k=1}^K\sum_{j=1}^{n_k}\lambda_{tkj}\max_{u_{jk}\in\{0,1\}}[p_{kj} - \Delta_kV(t,x)]u_{kj} \\
                                        & =\sum_{k=1}^K\sum_{j=1}^{n_k}\lambda_{tkj}[p_{kj}-\Delta_kV(t,x)]^+.
    \end{aligned}
}
The optimal solution to the optimization is to accept all fares for itinerary $k$ that exceed $\Delta_kV(t,x)$, and we can write the HJB equation as:
\eq{
    \frac{\partial V(t,x)}{\partial t}=\mathcal{R}_t(\Delta V(t,x))
}
We can aggregate $\lambda_{tkj}$ to $\lambda_{tk}=\sum_{j=1}^{\boldsymbol{n}_k}\lambda_{tkj}$, and set $\lambda_{tkj}=\lambda_{tk}q_{tkj}$, where $q_{tkj}$
follows the distribution $P_{tk}$. This yields:
\eq{
    \frac{\partial V(t,x)}{\partial t}=\sum_{k=1}^K\lambda_{tk}\operatorname{E}[P_{tk}-\Delta_kV(t,x)]^+=\mathcal{R}_t(\Delta V(t,x)).
}
\sep{The Optimal Policy}
In practice, for convenience \tbf{the single index} formulation is widely used. Let $n=\sum_{k=1}^K n_k$ be the number of ODFs, and $N=\{1,\ldots,n\}$ to be the
set of all ODFs. The single index formulation is given by:
\eq{\label{eq:singleindex}
    \frac{\partial V(t,x)}{\partial t}=\mathcal{R}_t(\Delta V(t,x))=\sum_{j\in M_t}\lambda_{tj}[p_j-\Delta_jV(t,x)]^+
}
In the optimal solution, we have:
\eq{\label{eq:singleindexoptimal}
    u_j^*(t,x)=\begin{cases}1&\text{if }j\in M_t,A_j\leq x,\text{and }p_j\geq\Delta_jV(t,x)\\0&\text{otherwise},\end{cases}
}
\sep{The LP Upper Bound}
\thm{The upper bound for $V(T,c)$}{
    For $V(T,c)$ from problem \ref{eq:singleindex}, we have the bound:
    \eq{
        V(T,c)\leq\mathbb{E}[\bar{V}(T,c|D)]\leq\bar{V}(T,c),
    }
    where the shaper bound comes from:
    \eq{\label{eq:rmsharperbound}
        \begin{aligned}
            \bar{V}(T,c|D):=\max\quad & \sum_{j=1}^np_jy_j                    \\\mathrm{s.t.}\quad&\sum_{j\in N}a_{ij}y_j\le c_i\quad&\forall i\in M \\
            0                         & \le y_j\le D_j\quad & \forall j\in N.
        \end{aligned}
    }
    and the wider bound comes from:
    \eq{\label{eq:rmwiderbound}
        \begin{aligned}
            \bar{V}(T,c)~:=~\max~ & \sum_{j\in N}p_j~y_j                               \\
            \text{s.t.}~          & \sum_{j\in N}a_{ij}~y_j\leq c_i~ & \forall i\in M  \\
            0                     & \leq y_j\leq\Lambda_j~           & \forall j\in N.
        \end{aligned}
    }
    The decision variable $y_j$ corresponds to the number of requests for ODF $j$, and $D_j$ is the aggregated demand for the ODF $j$, which is
    a Poisson with parameter $\Lambda_j=\int_0^T\lambda_{sj}ds$.
}
\proo{The upper bound for $V(T,c)$}{
    For every instance of $D$, the optimal policy \ref{eq:singleindexoptimal} constitute a feasible solution to program \ref{eq:rmsharperbound}, so $V(T,c)\leq\mathbb{E}[\bar{V}(T,c\mid D)]$.\\
    For the second inequality, note that $\mathbb{E}[\bar{V}(T,c\mid D)]$ is concave on $D$, using Jensen's inequality:
    \eq{
        \mathbb{E}[\bar{V}(T,c\mid D)]\leq\bar{V}(T,c\mid\mathbb{E}[D])=\bar{V}(T,c\mid\Lambda)=\bar{V}(T,c).
    }
}
By strong duality, we can transform program \ref{eq:rmwiderbound} to:
\eq{\label{eq:rmdual}
    \begin{aligned}
        \bar{V}(T,c)~=~\min~ & \sum_{i\in M}c_iz_i+\sum_{j\in N}\Lambda_j\beta_j         \\
        \text{s.t.}~         & \sum_{i\in M}a_{ij}z_i+\beta_j\geq p_j\quad\forall j\in N \\
                                & z_i\geq0,~\beta_j\geq0\quad\forall i\in M,~j\in N.
    \end{aligned}
}
At optimality, the dual variable $z_i^*$ is the marginal value of capacity of resource $i$,
while the dual variable $\beta_j^*$ is the marginal value of demand for ODF $j$. Setting $\beta_j=(p_j-\sum_{i\in M}a_{ij}z_i)^+$ so that
we can optimize on $z$ only. Consequently:
\meq{
\sum_{j\in N}\Lambda_j\beta_j=\sum_{j\in N}\Lambda_j(p_j-\sum_{i\in M}a_{ij}z_i)^+=\int_0^T\mathcal{R}_t(A^{\prime}z)dt,\\
\bar{V}(T,c)=\min_{z\geq0}\left\{\int_0^T\mathcal{R}_t(A^{\prime}z)dt+c^{\prime}z\right\}.
}
This provides a way to approximate $\Delta V(T,c)$.
\sep{Heuristics for the Network RM}
Suppose $\{z_i^*:i\in M\}$ from the dual program \ref{eq:rmdual} is known. The \tbf{bid-price heuristic} policy is: making ODF $j$ available iff
$A_j\le x$ and $p_j \ge \sum_{i\in M}a_{ij}z_i^*$.\\
Suppose$y=(y_1^*,\ldots,y_n^*)$ from the dual program \ref{eq:rmwiderbound} is known. The \tbf{probabilistic admission control heuristic} (PAC) policy is: making PDF $j$ available with
probability $\frac{y_j^*}{\Gamma_j}$ whenever $A_j\le x$.\\
Comparing the two heuristic policies, the bid-price uses $m$ parameters (resources) while the PAC uses $n$ parameters (ODF). Usually, $m\ll n$, so bid-price heuristics is more widely used.
\re{
    \tab{
        \item The bid-price heuristic is NOT asymptotically optimal;
        \item PAC is asymptotically optimal;
        \item However, the PAC does not perform as well as the bid-price heuristic with frequent updates over the horizon.
    }
}
\sep{Upgrades and Upsells}
Let $U_j$ be the set of products to fulfill a request $j$: the customers are willing to take any product $k\in U_j$ at the price $p_j$. The value function with upgrades is given by:
\eq{
    \frac{\partial V(t,x)}{\partial t}=\sum_{j\in\mathcal{M}_t}\lambda_j\max_{k\in U_j}(p_j-\Delta_kV(t,x))^+=\sum_{j\in\mathcal{M}_t}\lambda_j(p_j-\tilde{\Delta}_jV(t,x))^+
}
where
\eq{
    \tilde{\Delta}_jV(t,x)=\min_{k\in U_j}\Delta_kV(t,x).
}
In the upsell model, assume that $r_{jk}$ is the revenue by upselling product $k\in U_j$ (r_{jk}>p_j), and there is a probability $\pi_{jk}$ that the customer accepts the upsell.
This leads to the upsell HJB equation:
\eq{
\frac{\partial V(t,x)}{\partial t}=\sum_{j\in M_t}\lambda_j\max_{k\in U_j}\left[\pi_{jk}(r_{jk}-\tilde{\Delta}_kV(t,x))^++(1-\pi_{jk})(p_j-\tilde{\Delta}_jV(t,x))^+\right]
}

\subsubsection{Overbooking} \label{subsubsec:overbooking}
Suppose that the flight capacity is $c$, and the demand is $D$. Assume that passengers who do not show up are given a full refund
and that the unit cost for denied boarding is $\theta$. Let $b$ be the booking limit, then $N:= min(D, b)$ is the number of bookings.
The objective is to maximize:
\eq{
    R(b):=p\mathbb{E}[Z(\min(D,b))]-\theta\mathbb{E}[Z(\min(D,b))-c]^+].
}
The optimal $b^*$ is given by:
\eq{
    b^*=\min\left\{b\geq0:\mathbb{P}\{Z(b)\geq c\}>\frac p\theta\right\}.
}

\subsubsection{Fairness in Airline RM}
\cite{Aslani2014} summarize some unfair pricing phenomena in revenue management, such as the hidden-city ticketing and throwaway ticketing. \cite{wang2016hidden} studies the cause and impact of the hidden-city ticketing:
\ass{\cite{wang2016hidden}}{
\lis{
\item Airlines only provide itineraries with at most one stop. For an O-D (orientation-destination) pair $i\to j$, we use the notation $(k_{ij}^t,p_{ij}^t)_{(i,j)\in\mathcal{O}}$,
where $k_{ij}^t$ is the connection city ($k_{ij}^t=0$ indicates direct flight), and $p_{ij}^t$ is the price;
\item For a passenger with O-D pair $i\to j$, he only considers $k_{ij}^t$ without considering $p_{ij}^t$. Denote $\lambda_{ij}^t(p_{ij}^t)$ as the probability of purchase.
We assume that $\lambda_{ij}^t(p_{ij}^t)$ is known in advance and it is small enough so that at each period, the probability that there are more than one purchases can be ignored.
\item For any $i, j, t$: $\lambda_{ij}^t(p)$ is continuously differentiable and non-increasing in $p$;
\item For any $c>0$, $\lambda_{ij}^t(p)(p-c)$ is quasiconcave in $p$ and there exists a unique maximizer for $\lambda_{ij}^t(p)(p-c)$.
}
}
For one particular airline company, denote the capacity of the flight leg using vector $\boldsymbol{x}$, and the optimal expected revenue as $V^t(\boldsymbol{x})$, we can formulate the dynamic programming:
\eq{
    \begin{gathered}
        V^t(\boldsymbol{x}) =\max_{k_{ij}^t,p_{ij}^t,\forall(i,j)\in\mathcal{O}}\left\{\sum_{(i,j)\in\mathcal{O}}\lambda_{ij}^t(p_{ij}^t)\left(p_{ij}^t+V^{t-1}(\boldsymbol{x}-A_{ij}^{k_{ij}^t})\right)+\left(1-\sum_{(i,j)\in\mathcal{O}}\lambda_{ij}^t(p_{ij}^t)\right)V^{t-1}(\boldsymbol{x})\right\} \\
        =\quad V^{t-1}(\boldsymbol{x})+\max_{k_{ij}^t,p_{ij}^t,\forall(i,j)\in\mathcal{O}}\left\{\sum_{(i,j)\in\mathcal{O}}\lambda_{ij}^t(p_{ij}^t)\left(p_{ij}^t+V^{t-1}(\boldsymbol{x}-A_{ij}^{k_{ij}^t})-V^{t-1}(\boldsymbol{x})\right)\right\}
    \end{gathered}
}
where $A_{ij}^{k}$ is the capacity consumption vector for each flight leg. With boundary conditions $V^0(\boldsymbol{x})=0\quad\forall\boldsymbol{x}$, $V^t(0)=0\quad\forall t$ and $V^t(\boldsymbol{x})=-\infty $ if
$\boldsymbol{x}$ contains any negative entry. Note that we can solve the problem in two steps because the optimization over $k_{ij}^t$ and $p_{ij}^t$ are separated. We can first solve $k_{ij}^t$:
\eq{
\hat{k}_{ij}^t=\arg\max_{k_{ij}^t}V^{t-1}(\boldsymbol{x}-A_{ij}^{k_{ij}^t})
}
then we can solve the optimal $p^t_{ij}$ by maximize:
\eq{
\lambda_{ij}^t(p_{ij}^t)\left(p_{ij}^t+V^{t-1}(\boldsymbol{x}-A_{ij}^{\hat{k}_{ij}^t})-V^{t-1}(\boldsymbol{x})\right)
}
\sep{The Cause of Hidden City}
We can define $c_{ij}^t=V^{t-1}(\boldsymbol{x})-V^{t-1}(\boldsymbol{x}-A_{ij}^{\hat{k}_{ij}^t})$ as the opportunity cost of using capacity $A_{ij}^{\hat{k}_{ij}^t}$, then the optimal price for O-D pair $i\to j$ is determined by:
\eq{
\hat{p}_{ij}^t=\arg\max_p\lambda_{ij}^t(p)(p-c_{ij}^t).
}
This is an easy problem, at the optimal $\hat{p_{ij}}$ (without considering $t$), we have:
\eq{
    \begin{aligned}1-\frac{c_{ij}}{\hat{p}_{ij}}&=-\frac{\lambda_{ij}(\hat{p}_{ij})}{\hat{p}_{ij}\lambda_{ij}^{\prime}(\hat{p}_{ij})}=-(E_{ij}(\hat{p}_{ij}))^{-1}\end{aligned}
}
Where $E_{ij}(p)~=~\frac{p\lambda_{ij}^{\prime}(p)}{\lambda_{ij}(p)}$ is the price elasticity of demand at price $p$. Consider the trip $i\to j\to k$, ($k$ is the hidden city), the prices should satisfy:
\eq{
    1-\frac{c_{ij}}{p_{ij}}&=-(E_{ij}(p_{ij}))^{-1}\quad\text{ and }\quad1-\frac{c_{ik}}{p_{ik}}&=-(E_{ik}(p_{ik}))^{-1}.
}
It's easy to verify that $c_{ij}\le c_{ik}$, combined with that $p_{ij} > p_{ik}$, we can obtain:
\eq{
|E_{ij}(p_{ij})|<|E_{ik}(p_{ik})|
}
\pro{}{
    If the hidden-city phenomenon $i\to j\to k$ happens, then the price elasticity of demand $i\to k$ is greater than that of $i\to j$ at the optimal price.
}
The authors later provide an algorithm that would produce no arbitrage opportunity for hidden-city ticketing. however, under this circumstance, all parties' welfare
decreases compared to the baseline model.

\subsection{Consumer Choice Model and Assortment Optimization}

Assume the products that could be offered is $N:=\{1,\cdots,n\}$, for any assortment $S\subseteq N$ and product $j$, we denote $\pi_j(S)$ as the probability that a consumer will select product $j\in S$.
$\Pi(S)&:=\sum_{j\in S}\pi_j(S)=0$indicates that the consumer does not purchase or purchases outside $S$. Denote $p$ and $z$ as $n$-dimensioanl vectors for prices and costs of each product. The decision maker
should optimize a combinatorial problem:
\eq{
    \mathcal{R}(z):=\max_{S\subseteq N, p\ge z} R(S,p,z):= \sum_{j\in S}(p_j - z_j)\pi_j(S,p)
}
Most of the time, by setting $p_j \to \infty$, we can make $pi_j(N,j)=0$ (remove $j$ out of $S$). Thus, we can transform the problem above to:
\eq{
    \mathcal{R}(z):=\max_{p\geq z}R(p,z) :=\sum_{j\in N}(p_j-z_j)\pi_j(N,p)
}
\sep{Basic Attraction Model (BAM)}
Each $j\in N_+$ has an attraction value $v_j>0$ ($v_0$ indicates no-purchase), the choice model is given by:
\eq{
    \pi_j(S)=\frac{v_j}{v_0+V(S)}\quad\forall j\in S
}
BAM has a great property: for any $S\subseteq T$, we have $\pi_S(T):=\sum_{j\in S}\pi_j(T)$。
\ax{The Luce Axiom}{
    If $\pi_i(\{i\})\in(0,1), \forall i\in T$, then for any $Q\subseteq S_+,S\subseteq T$:
    \eq{
        \pi_Q(T)=\pi_Q(S)\pi_{S_+}(T).
    }
    If $\pi_i(\{i\})=0$ for some $i\in T$, then for some $i\in T$ and $S\subseteq T$:
    \eq{
        \pi_S(T)=\pi_{S\setminus\{i\}}(T\setminus\{i\}).
    }
}
\re{
    Shortage of BAM:
    \tab{
        \item $v_0$ is a fixed parameter and does not depend on the offered products;
        \item This approach ignores the possibility that the consumer may look for the products outside
        $S$ at a later time.
    }
}
Now consider the assortment optimization for BAM, the expected revenue when offering $S\subseteq N$ is:
\eq{
    R(S)=\sum_{j\in S}p_j\pi_j(S)=\frac{\sum_{j\in S}p_jv_j}{v_0+V(S)}.
}
This equation neglects the cost, one can replace $p_j$ by $r_j=p_j-z_j$. The goal is to find the $S^*$:
\eq{
    \mathcal{R}^*=\max_{S\subseteq N}\left.R(S)\right.=\max_{S\subseteq N}\left\{\frac{\sum_{j\in S}p_j\mathrm{~}v_j}{v_0+V(S)}\right\}.
}
Assume that the products are indexed such that $p_1\geq p_2\geq\ldots\geq p_n$, denote the class of nested-by-revenue assortments is $\{E_0,E_1,\ldots,E_n\}$, where 
$E_0=\emptyset$ and $E_j:=\{1,\ldots,j\}$.
\thm{Optimal Assortment for BAM}{
    An optimal assortment for BAM is in the class of nested-by revenue assortments $\{E_0,E_1,\ldots,E_n\}$.\\
    The proof is straightforward： since $R(S)\le \cR^*$, consequently $\sum_{i\in S}r_i v_i \le \cR^*(1+V(S))$, equivalently $\sum_{i\in S}(r_i-\cR^*)v_i\le\cR^*$, 
    which implies that $S^*=\{i\in N:r_i\ge\cR^*\}$.\\
    \emocool This theorem can reduce the number of assortments to search from $2^n$ to $\log n$, without estimating the value of $v_i$.
}
\sep{Generalized Attraction Model (GAM)}
In addition to the attraction value $v_j$, there are \tbf{shadow attraction values} $w_j\in[0,v_j]$, and the selection behaviour is given by:
\eq{
    \pi_j(S)=\frac{v_j}{v_0+W(\bar{S})+V(S)}\quad\forall j\in S,
}
where $\bar{S}=N \setminus S$ and $W(\bar{S}):=\sum_{j\in \bar{S}}w_j$.
\re{
    \tab{
        \item If $w_j =0$ for all products, it recovers to BAM;
        \item Parsimonious GAM (p-GAM) is given by $w_j = \theta v_j$ for all $j$;
        \item In p-GAM, if $\theta=1$, we can get the \tit{independent demand model} (IDM), under which $\pi_j(S)$ is independent of $j$.
    }
}
The GAM has a similar optimal assortment structure to the BAM: denote $\tilde{r}_i:=r_i v_i / (v_i-w_i)$, then $S^*=\{i\in N:\tilde{r}_i \ge \cR^*\}$. 
However, to reduce the search number to $log n$, the estimation for $v_i,w_i$ is needed.
\pro{Independence of Irrelevant Alternatives (IIA)}{
    \tbf{IIA} is a shortcoming for BAM and GAM, under which the following equation holds:
    \eq{
        \frac{\pi_j\left(S\right)}{\pi_j\left(S\cup\left\{k\right\}\right)}=\frac{\pi_i\left(S\right)}{\pi_i\left(S\cup\left\{k\right\}\right)}
    }
    This shouldn't happen when $k$ is a closer substitute for $j$ than for $i$. This equation indicates that the ratio $\pi_i(S)/\pi_j(S)=\frac{v_i}{v_j}$, which is \tbf{independent} of the set $S$. IIA results in the
    \rt{red bus, blue bus paradox}: $\pi_{rbus}(\{rbus,bbus\})=\frac{1}{2}$, but $\pi_{rbus}(\{rbus,bbus1,bbus2\})=\frac{1}{3}$.
}
\subsubsection{Assortment for BAM/GAM with TUM constraints}
In practice, the assortment optimization is usually a constrained optimization:
\eq{
    \begin{aligned}
        \mathcal{R}^*&=\max\quad\frac{\sum_{j\in N}p_j\upsilon_jx_j}{v_0+\sum_{j\in N}\upsilon_jx_j}\\
        \mathrm{s.t.}\quad&\sum_{j\in N}a_{ij}x_j\leq b_i\quad\forall i\in L\\
        &x_j\in\{0,1\}\quad\quad\forall j\in N,
    \end{aligned}
}
The above problem is hard because of the fractional objective function and binary constraint. However, if the matrix $a$ is TU (totally unimodular), It can be 
transformed into linear programming (This is called Cooper Transformation, rewrite $y_j=\frac{v_jx_j}{v_0+\sum_{j\in N}\upsilon_jx_j}$).
\re{
    A unimodular matrix $M$ is a square integer matrix with determinant $1$ or $-1$. It is an integer matrix that is invertible over another integer matrix. A matrix is totally unimodular 
    if every square non-singular submatrix is unimodular. For every equation $Mx=b$, where $M,b$ are integers, and $M$ is totally unimodular, the solutions are all integers.\\
}
\eq{
    \begin{gathered}
        \max\sum_{j\in N}r_jx_j \\
       \sum_{j\in N}x_j+x_0=1, \\
       \begin{aligned}0\leq\frac{x_j}{v_j}\leq x_0\forall j\in N\end{aligned} \\
       \sum_{j\in N}a_{ij}\frac{x_j}{v_j}\leq b_ix_0\forall i\in M. 
    \end{gathered}
}
The $x_j$ can be interpreted as the probability of selecting product $j$. The optimal structure for this optimization problem is to select $\{x_j:x_j>0\}$.
\subsubsection{Random Consideration Set Model}
This choice model assumes that all consumers have the same preference for ordering, each item in the assortment is \tbf{independently} considered with `attention' probability $w_i$. 
So the probability of purchasing $i$ form $S$ is given by:
\eq{
    \pi_i(S)=\lambda_i\prod_{j\succ i,j\in S}(1-\lambda_j)\quad\forall i\in S
}
\ex{Consider an assortment with $S=\{1,2\}$ with $1\prec2$. The consideration set for $\{1\}$ is $w_1(1-w_2)$, $\{\emptyset\}$ is $(1-w_1)(1-w_2)$.}{
    The selection probability for product $2$ is $w_2$, $w_1(1-w_2)$ for product $1$, $(1-w_1)(1-w_2)$ for selecting nothing.
}
\lem{}{
    Let $S\subseteq N$, and let $k\in N$ be such that $i\prec k$ for all $i\in S$, then:
    \eq{
        R(S\cup\{k\})=(1-\lambda_k)R(S)+\lambda_k r_k
    }
    and consequently $R(S\cup\{k\})>R(S)$ iff $r_k>R(S)$. (The proof is obvious).
}
To optimize the assortment for the RCS, denote $E_0=\emptyset$ and $E_j=\{1,\ldots,j\}$ (consecutive sets in the full ranking). 
Set $H(E_0):=0$, $H(E_j)$ is given by:
\eq{
    H(E_j):=H(E_{j-1})+\lambda_j(p_j-H(E_{j-1}))^+.
}
Let $\tilde{E}_j:=\{i\in E_j:p_i>H(E_{i-1})\},\quad j\in N$. Hence $\mathbf{S}^{*}=\tilde{E}_{n}$ is an optimal assortment:
\eq{
    H(E_j)=R(\tilde{E}_j)\geq R(S)~\forall S\subseteq E_j,~j\in N,
}
\subsubsection{Random Utility Models (RUM)}
Consumers observe $v_i=u_i+\epsilon_i$ and choose product $i$ with the highest $v_i$. The distribution of $\epsilon_i$ can lead to different choice models.
\tab{
    \item i.i.d. Gaussians lead to the Probit model;
    \item i.i.d. Gumbels lead to the Multi-nomial model (MNL), which can be generated to avoid IIA.
}
\sep{Multi-Nomial Model (MNL)}
The CDF of Gumbel distribution is given by:
\eq{
    F(x:\nu,\phi)=\exp(-\exp(-\phi^2(x-\nu))),
}
where $\nu$ and $\phi$ are location and scale parameters respectively. The variance is $\frac{\pi^2}{6\phi^2}$ ad the mean is $\nu+\frac{\gamma}{\phi}$ ($\gamma$ is the
Euler constant).\\
There is also a mixture of choice models: mixed-MNL aka latent class MNL (LC-MNL) assumes that the decision-maker belongs to an unobserved market
segment $i\in M$ w.p. $\theta_i>0,\sum_{i\in M}\theta_i=1$. The probability of selecting $j$ from $S$ is given by:
\eq{
    \pi_j(S) = \sum_{i\in M}\theta_i \pi_{j}(S;i)
}
\thm{McFadden}{
If $\epsilon_i$ are i.i.d. Gumbels, by choosing $i$ with the highest $u_i$, we can get：
\eq{
\pi_j(S)=\frac{e^{\phi u_j}}{1+\sum_{k\in S}e^{\phi u_k}}\quad\forall j\in S,
}
MNL is a kind of BAM. All RUM can be approximated well by an LC-MNL.
}
\subsubsection{Nested Logit Model}
In the nested model, there are two stages: first the consumers select either one of the nests or decide to leave; then they decide which product to choose from the selected nest.
We use $M:=\{1,\ldots,m\}$ to denote the set of nests. For product $j$ in nest $S_i$, the probability of choosing $i$ is given by:
\eq{
q_{j|i}(S_i):=\frac{v_{ij}}{V_i(S_i)} = \frac{v_{ij}}{\sum_{j\in S_i}v_{ij}}
}
We use $\gamma_i$ to denote how easily the products in nest $i$ substitute for each other, so we can get:
\meq{
    Q_i(S_1,\ldots,S_m):=\frac{V_i(S_i)^{\gamma_i}}{v_0+\sum_{l\in M}V_l(S_l)^{\gamma_l}}\\
    Q_i(S_1,\ldots,S_m)q_{j|i}(S_i)=\frac{V_i(S_i)^{\gamma_i}}{v_0+\sum_{l\in M}V_l(S_l)^{\gamma_l}}\frac{v_{ij}}{V_i(S_i)}
}
\re{
    \tab{
        \item When $\gamma_i=1$ for all $i$, the Nested Lgoit model reduces to an MNL;
        \item The nested choice model doesn't suffer from IIA;
        \item But it's difficult to decide the order of nesting.
    }
}
The assortment optimization for the nested logit model is to find the sets of products $(S_1,\ldots,S_m)$ to maximize:
\eq{
    R(S_1,\ldots,S_m):=\sum_{i\in M}R_i(S_i)Q_i(S_1,\ldots,S_m)=\frac{\sum_{i\in M}R_i(S_i)V_i(S_i)^{\gamma_i}}{v_0+\sum_{i\in M}V_i(S_i)^{\gamma_i}}.
}
We can define the optimal solution as:
\eq{
    \begin{aligned}
        \mathcal{R}^*&=\max_{\begin{array}{c}(S_1,\ldots,S_m)\\
        S_i\subseteq N\mathrm{~}\forall i\in M\end{array}}R(S_1,\ldots,S_m).
    \end{aligned}
}
This is a binary fractional program if we identify each subset $S_i$ with an incidence vector $x_i=(x_{i1},\ldots,x_{in})\in\{0,1\}^n$. 
Let $V_i(x_i):=\sum_{j\in N}v_{ij}x_{ij}$, $f_i(x_i):=R_i(x_i)V_i(x_i)^{\gamma_i}$ and $g_i(x_i)&:=&V_i(x_i)^{\gamma_i}$. 
Then we have:
\eq{
    \cR^* \ge \sum_{i\in M}f_i(x_i)/(v_0+\sum_{i\in M}g_i(x_i)),
}
which can be rewritten as:
\eq{
    \sum_{i\in M}f_i(x_i)-\mathcal{R}^*\sum_{i\in M}g_i(x_i)\leq\mathcal{R}^*v_0,\forall x_i\in\{0,1\}^n,i\in M.
} 
The inequality is tight at the optimal solution, we can transform it into a linear program:
\eq{
    \begin{aligned}
        \mathcal{R}^*&=\min z\\
        &\mathrm{s.t.}\quad\sum_{i\in M}v_i\leq v_0z\\
        &f_i(x_i)-zg_i(x_i)\leq y_i\quad\forall x_i\in\{0,1\}^n,\quad i\in M,
    \end{aligned}
}
where $z$ and $y_i$ are decision variables ($m+1$). 
\subsubsection{Properties of the Choice Model}
There are two important properties for the MNL-based choice model: \tbf{regularity} and \tbf{submodularity}. 
Regularity means for any assortment $S$, adding a new item $j\notin S$ would reduce the probability of $P_i(S\cup j)$ for any $i\in S$ under the choice model.\\
\defi{Submodularity}{
    Given a choice model $P$, let the demand function: $2^{\mathcal{J}}\mapsto\mathbb{R}$ of the choice model be given by $d(S):=\sum_{j\in S}P_{j:S}$ for any assortment $S\subset\mathcal{J}$. 
    The intuitive understanding of submodularity is the marginal increment of new items on $d$ is always decreasing:
    \eq{
        d(S_2\cup\{k\})-d(S_2)\quad\le\quad d(S_1\cup\{k\})-d(S_1),\quad\forall S_1\subset S_2\subset\mathcal{J},k\in\mathcal{J}\setminus S_2.
    }
} 
\subsubsection{Markov Chain Choice Model}
In the preference list (permutation) model, each decision maker has a strict preference list $\sigma$, which has a distribution $\cQ$. The choice probability is given by:
\eq{
    \lambda_i := \pi_i(N) = \sum_{\sigma} \cQ(\sigma(1) = i)
}
If $i$ is not available, consumers would substitute $j$ w.p. $\rho_{ij}=\sum_\sigma \cQ(\sigma(2)=j | \sigma(1)=i)$. In an example with $n=2$ and
$\lambda = (\lambda_0, \lambda_1, \lambda_2)$, the transition matrix is given by:
\eq{
    \rho=\begin{pmatrix}
        1         & 0         & 0         \\
        \rho_{10} & 0         & \rho_{12} \\
        \rho_{20} & \rho_{21} & 0
    \end{pmatrix}
}
We denote $\phi_j(S)$ the probability that a consumer considers product $j$ but $j\notin S$, we have:
\meq{
    \pi_j\left(S\right)=\lambda_j+\sum_{i\in\bar{S}}\phi_i\left(S\right)\rho_{ij}\quad\forall j\in S\\
    \phi_j(S)=\lambda_j+\sum_{i\in\bar{S}}\phi_i(S)\rho_{ij}\quad\forall j\in\bar{S}.
}
There is an efficient way to solve the assortment optimization problem under the MC choice model. Let $g_i$ be the optimal expected revenue that can be obtained from a consumer that is currently considering product $i$:
\eq{
    g_i=\max\left\{p_i,\sum_{i\in N}\rho_{ij}g_j\right\}\quad\forall i\in N.
}
If $i\in S$, we have $g_i=p_i$. Unfortunately, $g_i$ doesn't have a closed form. One way to find the value of $g_i$ is through value iteration, or we can solve it by LP:
\eq{
    \begin{aligned}
        \mathcal{R}^*&=\min \sum_{i\in N}\lambda_i g_i\\
        \mathrm{s.t.}&\quad g_i \ge p_i \quad\forall i\in N\\
        &g_i \ge \sum_{i\in N}\rho_{ij}g_j \quad\forall i\in N
    \end{aligned}
}
Mention that under the optimal $g^*$, the expected revenue is given by $\sum_i g_i^* \lambda_i$. The optimal assortment is given by $S^*:=\{i\in N:g_i^*=p_i\}$
\re{
    The logic behind the LP is: if we want to find the expected revenue for a given $S$, we need to solve a similar LP, where the first inequality falls in $i\in S$ and the second 
    inequality falls in $i\in \bar{S}$. Which takes exponential time if we want to solve $R(S)$ for all $S$. We can solve one LP to find the maximum $\cR$.
}
\sep{Rank-1 Markov Chain}
\thm{Outer product representation of a rank-one matrix}{
    Every rank-one matrix $A\in \RR^{m\times n}$ can be written as an `outer product':
    \[A=pq^T,\]
    where $p\in \RR^m,q\in\RR^n$. A Markov chain rank-one needs to satisfy $\sum_{j\neq i}p_j=1/q_i$.
}
\re{
    \rt{\tbf{It can be shown that BAM, GAM and RCS are all rank-1 Markov Chain Models}}.
}
\subsubsection{Heuristics in Assortment Optimization}
The assortment optimization problem is usually NP-hard (such as mixed MNL), there are some heuristic algorithm to approximate $\cR^*$.
\sep{Revenue-Ordered}
The RO-assortment is to find $\tau$ and the corresponding assortment under $\tau$: $S(\tau):=\{i\in N:r_i\geq\tau\}.$ The optimal assortment is 
$\mathcal{R}^0:=\max_{\tau\geq0}R(S(\tau))$, and it's proven that $\mathcal{R}^0\geq\frac1n\mathcal{R}^*$.
\sep{The Refined Greedy Heuristic}

\cite{NiP2021} consider the competitive and cooperative assortment games under the Markov Chain Choice Model. They assume There are $\mathcal{M}=\{1,\ldots,m\}$ retailers, each manages a
collection of $\cN_k$ products. We call product $j$ an \tit{exclusive} product to $k$ if $j\in\mathcal{N}_{k}\backslash\mathcal{N}_{-k}$, \tit{common} otherwise. In the game, each retailer needs to dicide
the assortment $S_k \subseteq \cN_k$, and the prices $r_j^k$ are assumed to be fixed. We assume the consumers follow the Markov Chain Choice Model to select the products: with probability $\lambda_j>0$ and $\sum_{j=1}^n \lambda_=1$, a customer
arrives for product $j$, he would choose $j$ from seller $k$ following the distribution $\beta_j^k(S_1,\ldots,S_m)$:
\tab{
    \item TBW
}
If $j\notin S$, the consumer would switch to product $i$ w.p. $\rho_{ji}$.
\sep{Endogenized MNL model}
\cite{wang2017consumer} propose the \tbf{endogenized choice model}, which is an extent on MNL and incorporating the network effects.
\defi{Network Effects}{
    \lis{
        \item \tit{global} network effects: the utility for consuming a product depends on the total level of consumption of the product;
        \item \tit{local} network effects: a consumer gains utility only if his “neighbors” purchase the same product.
    }
}
To capture the impact of market share on consumers’ choice, the utility for product $i$ at time $t+1$ is modeled as:
\eq{
U_i^{t+1}=u_i(q_i^t)+\xi_i^{t+1},
}
where $\mathbf{q}^t:=(q_i^t)_{i\in S}$ is the market share in assortment $S$. The function $u_i(\cdot)$ is assumed to be linear or log-linear:
$u_{i}(q_{i})=\alpha_{i}+\gamma_{i}q_{i}$, where $\alpha_i$ is the intrinsic utility and $\gamma_i$ measures the magnititude pf the network effects.
According to the MNL model, the market share at time $t+1$ is given by:
\eq{
q_i^{t+1}=\frac{\exp(u_i(q_i^t))}{1+\Sigma_{j\in S}\exp(u_j(q_j^t))},\quad\forall i\in S.
}
This could result $\mathbf{q^t}$ to converge to a steady-state $\mathbf{q}$:
\eq{
    q_i=\frac{\exp(u_i(q_i))}{1+\sum_{j\in S}\exp(u_j(q_j))}:=G_i(\mathbf{q}),\quad\forall i\in S,
}
and $q_0=1-\sum_{i\in S}q_i$. This is the \tbf{endogenized MNL model}. Given sufficient condition on $u_i$, $\mathbf{q}^t$ would converge to an unique $\mathbf{q}$
whatever $\mathbf{q}^0$.

\subsection{RM in the Railway System}

The revenue management system has a huge difference compared to the traditional RM system in the airline industry:
\lis{
    \item Regulated by government, usually we can not adjust the prices too frequently;
    \item The seats on different legs are assumed to be `heterogeneous' (even in the same class) because of the \tit{assign-to-seat} policy.
}
Although the train company can not use the pricing technique to increase revenue, it can decide the capacity reserved for those seats for longer
itineraries (\tit{capacity-control problem}). \cite{Zhu2023} is the first paper studying this question.
\ass{\cite{Zhu2023}}{
\tab{
\item There are $N$ homogeneous seats consisting of $M+1$ stops ($M$ legs), the seller can sell itineraries $ij$ at price of $p_{ij}$ for any leg(s) with $j\ge i$;
\item The time is starting from $1$ and ending at $T$, each with an arrival rate $\lambda_{ij}^t$. Assume the time is short enough that $\sum_{1\leqslant i\leqslant j\leqslant M}\lambda_{ij}^{t}\leqslant1$;
\item Let $C^{t}\in\{0,1\}^{N\times M}$ be the capacity matrix ;
\item A policy $\pi$ is defined as mapping $t$ and $C^t$ to a set of binary variables $u^t_{k,ij}$, which represents a request $i\to j$ arriving in time $t$ will be accepted and assigned to seat $k$.
}
}
The authors first investigate the static problem, under which the demands $d_{ij}$ are known in advance and we don't need to consider $t$. We can formulate it into a integer program:
\eq{\label{eq:trainip}
    \begin{aligned}
        \text{maximize}   & \sum_{1\leqslant i\leqslant j\leqslant M}p_{ij}\sum_{k=1}^Nx_{k,ij}                                  \\
        \text{subject to} & \sum_{k=1}^{N}x_{k,ij}\leqslant d_{ij},\quad\forall1\leqslant i\leqslant j\leqslant M,               \\
                            & \sum_{(i,j):i\leqslant\ell\leqslant j}x_{k,ij}\leqslant C_{k\ell},\quad\forall k\in[N],\ell\in[M],   \\
                            & x_{k,ij}\in\{0,1\},\quad\forall k\in[N],1\leqslant i\leqslant j\leqslant M.                        &
    \end{aligned}
}
This is an NP-hard problem, and we can relax it into a linear program:
\eq{\label{eq:trainlp}
    \begin{aligned}
        \text{maximize}\quad   & \sum_{1\leqslant i\leqslant j\leqslant M}p_{ij}\sum_{k=1}^Nx_{k,ij}                                \\
        \text{subject to}\quad & \sum_{k=1}^Nx_{k,ij}\leqslant d_{ij},\quad\forall1\leqslant i\leqslant j\leqslant M,               \\
                                & \sum_{(i,j):i\leqslant\ell\leqslant j}x_{k,ij}\leqslant C_{k\ell},\quad\forall k\in[N],\ell\in[M], \\
                                & x_{k,ij}\geqslant0,\quad\forall k\in[N],1\leqslant i\leqslant j\leqslant M.
    \end{aligned}
}
It can be shown that the structure of $C$ not only influences the computational complexity but also decides whether the solutions of problem \ref{eq:trainip} and \ref{eq:trainlp} are identical.
\defi{Maximal Sequence}{
We call $[u,v]$ a maximal sequence of seat $k$ (denote as $[u,v]\sim C_k$) iff:
\eq{
C_{ku}=C_{k(u+1)}=\cdots=C_{kv}=1\quad\mathrm{and}\quad C_{k(u-1)}=C_{k(v+1)}=0
}
To complete the definition, we need to define $C_{k0}=C_{k(M+1)}=0$ for all $k$. We also define $\mathcal{M}_{uv}(C)=\{k\in[N]|[u,v]\sim C_{k}\}$.
}
\defi{}{}

\clearpage
\section{Dynamic Pricing}

\cite{den2015dynamic} reviews the literature studying dynamic pricing and learning - the study of optimal dynamic pricing in an uncertain environment where characteristics of consumer behavior can be learned from accumulating sales data -
by 2014. There are two main streams of research in dynamic pricing: one assumes the demand function is changing with time, while another assumes static demand but finite inventory.

\subsection{Basic Pricing Theory}

This subsection summarizes the basic pricing theory for \tbf{multi-product momnpoly} firms.\\
\sep{Perspective from the Firm}
The firm's profit function is given by:
\eq{
    R(p,z):=(p-z)^{\prime}d(p)=\sum_{i=1}^n(p_i-z_i)d_i(p_1,\ldots,p_n),
}
Where $z=(z_1,\cdots,z_n)$ is the variable cost vector, $p=(p_1,\cdots,p_n)$ is the price vector, $d(p)$ is the demands.
Currently it's popular to set $p_i \in [0,\infty]$, whereby setting $p_i=\infty$ is equivalent to not offering product $i$.\\
Consider the revenue as a function only of $z$, given by:
\eq{\mathcal{R}(z):=\max_{p\in X}R(p,z),}
Where $X$ is the set of allowable prices.
\thm{The Decreasing Covex Property}{
    $\cR (z)$ is \tbf{decreasing convex} in $z$.
    \proo{}{
        The decreasing property is obvious since $R(p,z)$ is decreasing in $z$, here is the proof for convexity.\\
        For any $z,\tilde{z}$:
        \eq{
            \begin{aligned}
                \begin{aligned}\mathcal{R}(\alpha z+(1-\alpha)\tilde{z})\end{aligned} & =\max_{p\in X}R(p,\alpha z+(1-\alpha)\tilde{z})                                      \\
                                                                                        & =\max_{p\in X}R(\alpha p+(1-\alpha)p,\alpha z+(1-\alpha)\tilde{z}))                  \\
                                                                                        & =\max_{p\in X}\left[\alpha(p-z)^{\prime}+(1-\alpha)(p-\tilde{z})^{\prime}\right]d(p) \\
                                                                                        & =\max_{p\in X}\left[\alpha R(p,z)+(1-\alpha)R(p,\tilde{z})\right]                    \\
                                                                                        & \leq\alpha\max_{p\in X}R(p,z)+(1-\alpha)\max_{p\in X}R(p,\tilde{z})                  \\
                                                                                        & =\alpha\mathcal{R}(z)+(1-\alpha)\mathcal{R}(\tilde{z}).
            \end{aligned}
        }
        \emogood \rt{This proof is very similar to proving the convexity of the hyperplane}.
    }
}
Transforming the revenue function as a function only to $z$ is useful. Because by \tbf{Jensen's Inequality}, $\mathbb{E}[\mathcal{R}(\mathbb{Z})]\geq\mathcal{R}(\mathbb{E}[\mathbb{Z}])$.
Their differences can be interpreted as the difference between a dynamic pricing policy $p(Z)$ that responds to changes in $Z$ and a static policy $\mathcal{R}(\mathbb{E}[\mathbb{Z}])$,
the larger the variance of $Z$, the larger the gap between them. This implies that an interesting phenomenon, in the revenue management domain, considering dynamic pricing, the firms can prefer 
stochastic production cost over static cost.
\sep{Perspective from the Consumers}
To answer whether consumers are better off with pricing policy $p(Z)$ or $p(\bE(Z))$,
we can frame this using the \tbf{utility theory} (\cite{chen2019welfare}).
Assume that consumers purchase a non-negative vector $q=(q_1,q_2,\cdots,q_n)$ of products, their \tit{net utility} is given by:
\eq{
S(q,p):=U(q)-q^{\prime}p,
}
where $U(q)$ us an increasing concave function. Then the \tit{optimal surplus} can be conputed by:
\eq{
    \mathcal{S}(p):=\max_{q\geq0}S(q,p)
}
where the solution $q^{\star}=d(p)=\nabla^{-1}U(p)$. under the first-order condition. Similar to the analysis from the firm's
perspective, we can get $\mathbb{E}[\mathcal{S}(P)]\geq\mathcal{S}(\mathbb{E}[P])$.
And at last, we can achieve:
\eq{
\mathbb{E}[\mathcal{S}(p(\mathbb{Z}))]\geq\mathcal{S}(\mathbb{E}[p(\mathbb{Z})])\geq\mathcal{S}(p(\mathbb{E}[Z])).
}

\subsection{Dynamic Pricing and Reinforcement Learning}

\cite{den2023mathematical} provides a mathematical definition on algorithmic collusion, there are four conditions for collusion:
\lis{
    \item the algorithm should be implementable in practice (\tbf{incomplete information} about the payoff structure);
    \item the algorithm should be rational, ensuring a good performance against various alternative algorithms is also essential;
    \item supracompetitive outcomes;
    \item no illegal communications.
}
Let $\nu:=\{\nu_{a_1,a_{-1}}:(a_1,a_{-1})\in\mathcal{A}_1\times\mathcal{A}_{-1}\}\in\mathcal{V}_0$ be the collection of probability distributions of $(Y_{1,a_1,a_{-1}},Y_{-1,a_{-1},a_1})$, we call $\nu$ the \tit{true parameter} in the repeated game.
\cite{misra2019dynamic} is probably the first paper that incorporates reinforcement learning (MAB) algorithm
into dynamic pricing.
\ass{\cite{misra2019dynamic}}{
For consumers:
\lis{
    \item She has stable preferences $v_i$ doesn't change over time;
    \item Her choice satisfies weak axiom of revealed preference: if $v_i>p$, then she would buy it;
    \item Heterogeneity among consumers can be separated as observable and unobservable heterogeneity: $v_i=f(\mathbb{Z}_i)+\nu_i$;
    \item There are bound for $\nu_i: \nu_{i}\in[-\delta,\delta]$；
}
Model Setting:
\lis{
\item There are $K$ prices that a firm can choose: $p\in\{p_1,\ldots,p_K\}$;
\item sample mean $\bar{\pi_{kt}}=1/n_{kt}\sum_{\tau=1}^{n_{kt}}\pi_{k\tau}$, a policy is defined as $p_t=\Psi(\{p_\tau,\pi_\tau|\tau=1,\ldots,t-1\})$;
\item The regret is given by: $\mathrm{Regret}(\Psi,\{\pi(\mathfrak{p_k})\},t)=\pi^*t-\sum_{k=1}^{K}\pi(p_k)\mathbb{E}[n_{kt}]$
}
There are $K$ segments of consumers, each segment is denoted by $\theta_k$ and $s$, and their behaviour follows:
\lis{
    \item $D(p_k)_{s,t}=0$ is consistent with consumers being of type $\{\theta_1,\cdots,\theta_{k-1}\}$;
    \item $D(p_k)_{s,t}=1$ is consistent with consumers being of type $\{\theta_k,\cdots,\theta_{K}\}$;
}
}
In this paper a novel algorithm (UCB-PI) is proposed and verified that can outperform other algorithms.\\

\subsection{Multi-agent Dynamic Pricing}

\subsubsection{Algorithmic Collusion using Reinforcement Learning}
\cite{calvano2019algorithmic}, \cite{calvano2020artificial}, \cite{calvano2020protecting} are the first few papers studying the algorithmic collusion of RL algorithm in a Non-monopoly market.
In these papers, the demand function is assumed as
\eq{
q_{i,t}=\frac{e^{\frac{a_i-p_{i,t}}\mu}}{\sum_{j=1}^ne^{\frac{a_j-p_{j,t}}\mu}+e^{\frac{a_0}\mu}}.
}
Where the parameters $a_i$ indicate the vertical differentiation and $a_0$ is an inverse index of aggregated demand. $\mu$ indicates the
horizontal differentiation. Each firm's objective is to maximize the profit: $\pi_{i,t}=\left(p_{i,t}-c_{i}\right)q_{i,t}$, where $c_i$ is the cost.
For a set of given parameters, we can compute both the Bertrand-Nash equilibrium of the one-shot game and the monopoly prices for each firm, denoted by $P^N$ and $P^M$.
The action space for each firm is given by $\left[\mathbf{p}^N-\xi(\mathbf{p}^M-\mathbf{p}^N),\mathbf{p}^M+\xi(\mathbf{p}^M-\mathbf{p}^N)\right], \xi>0$.
To ensure the state space is finite, the state space is defined as $s_t=\{\mathbf{p}_{t-1},\ldots,\mathbf{p}_{t-k}\}$. So for each player $i$, we have $|A|=m$ and $|S|=m^{nk}$. \\
This paper assumes each firm utilizes the Q-learning algorithm with $\epsilon$ exploration:
\meq{
Q_{t+1}(s,a)=\big(1-\alpha\big)Q_t\big(s,a\big)+\alpha\big[\pi_t+\delta\max_{a\in A}Q_t\big(s',a\big)\big]\\
\epsilon_t=e^{-\beta t}.
}
To test the final results, the authors further define the \tbf{average profit gain} $\Delta$:
\eq{
    \Delta\equiv\frac{\bar{\pi}-\pi^N}{\pi^M-\pi^N}
}
$\Delta=0$ corresponds to the competitive outcome and $\Delta=1$ corresponds to the perfectly collusive outcome. Their simulation shows that Q-learning pricing
algorithms systematically learn to collude. The collusion is typically partial and is enforced by punishment in case of deviation. The article has no analytical proof.
The authors also investigate the algorithm's strategy toward competitor's deviation: at epoch $0$ the supra-competitive outcomes have been achieved, and at epoch $1$ one party is defected by force, but then
back to using the algorithm to decide as normal. The dynamics can be shown in the picture below:
\fig{collusion deviation}{collusion deviation}{0.5}
\cite{den2022artificial} verified this collusion but criticized that this paper doesn't reveal the `true' algorithmic collusion. 
\cite{asker2022artificial} and \cite{asker2023impact} extend Calvano's research methodology using the Bertrand Pricing model, and find that during such setting, the game would converge to the collusive outcome when both agents use the \tit{synchronous updating} scheme.\\
\cite{hansen2021frontiers} follows the research and verifies whether the UCB algorithm would lead to collusion in the duopoly market. They assume a symmetric linear demand:
\eq{
d_j^*(p_j,p_{-j})=\alpha-\beta p_j+\gamma p_{-j}.
}
The competitive price and the joint monopoly (supra-competitive) price can be calculated:
\eq{
    p^D = \frac{\alpha}{2\beta-\gamma}, p^M=\frac{\alpha}{2(\beta-\gamma)}.
}
In the experiment, a firm observes a noisy outcome:
\eq{
\pi_{t}(p_{j},p_{-j})=p_{j}d_{j}^{*}(p_{j},p_{-j})+\varepsilon_{j,t},
}
where $\varepsilon_{j,t}\sim U[-\frac{1}{\delta},\frac{1}{\delta}]$, and $\delta$ is the Signal-to-Noise-Ratio (SNR). The two firms use
UCB-tuned algorithm to dynamically price their products:
\eq{
    \begin{aligned}
        \mathrm{V}_{k,t}         & =\overline{\pi_{k,t}^{2}}-\bar{\pi}_{k,t}^{2}+\sqrt{\frac{2\log t}{n_{k,t}}},                    \\
        \mathrm{UCB-tuned}_{k,t} & =\bar{\pi}_{k,t}+\sqrt{\frac{\log t}{n_{k,t}}\mathrm{min}\left(\frac14,\mathrm{V}_{k,t}\right),}
    \end{aligned}
}
The simulation results show that the higher the SNR, the higher the chance of converging to the collusive outcome. (A simple $\epsilon$ greedy algorithm would converge to Nash-equilibrium).
\thm{\cite{hansen2021frontiers}}{
    Suppose the true demand function is deterministic, and both firms use independent UCB algorithms. Then,
    \lis{
        \item The prices are always exactly correlated from the third period onward;
        \item The fraction of times in the first $t$ periods that \tit{either} seller charges $p_L$ converges to zero as $t\to \infty$.
    }
}
\proo{}{
    Assume for each firm there are only two prices that can be chosen: $p_L, p_H$, where $p_L$ can be thought of as the competitive price and $p_H$ is the supra-competitive price.
    After the first two rounds, there can only be two cases:
    \lis{
        \item Matched prices, that is, $(p_H, p_H)$ in one round and $(p_L, p_L)$ in the other;
        \item Mismatched prices.
    }
    In both cases, the two agents have the same index for the two arms. Because the decision process and the demand function are
    deterministic, they will pose the same action forever. Although they might choose $p_L$ at some time when $n_L$ is small, the actions would be made simultaneously, and revert to
    $p_H$ ar the next epoch.
}
\cite{klein2021autonomous} discuss the algorithmic collusion of Q-learning in another economic environment. Consider two agents $i\in \{1, 2\}$ price in infinitely discrete time indexed by $t\in\{0, 1, 2, \cdots \}$.
Price adjustments occur \tbf{sequentially}: agent 1 can only adjust its price in odd-numbered periods, agent 2 in even-numbered periods. Each agent selects its price from a discrete set $P=\{0,\frac{1}{k},\frac{2}{k},\ldots,1\}.$
Firm $i$ profit at time $t$ is derived as $\pi_i(p_{it},p_{jt})=p_{it}D_i(p_{it},p_{jt})$, its objective function can be expressed as $\sum_{s=0}^{\infty}\delta^s \pi_i(p_{i,t+s}, p_{j,t+s})$.
The demand is linear:
\eq{
D_i(p_{it},p_{jt})=\begin{cases}1-p_{it}      & \mathrm{if~}p_{it}<p_{jt}   \\
            0.5(1-p_{it}) & \mathrm{if~}p_{it}=p_{jt}   \\
            0             & \mathrm{if~}p_{it}>p_{jt} &\end{cases}.
}
This setting provides a monopoly collusive price of $0.5$ with a per-firm profit of $0.125$. Here, we use the Markov Perfect Equilibrium (MPE):
\eq{\label{eq:klein2021}
V_i(p_{jt})=\max_p\big[\pi_i(p,p_{jt})+E_{p_{j,t+1}}\big[\delta\pi_i(p,p_{j,t+1})+\delta^2V_i(p_{j,t+1})\big]\big]
}
An action pair $(R_1, R_2)$ is an MPE if equation \ref{eq:klein2021} holds for both agents. To align with the game setting, the Q-learning algorithm is simply modified,
the state $s_t$ is the opponent's price $p_{j,t}$, and the updating formula is given as:
\eq{
    \tilde{Q}(p_{i,t}, s_t) = \pi_i(p_{i,t}, s_t) + \delta \pi_i(p_{i,t}, s_{t+1}) + \delta^2 \max_p Q_i(p, s_{t+1})
}
Both agents use the $\epsilon$-greedy strategy to select their actions. The performance metric is the average profit in the last 1000 runs.
In this paper, the author distinguishes whether a strategy is Nash-equilibrium by looking at the optimality of both agents:
\eq{
    \Gamma_i(p_i,p_j)\doteq\frac{Q_i(p_i,p_j)}{\max_pQ_i^*(p,p_j)},
}
If $\Gamma=1$ for both agents, the action pair $p_i,p_j$ can be regarded as a Nash equilibrium. Among all Nash equilibriums, some are collusive equilibriums when profitability is above the competitive benchmark.
It is found that agents have high chances to collude, and \tit{Edgeworth price cycles} are observed. However, unlike theoretical analysis in past works, these price cycles are deterministic: It is always the same firm that undertakes the
costly action of “resetting” the price cycle by jumping up in price rather than undercutting, with the other firm able to free-ride on this.
\fig{Edgeworth price cycles}{Edgeworth price cycles}{0.6}
\re{Two Limitations of \cite{klein2021autonomous}
    \tab{
        \item The convergence is not guaranteed;
        \item Q-learning is restricted to playing pure strategies.
    }
}
\cite{calvano2023algorithmic} surveys some papers on algorithmic collusion and further categorizes the collusion into two groups:
\tab{
    \item \tbf{genuine collusion}: the algorithms do not cut their prices because they anticipate retaliatory responses by rivals;
    \item \tbf{spurious collusion}: the algorithms do not cut their prices simply because they are missing opportunities to earn more.
}
\cite{aouad2021algorithmic} proposes an efficient algorithm to support the tacit collusion in the multi-agent assortment game (solving the collusive factor is NP-hard, thus needs an efficient way). 
\cite{meylahn2022learning} presents a price algorithm that can learn to collude when used by both players in a duopoly with unknown demand function and no inventory constraints. 
They assume there is a \tbf{unique} maximizer $\mathbf{p}^{col}$ and a unique Nash Equilibrium $\mathbf{p}^{com}$. 
There are five phases in their algorithm, in the first and second phases, the two players explore and estimate the collusive outcomes; in the third and fourth phases, the two players explore and estimate the competitive outcomes; in the 5th stage, the two players make decision based on the previous outcomes. 
The authors proved that this algorithm can converge to the supracompetitive outcomes when used by both firms and can respond competitively to the reaction function when used by a single firm.\\ 
\cite{loots2023data} proposed an algorithm that can collude under the multinomial logit demand function, and this algorithm can learn another firm's private information using the public pricing information. \\
\cite{abada2023artificial} studies the dynamic pricing collusive scenario using the model of the electricity markets. Agents are indexed by $i\in I$ and their actions time $t$ are denoted by $a_i^t$, $a_i^t\in\RR$ is positive
(negative) if the agent $i$ sells to (buys from) the market at time $t$. There is a group of fringe producers $a_f^t$, so the overall demand at time $t$ is given by:
\[
    D^t=\sum_ia_i^t+a_f^t\quad\forall t\in1,\ldots, T.
\]
But the fringe producers are price-takers and can be ignored, thus the residual inverse demand function is given by:
\[
    p^t=\tilde{p}^t-\beta^t\sum_ia_i^t\quad\forall t\in1,\ldots,T,
\]
Each agent has a capacity $l_i^t$ with a linking function $l_i^{t+1}=l_i^t-a_i^t$, facing the trading constraint as well as the stocking constraint. 
Denoting $\mathbf{a}^{t}\equiv(a_{1}^{t},a_{2}^{t},\ldots,a_{I}^{t})\in\mathbb{R}^{|I|}$, we can write the payoff function for each agent as (ignore the discounted factor):
\eq{
    \Pi_i=\sum_{t=1}^Tp^t(\mathfrak{a}^t)a_i^t,
}
and the corresponding optimization can be written as:
\eq{
    \begin{aligned}
        \mathrm{Max}_{a_i^1,\ldots,a_i^T}&\sum_{t=1}^Tp^t(\mathbf{a}^t)a_i^t\\
        \mathrm{s.t.}&\quad l_i^{t+1}=l_i^t-a_i^t&\forall t\in0,\ldots,T-1,\\
        &|a_i^t|\leq K_i&\forall t\in1,\ldots,T,\\
        &0\leq l_i^t\leq L_i&\forall t\in1,\ldots,T.
    \end{aligned}
}
Each agent uses Q-learning with $\epsilon$-greedy exploration, the state space is defined as the historical prices and capacities (in a discrete style). 
The experiments show that the agent can converge to the collusive outcomes, and each agent exhibits the punishment for any deviation. 
The authors explain this `seemingly' collusion as a result of \tbf{lack of exploration}, to support this intuition, they artificially force the learning process to keep exploring at some steps, and the results learned are closer to the full-competition phenomenon. 
Finally, the authors provide two methods to avoid(reduce) this collusion:
\lis{
    \item Local learning: prevent the aggregated learning;
    \item The regulator can play in the market as a participant.
}
\sep{Some Theoretical Works from a Game Theory Perspective}
\cite{dolgopolov2024reinforcement} studied whether a prisoner's dilemma would converge to the collusive outcomes using bandit RL algorithms. 
He proved that by using a large enough learning rate, the game would converge to the collusive outcomes, the proof uses one-shot deviation principle, and to simply the proof, the author discretizes the Q-value. 

\clearpage
\section{Platform Operations Management}

\cite{rietveld2021platform} reviews literature in platform competitions.

\cite{wang2023} uses the game theory framework to analyze the cross-licensing policy initiated by Qualcomm, which provides some insights
for the up-stream manufacturing company:
\lis{
    \item The supplier shouldn't adopt the cross-licensing policy if the inferior manufacturer's cost of innovation is high;
    \item Cross-licensing may achieve a higher level of total innovation if the superior manufacturer's cost of innovation is low;
    \item The superior manufacturer can benefit from cross-licensing, if innovation is costly but the manufacturers' costs of
    innovation is similar.
}
\cite{deng2021urban} studies the business pattern operation for the last-mile delivery in the city using the game theory framework.
In the model, the decision maker can decide whether to launch a UCC (urban consolidation center) or to operate a P2P platform for carriers to share the capacity.
There are $n$ carriers in the region, each jas a delivery task with volume $v_i$. $v_i=v_L$ with a probability $\lambda$, and $v_i=v_H(>v_L)$ with a probability
$1-\lambda$. Each carrier can deliver his own task with a fixed cost of $c$ and a variable cost of $m$, or deliver by the platform.
In the UCC mode, the UCC incurs a fixed cost of $C$, a variable cost of $M$, and a variable subsidy $S$. The center charges a unit fee $\bar{p}$ for the carriers, after observing the unit fee, each carrier
decides to deliver on his own or to outsource the task independently. In the P2P mode, the platform follows a similar decision process.
The equilibrium analysis shows the critical condition (related to $m, n$) under which the UCC or the P2P mode can reduce more social-environmental cost.

\subsection{Hotel Platform}
\sep{An overview of Airbnb}
\cite{Dolnicar2021} provides a comprehensive illustration of all aspects of Airbnb, including the business model, competitive landscape, and the
regulations of Airbnb. \\
\cite{guttentag2019progress} reviewed some tier c papers about the progress on Airbnb, their main focus is on the loyalty and motivation of
guests and hosts, Airbnb's regulation and culture, as well as Airbnb's impact on the tourism sector.\\
Airbnb makes money by renting out property that it doesn't own,
the hosts can be an individual or a company (But Airbnb doesn't own property, it's just an intermediary). (\cite{Folger2023})
In 2017, Airbnb invested in Niido, a hotel-like apartment program managed by Airbnb. In 2020, Airbnb ended its
partnership with Niido apartments. During the whole process, only 2 apartments were in service.\\
\cite{zhang2022makes} studies how Airbnb property demand changed after the acquisition of \tit{verified} images. Variables description:
\tab{
    \item Treatment variable: 212 properties had verified photos by the end of April 2017, the remaining 7,211 did not;
    \item Property demand: purchased date, the number of days in a month in which the property was open, blocked, and booked. $\frac{booked}{open}\times 100$
    \item Property Price: the price is endogenous because of the random demand shocks, characteristics of competing properties
    were used as IVs (The logic is that the characteristics of competing products are unlikely to be correlated with unobserved
    shocks in the demand for the focal property. However, the proximity of the characteristics of a property and its competitors influences
    the competition and as a result, the property markup and price). Cost-related variables are collected including residential utility fees.
    \item Property Photos: CNN architecture was used to predict the quality of a photo (dummy variable).
}
\tbf{Other Identification Techniques}:
\tab{
\item DiD model: $DEMAND_{itcym}=INTERCEPT+\alpha TREATIND_{it}+\lambda CONTROLS_{it}+PROPERTY_i+SEASONALITY_{cym}+\varepsilon_{it}$;
\item PSW method: calculate the prosensity score $\widehat{ps_i(X_i)}$ and use IPTW ($\omega_i(T,X_i)=\frac{T}{p\widehat{s_i(X_i)}}+\frac{1-T}{1-p\widehat{s_i(X_i)}}$) to weight the sample.
\item Relative time model was used to test the common trends assumption, Rosenbaum bounds test was used to test the validation of PSW methods.
}
Besides the work above, the authors investigate what makes a picture good. They listed 3 components (composition, color, figure-ground relationship) including 12 attributes and used the
following regression to give some human-interpretable suggestions:
\eq{
    \begin{aligned}
        DEMAND_{itcym}= & INTERCEPT+\alpha TREATIND_{it}                          \\
                        & +\mu IMAGE_{-}COUNT_{it}                                \\
                        & +\rho_1\text{ВАТНRООМ}_-\text{РНОТО}_-\text{КАТI}O_{it} \\
                        & +\rho_2BEDROOM_-PHOTO_-RATIO_{it}                       \\
                        & +\rho_3\text{КIТСНЕ}N_-\text{РНОТО}_-\text{КАТI}O_{it}  \\
                        & +\rho_4LIVING_-PHOTO_-RATIO_{it}                        \\
                        & +\eta IMAGE\_ATTRIBUTES_{it}                            \\
                        & +\lambda CONTROLS_{it}+SEASONALITY_{cym}
                        & + PROPERTY_i + \epsilon_{it}
    \end{aligned}
}
\cite{chen2023regulating} investigates the professional players' effects on the non-professional host. They define a host who has more than
one properties on the platform simultaneously as professional players, and use a quasi-experiment (OHOH policy) and DiD model to analysis
whether competition effects or differentiation effects is dominant.\\
They predict 2 propositions:
\lis{
    \item If differentiation effects dominate, then OHOH would not affect the supply and price of non-professional hosts;
    \item If competition effects dominate, then OHOH would increase the supply and price of non-professional hosts.
}
\eq{
    Y_{it}=\mu_i+\nu_t+\beta{\cdot}1(\text{Policy})_{it}+\gamma^{\prime}\textbf{X}_{it}+\varepsilon_{it}
}
Their results show that the competition effects dominate the role of professional hosts.
\cite{farronato2022welfare} investigate the peer's entry on consumer's welfare in the accommodation industry using a structural model.\\
\tbf{Intuition}: Peer hosts are responsive to market conditions, expand supply as hotels fill up, and keep hotel prices down as a result.

\subsection{Platform Owner's Entry}

\key{Complementory Markets, Spillover Effects}

\cite{chen2023price} is the first paper considering the asymmetric information between the platform owner and the third-party sellers, and the effects of the information disadvantage
on consumer's welfare. They assemble data from Amazon: 122000 products, each with two sellers offering the same product. \tit{The information asymmetry}: when Amazon’s competitors make sales,
Amazon adjusts its prices accordingly; conversely, third-party sellers do not react to their competitors’ sales.\\
After observing the empirical evidence of the information asymmetry, they design a theoretical model and identify the parameters. Using the structural regression, they find that by giving
information to third-party sellers. Both the Amazon, third-party sellers and the consumers' welfare would be increased.

\cite{zhu2018competing} surveys empirical studies that examine the direct entry of platform owners
into complementary product spaces.

\cite{zhu2018competing} studies the entry of Amazon platform. Logit regression is adopted to verify the following \tbf{Hypothesis}:
\tab{
    \item Platform owners are more likely to compete with a complementor when its products are successful;
    \item Platform owners are less likely to compete with a complementor when its products require significant platform-specific investments to grow.
}
\tbf{Identification Techniques}:
\tab{
    \item The sales ranking is used as the proxy variable for the sales of the products;
    \item To overcome the impact of the referral rates by category-level fixed effects;
    \item To measure the seller's platform-specific investment, they calculate the seller's average answers;
}

\cite{he2020impact} investigates the effects and the mechanisms of platform owner's entry on third-party's online and offline demands using a B2B shopping platform's data.
They use DiD to identify that the platform owner's entry does harm the demands (more in online channels than offline channels).\\
What's more, they propose three mechanisms to explain the effects:
\lis{
    \item Competition Effects: The owner can appropriate value from the third-party sellers (only significant for online channels);
    \item Spillover Effects: increasing the exposure or awareness of the products (not the same as the mobile app market);
    \item Disintermediation effect: sellers would use defensive strategy to transact outside the platform (mostly in offline channel) see
    \cite{gu2021trust} and \cite{ha2022channel}.
}
Finally, DDD and PSM were adopted to identify the heterogeneity of the effects between large and small third-party stores.\\
\cite{deng2023can} use data from JD.com and provide an unexpected result, thet
\cite{wen2019threat} and \cite{foerderer2018does} focus on complementors' reactions, especially on innovation strategy for the platform owner entry.\\
\cite{shi2023comparing} investigates the timing of the platform owner's entry on the value creation.
\defi{Timing of Owner's Entry}{
    \tab{
        \item \tbf{Platform Complementors}: actors that offer an application that brings additional value to platform users when used in combination with the platform;
        \item \tbf{Early-entry}: the entry occurs when the ratio between the current and the eventual complementary market's size is low;
        \item \tbf{Late-entry}: entry to a relatively mature complementary market;
        \item \tbf{Value creation}: the activities geared toward increasing the perceived attractiveness of the platform ecosystem among customers and
        measure it as changes to complement popularity among customers. (proxy variable)
    }
}
\tbf{Identification Techniques}:
\tab{
    \item Use the \tit{the number of reviews} as the proxy for the popularity of complements (dependent variables);
    \item \tit{functional specificity} measures the heterogeneity of a complement based on the complexity of services offered by the compliment;
    \item Follows \cite{zhu2018competing} to account for platform-specific investments by \tit{interfacce coupling}: whether the complement connects with the platform core;
    \item To verify the exogeneity of Amazon's entry decision, a logit regression is conducted to test the number of reviews (popularity)
    does not influence Amazon's decision;
    \item PSM method is adopted.
}
\eq{
    Reviews_{it}=\alpha+\beta Treated_i \times After_t + \delta Controls_{it} + C_i + T_i + \epsilon_{it}
}
Many papers analyze the platform owner's entry from a game-theory perspective.\\
\cite{hagiu2020creating} investigate the owner's entry decision in the complementary markets.
\ass{Creating platforms by hosting rivals}{
    \tab{
        \item There are two companies $M$ and $S$: $M$ sells product $A$ and $B_M$, $S$ can only sell $B_S$;
        \item The costs to produce are all set to zero;
        \item The number of customers is normalized to one, $\lambda_A$ customers are only interested in $A$ ($A$-type), $1-\lambda_A$ are interested in both ($B$-type);
        \item $u_A>0$ for both type of customers, $u_B>0$ and $u_S=u_B+\Delta$ for $B$-type;
        \item There are costs for consumers to go to each store with a cost $\sigma$, and $0<\sigma<\min\{u_A,u_B\}, \Delta<\sigma$.
    }
}
The authors analyze two conditions: without hosting and hosting. By solving the equilibrium given two conditions, they find the following conclusion:
\lis{
    \item In the without hosting condition, $M$ would dominant the whole market, the profit for $S$ is zero;
    \item Even without transfer for $S$ to sell products in the platform $M$, $M$ can still benefit if $\lambda_A\leq\sigma/u_A$ and $\Delta>\lambda_A(u_A-\sigma)^+F/(1-\lambda_A)$.
}
\cite{cheng2022impact} analyze the owner's entry of the incumbent sellers in the E-commerce platform.
\ass{
    Sell-on Contract
}{
    Without the entry, the demands for the two incumbent sellers are:
    \eq{
        \begin{aligned}D_1^s&=\frac12[1-p_1^s+\theta(p_2^s-p_1^s)]\\D_2^s&=\frac12[1-p_2^s+\theta(p_1^s-p_2^s)]\end{aligned}
    }
    After the entry, it is modified to:
    \eq{
        \begin{gathered}
            D_{r}^{o}= \frac{1}{2+a}\left[a-p_{r}^{o}+\frac{1}{2}[\delta(p_{1}^{o}-p_{r}^{o})+\delta(p_{2}^{o}-p_{r}^{o})]\right] \\
            D_{1}^{o} =\frac{1}{2+a}\left[1-p_{1}^{o}+\frac{1}{2}[\theta(p_{2}^{o}-p_{1}^{o})+\delta(p_{r}^{o}-p_{1}^{o})]\right] \\
            D_{2}^{o}= =\frac{1}{2+a}\left[1-p_{2}^{o}+\frac{1}{2}[\theta(p_{1}^{o}-p_{2}^{o})+\delta(p_{r}^{o}-p_{2}^{o})]\right]
        \end{gathered}
    }
    Where $a$ captures the strength of pltform's own brand. $\theta,\delta$ are the cross sensitivity.
}
They find that in both the sell-to and sell-on conditions, the entry of the platform owner may not harm the incumbent sellers.
What's more, comparing to the entry of a new third-party sellers, the incumbent sellers profit more when facing the entry of the owner.
\cite{lai2022fulfilled} investigates the introduction of Amazon's fullfillment program (FBA) on the third-party sellers in the E-commerce platform.
\ass{FBA on the third-party sellers}{
    \tab{
        \item Both Amazon and the third-party sell substitute products, at price of $p_A$ and $p_S$;
        \item Amazon procure the products from OEM supplier at price $w$ with a cost $c_0$, the third-party obtain its products with a cost $g$;
        \item The third party pays $r$ share of its revenue to Amazon as commission;
        \item The delivery cost for different means are all $c$, the Amazon provides a better service $S_A>S_s$, the third party can pay $T>c$ to use FBA;
    }
    Without FBA, the demands are given by:
    \eq{
        \begin{aligned}q_A&=Q_A-p_A+\beta p_S+\alpha s_A-\eta s_S,\\q_S&=Q_S-p_S+\beta p_A+\alpha s_S-\eta s_A,\end{aligned}
    }
    With FBA, the demands are given by:
    \eq{
        \begin{aligned}q_A&=Q_A-p_A+\beta p_S+\alpha s_A-\eta s_A,\\q_S&=Q_S-p_S+\beta p_A+\alpha s_A-\eta s_A.\end{aligned}
    }
    They assume are informations are common knowledge.
}
There are three decision makers: Amazon: $p_A, T$, Third-party: $p_S, FBA$, OEM: $w$:
\eq{
    \begin{array}{l}\Pi_{A,N}=(p_A-w-c)q_A(p_A,p_S)+rp_Sq_S(p_A,p_S)\\\Pi_{S,N}=[(1-r)p_S-g-c]q_S(p_A,p_S)\\\Pi_{M,N}=(w-c_0)q_A(p_A,p_S)\end{array}
}
To given insights which policy would benefit different parties, they took the following steps:
\lis{
\item In the without-FBA condition, given $w$, solve the equilibrium for $p_A^{\star}(w),p_S^{\star}(w)$;
\item Substituting them into the decision model of OEM, solve the optimal $w^{\star}$ in the SOSC condition.
\item In the FBA condition, repete above steps, solve the equilibrium for $p_A^{\star}(w),p_S^{\star}(w),w^{\star}$;
\item Substituting them into the decision model of OEM, solve the optimal $T^{\star}$.
}
They find some interesting insights:
\lis{
    \item The third-party benifits from FBA when $T<\bar{T}$;
    \item Amazon can benefit from FBA if $\eta$ is either small or large;
    \item The FBA program can achieve a \tit{win-win-win} outcome for the third-party seller, Amazon and its OEM supplier.
}

\subsection{Consumer Polarization}

Consumer polarization is a topic in consumer research. \textbf{Group-Polarization Hypothesis} suggests that
group discussion generally produces attitudes that are more extreme in the direction of the average of
prediscussion attitudes in a variety of situations. Works like \cite{rao1991polarization} provides a mathematical
presentation for this phenomenon (in the domain of preference):
\eq{U_{\mathrm{s}}=\sum_{i=1}^{m}\lambda_{i}u_{i}+\phi(\bar{u}-K)\\
0\le \lambda_i \le 1, \sum_{i=1}^{m}\lambda_i = 1, \phi \ge 0}
In this model, the $\bar{u}$ is the algebraic mean of all consumers' utility, and $K$ is the \textbf{Pivot Point}.
Rewrite this formula:
\eq{\begin{align}
        U_{\mathrm{g}} & = \sum_{i=1}^{m} \left( \lambda_{i} + \frac{\phi}{m} \right) u_{i} - \phi K \\
                        & = w_{0} + w_{1} u_{1} + w_{2} u_{2} + \cdots + w_{m} u_{m}
    \end{align}\\
    \begin{array}{c}w_{0}\leq0; \\\sum\limits_{i=1}^{m}w_{i}\geq1;
        \\\\0\leq\frac{w_{0}}{1-\sum w_{i}}\leq1.\end{array}}

\re{\tab{
        \item \cite{Zhao2023} use experiment results to suggest that eWOM (electronic word of mouth) polarization (the
        degree of eWOM to which positive and negative sentiments are simultaneously strong) would decrease the consumers' intention
        to purchase, mediating by the enhancement of attitude ambivalence.
        (Ambivalence is a psychological state where a person endorses both positive and negative attitudinal positions)
        \item \cite{Iyer2021} use game theory framework, to get the conclusion that sequential decision-making could
        reduce the polarization.
    }}

\subsection{Network Effect}

\textbf{Network Effect} and \textbf{Network Externality}:

\cite{Narayan2011} verifies that peer influence affects attribute preferences via a Bayesian updating mechanism. In
their model, the utility is given as follows:
\eq{U_{ijp}^R=X_{jp}\beta_i^R+\lambda_i\varepsilon_{ijp}^R}
Where $U_{ijp}$ is the utility of consumer $i$ for product $j$ given choice set $p$, $X_{jp}$ is the attribute of
product $j$ in the choice set $p$, $\beta_i^R$ is the customers' weights. The Bayesian updating process is given below:
\eq{\begin{aligned}\beta_{ik}^R=\rho_{ik}\beta_{ik}^l+(1-\rho_{ik})\frac{\sum_{i=1,i\neq i}^Nw_{ii}\beta_{ik}^l}
        {\max\left[\left(\sum_{i=1,i\neq i}^Nw_{ii}\right),1\right]}, \\\mathrm{where~}0\leq\rho_{ik}\leq1.\end{aligned}}

Other research on peer influence:
\re{\item The consumers' interaction and social connections have a proposition proposed in \cite{Zhang2017} for
    their goal attainment and spending: a positive linear term plus a negative squared term;}

\subsection{Online Gaming}

Many industrial news about online gaming can be found in \cite{Chen2017}.
In \cite{lei2022revenue}, the dissertation fully discussed loot box pricing, matchmaking, and
price discrimination with fairness constraints.

\subsubsection{Play-Duration and Spending}

\cite{Zhang2017}'s work shows a nonlinear effect of social connections and interactions
on consumers’ goal attainment and spending: A positive linear terms and a negative squared term. Mechanism:
functional in providing useful information or tips that can facilitate goal attainment,
but would raise information overload problems.\\

Player engagement can be embodied by many specific metrics, such as time or money
spent in the game, the number of matches played within a time window, or churn risk.
\cite{Chen2017} define churn risk as the proportion of total players stopping playing the game over a period of time.

\subsubsection{Matchmaking}

Matchmaking connects multiple players to participate in online PvP games. (PvP(Player-versus-Player) games,
which cover many popular genres, such as multiplayer online battle arena (MOBA), first-person shooting (FPS),
and e-sports, have increased worldwide popularity in recent years.)\\

The past matchmaking strategy matches similar skilled players in the same round (SBMM), the current MM system focuses on improving
the players' engagement and decreasing the churn rate. For example, in \cite{Chen2017} EOMM (Engagement Optimization MatchMaking) is
proposed to minimize the churn rate.\\

\cite{chen2021matchmaking} propose an algorithm to maximize the cumulative active players.
\ass{Chen 2021 MatchMaking}{\lis{
\item players can have heterogeneous skill levels: level $1$ to level $K$;
\item the outcome of each match is a Bernoulli random variable: $p_{kj}=1-p_{jk},p_{kk}=0.5$,
$p_{kj}>0.5$ if $k>j$;
\item player's skill level is fixed: \textit{relative} level;
\item and their state depends on the win-loss outcomes of the past $m$ matches: $g\in\mathcal{G}$ ($2^m+1$
possible cardinality);
\item A geometric losing churn model: players churn with a fixed probability, starting
from the second loss in a row;
\item  $P_{win}^{k},P_{lose}^{k}\in[0,1]^{|\mathcal{G}|\times|\mathcal{G}|}$ is the transition matrix
of level $k$ player's engagement state;
\item $M_{kj}=p_{kj}P_{win}^{k}+(1-p_{kj})P_{lose}^{k}$ is the aggregate transition matrix.
($\bar{\matgcal{G}}$ is the reduced aggregate transition matrix);
\item using the fluid matching model and assume players are infinitely divisible;
}}
The \textbf{Dynamic Programming} formulation: $f_{kg,jg^{\prime}}$ is the amount of $kg$ players
matched with $jg^{\prime}$ players, $s^t_{kg}$ is the number of $kg$ players at time $t$.\\
\textbf{FB} \textit{flow balance} constraints:
\eq{\begin{aligned}
                                                                            & \sum_{j=1}^K\sum_{g^{\prime}\in\bar{\mathcal{G}}}f_{kg,jg^{\prime}}^t=s_{kg}^t,k=1,\ldots,K,\forall g\in\bar{\mathcal{G}}, \\
        \sum_{j=1}^K\sum_{g^{\prime}\in\bar{\cal G}}f_{jg^{\prime},kg}^t & =s_{kg}^t,k=1,\ldots,K,\forall g\in\bar{\mathcal{G}},                                                                      \\
        f_{kg,jg^{\prime}}^{t}                                           & =f_{jg^{\prime},kg}^t,j=1,\ldots,K,k=1,\ldots,K,\forall g\in\bar{\mathcal{G}},g^{\prime}\in\bar{\mathcal{G}}               \\
        f_{kg,jg^{\prime}}^{t}                                           & \geq0,j=1,\ldots,K,k=1,\ldots,K,\forall g\in\bar{\mathcal{G}},g^{\prime}\in\bar{\mathcal{G}}
    \end{aligned}}
\textbf{ED} \textit{evolution of demographics}:
\eq{\mathbf{s}_k^{t+1}=\sum_{j=1,\ldots,K}
\left(\mathbf{f}_{kj}^t\mathbf{1}\right)^\top\left(\bar{M}_{kj}+N_k\right)k=1,\ldots,K}
The value-to-go function is:
\eq{\begin{aligned}V^{\pi}(\mathbf{s}^{t})=&\sum_{k=1}^{K}\sum_{g\in\bar{\mathcal{G}}}s_{kg}^{t+1}+\gamma V^{\pi}(\mathbf{s}^{t+1})\\&\text{subject to (FB), (ED).}\end{aligned}}
The above model can be formulated in a linear programming style:
\thm{Chen 2021 MM LP Formulation}{
    \eq{\begin{aligned}
            V^{*}(\mathbf{s}^{0}) & =\max\sum_{t=1}^\infty\gamma^{t-1}\sum_k\sum_{g\in\bar{\mathcal{G}}}s_{kg}^t                                                                           \\
                                    & \mathrm{s.t.}\sum_{j=1}^{K}\sum_{g^{\prime}\in\bar{\mathcal{G}}}f_{kg,jg^{\prime}}^{t}=s_{kg}^{t},\forall k,\forall g\in\bar{\mathcal{G}},t=0,1,\ldots \\
                                    & \sum_{j=1}^K\sum_{g^{\prime}\in\bar{\mathcal{G}}}f_{jg^{\prime},kg}^t=s_{kg}^t,\forall k,\forall g\in\bar{\mathcal{G}},t=0,1,\ldots                    \\
                                    & f_{kg,jg^{\prime}}^t=f_{jg^{\prime},kg}^t,j=1,\ldots,K,k=1,\ldots,K,\forall g\in\bar{\mathcal{G}},g^{\prime}\in\bar{\mathcal{G}},t=0,1,\ldots          \\
                                    & f_{kg,jg^{\prime}}^t\geq0,j=1,\ldots,K,k=1,\ldots,K,\forall g\in\bar{\mathcal{G}},g^{\prime}\in\bar{\mathcal{G}},t=0,1,\ldots                          \\
                                    & \mathbf{s}_{k}^{t+1}=\sum_{j=1,\ldots,K}\left(\mathbf{f}_{kj}^{t}\mathbf{1}\right)^{\top}\left(\bar{M}_{kj}+N_{k}\right),\forall k,t=0,1,\ldots
        \end{aligned}}
}
\re{
    \tab{
        \item Using an optimal matchmaking policy instead of SBMM may reduce the required
        bot ratio significantly while maintaining the same level of engagement.
    }
}

\clearpage
\section{Behavioral Operations Management}

\cite{Donohue2020} surveys the behavioral issues in the OM literature: the behavioral OM studies the difference between the \tbf{normative models} and the actual observed behaviors. 
Two important examples: 
\tab{
    \item \bt{pull-to-center} bias: the participants’ ordering decisions are systematically pulled toward mean demand and away from the “optimal” ordering decision (the mechanism is not unified);
    \item The influence of \tit{trust} on information sharing in the distributed supply chain
}

\subsection{Behavioral Economics}

\subsubsection{Individual choice theories}
\sep{Prospect Theory} 
There are many cases where the consumers' choice is inconsistent with the expected utility theory. The key idea of the prospect 
theory is that people evaluate an outcome based on the comparison of the outcome with some subjective reference point, rather than based on the absolute outcome itself: 
the evaluation of a lottery relies on a value function and a nonlinear probability function:
\fig{Prospect Theory}{The Valuation in Prospect Theory}{0.7}
\tab{
    \item the valuation of the outcomes of a lottery is classified as gains or losses compared to the reference point: consumers are usually `risk aversion' in the gain domain (concavity) and `risk seeking' in the loss domain (convexity);
    \item the consumers tend to overreact to extreme events that have small probabilities (see the solid curve above the dashed line).
}
\tbf{Framing effetcs}: different presentations of the same choice problem may yield reversed preferences. \tbf{Endowment effects}: 
People value a product or service more once they establish property rights to it. 
\sep{Choice Over Time}
The \tbf{hyperbolic} discounting model consider a sequence of dated consumptions denoted by $\{(x_i,t_i),i=1,\ldots,n\}$, the utility is given by:
\eq{
    U(x_1,t_1;\ldots;x_n,t_n)=\sum_{i=1}^nv(x_i)\phi(t_i).
}
$v(\cdot)$ is the value function with the same structural properties as the one in prospect theory, and $\phi(\cdot)$ is the discount function with a hyperbolic form: 
$\phi(t)=(1+\alpha t)^{-\beta/\alpha}$, with $\alpha,\beta>0$. When $\alpha\to 0$, the hyperbolic form reverts to the standard exponential discount function: 
$\phi(t)=e^{-\beta t}$ (which is more tractable).

\subsubsection{Social Preferences}
\sep{Reciprocity and Inequality Aversion}
\ex{Consider the bargaining game with two players: a proposer (he) and a responder (she). He is provided with an endowment $X$, and he offers 
an amount $Y\in[0,X]$ to her, and she decides whether to accept it. The unique SPNE is to offer the smallest amount while accepting all the offers. 
However, the experiments give contradictory results.}{
    There are two explanations for this phenomenon:
    \tab{
        \item \tbf{Reciprocity:} proposers make high offers because they fear rejections of low offers by responders;
        \item \tbf{Fairness}: proposers make high offers because they are purely generous.
    }
}
The inequality aversion model captures the fairness: for $n$ individuals, player $i$'s utility is given by:
\eq{
    \begin{aligned}U_i(x)=x_i-\frac{\alpha_i}{n-1}\sum_{j\neq i}\max\{x_j-x_i,0\}-\frac{\beta_i}{n-1}\sum_{j\neq i}\max\{x_i-x_j,0\}\end{aligned}
}
where $\alpha$ is the envy weight and $\beta$ is the guilt weight: $0\le\beta_i\le1$ and $\beta_i \le \alpha_i$ because people are more averse to being worse o than being better off.
\sep{Trust and Reputations}

\subsubsection{Complexity and Bounded Ratinoality}
\defi{Level-k Thinking}{A level-0 player is a `native' type who picks choices randomly.  A level-1 player believes that other players are level-0 type and best responds to the level-0
types strategy. A similar definition for level-k thinking.}

\subsection{Behavioral Revenue Management}

Chapter 20 in \cite{ozer2012oxford} discussed how human deviations affect pricing management. The first stream studies how individual choice theory affects consumer pricing.
\sep{Context effects}
\fig{Demonstration of Attraction Effect and Compromise Effect}{Demonstration of Attraction Effect and Compromise Effect}{0.6}
The \rt{attraction effect} happens when adding an \tit{asymmetrically dominated} product or a \tit{relatively inferior} product (comparing with product $i$) in the assortment, the selection probability of $i$ increases. \\
There is another kind of context effect called the \rt{compromise effect}: suggesting that the choice probability of an alternative can be increased by making the alternative become a compromise/an intermediate alternative in the choice set. 
This effect is a consequence of loss aversion implied by prospect theory.
\tab{
    \item the product $C$ can lead an attraction effect for $B$;
    \item the product $D$ can lead a compromise effect for $B$.
}
The last context effect is \rt{similarity effect}: consider three products $A,B,C$, where $B$ and $C$ share more similarity. For assortment $A, B$, the inclusion of new product $C$ could decrease the purchasing probability of $B$ more than $C$ because they are more similar.
\sep{Price Presentation Effects}
One well-known example is the \tbf{price-ending effect}. The digits 0, 5, and 9 are the most frequently used rightmost (i.e., ending) digits in retail prices, with the digit 9 especially dominant for discounted prices.\\
There is another stream of literature studying how social preferences affect pricing and how behavioral issues can influence the B2B contract. \\

\cite{Kocabiyikoglu2015} study and compare decision-making behavior in an experimental setting under the newsvendor model and the two-fare revenue management model (the two models have a clear analytical solution). 
Under the two models, the decision-maker needs to decide on the \tit{order quantity} or the \tit{protection level}:
\eq{
    x^*=F^{-1}\left(\frac{c_U}{c_U+c_O}\right),
}
where $F$ is the distribution (class 1 for RM), $c_U$ is the underage cost and $c_O$ is the overage cost. 
So the analytical solutions for the models are $x^*=F^{-1}\left(\frac{p-c}p\right)=F^{-1}\left(\frac{p_1-p_2}{p_1}\right)$ respectively. 
The authors employ a $2\times 2$ design in the experiments: NV/RM $times$ high/low-cost, and adapt the uniform distribution on the demand. 
They ask several participants to play the games for several rounds, the main results show that in the low-cost condition, the decision-makers tend to make decisions with smaller values than the analytical solution, and vice versa. 
They analyze the difference in decisions between the RM and NV problem: RM is always higher than NV, which indicates that the decision-maker identifies the perceived losses due to leftovers as the main driver of the behavioral patterns.\\

\subsubsection{Choice Model \& Assortment Optimization Considering Behavioral Issues}
\cite{wang2018prospect} solves the assortment optimization problem considering the prospect theory. 
Define $r(S,\mathbf{p})$ as the reference price for assortment $S$ when the current price vector is $\mathbf{p}$, and the utility for each product $i$ can be expressed by:
\eq{
    U_i=\alpha_i-\beta p_i+\lambda(r(S,\mathbf{p})-p_i)^+-\gamma(p_i-r(S,\mathbf{p}))^++\xi_i.
}
$\lambda<\gamma$ ($\lambda>\gamma$) indicates the ``loss-averse'' (``gain-seeking'') effect respectively, the utility of no-purchase is $U_0=\xi_0$. 
After obtaining the utility under the reference price, the consumers follow an MNL model (Gumbel distribution for $\xi_i$). 
The formation of the reference price could be varied: highest, average, etc, and the model does not suffer from IIA because a new product into the assortment could influence the selection probability by changing the reference price. 
For an assortment $S$ at prices $\mathbf{p}$, the \tit{consumer surplus} is defined as:
\eq{
    \log \left(1+\sum_{i\in S}\exp(\alpha_i-\beta p_i+\lambda(r(S,\mathbf{p})-p_i)^+-\gamma(p_i-r(S,\mathbf{p}))^+ )\right)
}
Now considering the lowest price as the reference price: $r_l(S,\mathbf{p})=\min_{k\in s}p_k$, and the selecting probability is:
\eq{
    q_i(S,\mathbf{p};r_l)=\frac{\exp(\alpha_i-(\beta+\gamma)p_i+\gamma\min_{k\in S}p_k)}{1+\sum_{j\in S}\exp(\alpha_j-(\beta+\gamma)p_j+\gamma\min_{k\in S}p_k)}.
}
\pro{}{
    For the lowest price reference, the choice probability $q_i(S,\mathbf{p};r_l)$ is decreasing in $p_i$ and is increasing in $p_j$ for any $j\in S,j\ne i$. 
}
Using this framework, one may find that sometimes increasing the price of the products (with the lowest price) can increase the total welfare.
\sep{Assortment Planning and Pricing}
First, assume the prices of all products are predetermined, so the problem can be formulated by:
\[
    \max_{S\subseteq\mathcal{N}}\mathbb{R}(S,\mathbf{p};r_l).
\]
In this case, a quasi-markup-ordered assortment is optimal:
\defi{Quasi-markup-ordered assortment}{
    Suppose that products are labeled in the price-decreasing order: $p_{1}\geq p_{2}\geq\cdots\geq p_{N}$. 
    For any $2\le n\le N$, relabel all products in $S_{n-1}:=\{1,2,\cdots,n-1\}$ in the markup(revenue)-decresing order. 
    For any $i\le n-1$, the subset $\{k:k\le i,k\in S_{n-1}\}\cup \{n\}$ is called a quasi-arkup-ordered assortment.
}
\proo{QMOA Optimal}{
    Since the reference is the lowest price, then there are $N$ circumstances. For each $n=1,\cdots,N$m the firm need to select the other products from $S_{n-1}$:
    \eq{
        \max_{S\subseteq S_{n-1}}R(S\cup\{n\},\mathfrak{p};r_l).
    }
    Then, the choice probability for each product $i$ is:
    \eq{
        q_i(\mathcal{S}\cup\{n\},\mathbf{p};r_l)=\frac{\exp(\alpha_i-(\beta+\gamma)p_i+\gamma p_n)}{1+\sum_{j\in S\cup\{n\}}\exp(\alpha_j-(\beta+\gamma)p_j+\gamma p_n)}
    }
    Let $\max_{S\subseteq S_{n-1}}R(S\cup\{n\},\mathbf{p};r_l)=x$. We can write:
    \eq{
        \begin{aligned}
            x=H_n(x)& :=(p_{n}-c_{n}-x)\cdot\exp(\alpha_{n}-\beta p_{n})  \\
            &+\max_{S\subseteq S_{n-1}}\sum_{i\in S}(p_i-c_i-x)\cdot\exp(\alpha_i-(\beta+\gamma)p_i+\gamma p_n).
        \end{aligned}
    }
    Note the RHS has a familiar structure and for every $n$, there is a revenue-order assortment. 
}
There are two parameters needed for searching the optimal assortment: the reference price parameter $n$ and the revenue order parameter $i$, so this assortment optimization costs $O(n\log n)$. 
The author also considers the joint problem of optimizing the assortment as well as determining the price:
\eq{
    \max_{S,\mathbf{p}}R(S,\mathbf{p};r_l).
}
This objective function is not jointly concave even without the reference price. 
\defi{Same-markup/Same-price policy}{
    Label the products using cost, $c_1\ge c_2\ge\cdots\ge c_N$. 
    For $1\le n\le N$, products $1,2,\cdots,n$ are referred as high-cost products, whereas products $n + 1, n + 2, \cdots N$ are called low-cost products. 
    The pricing policy that charges the same markup for high-cost products and the same price for low-cost products is called the same-markup/same-price policy.
}
\thm{The Optimal Policy}{
    It is optimal to offer all products, and the same-markup/same-price policy is optimal.
    \proo{}{
        Assuming that product $i$ is not included in the optimal assortment, it's obvious to note that if including $i$, by setting its price and revenue sufficiently high, the revenue would increase, so the optimal assortment is $N$. 
        Let $x^*$ as the optimal profit:
        \eq{
            x^*=\max_{\mathbf{p}}\sum_{i\in\mathcal{N}}(p_i-c_i-x^*)\cdot\exp\left(\alpha_i-(\beta+\gamma)p_i+\gamma\min_{k\in\mathcal{N}}p_k\right).
        }
        We use $y$ to substitute $\min \mathbf{p}$, and the optimization can be transform to:
        \eq{
            \max_{\mathbf{p}\geq y}\sum_{i\in\mathcal{N}}(p_i-c_i-x^*)\cdot\exp(\alpha_i-(\beta+\gamma)p_i+\gamma y).
        }
        This means that we can optimize $p_i$ for each $i$ separately, and the optimal solution is given by $p_i=\max(c_i+x^*+1/(\beta+\gamma),y)$.
    }
}
Following the conclusion of the last theorem, the AOP can be transformed to solving $N$ two-dimensional problem (for each $n$):
\eq{
    \begin{gathered}
        \max_{m+c_n\geq p}\left(\sum_{i=1}^nm\cdot\exp(\alpha_i-(\beta+\gamma)(m+c_i)+\gamma p)\right. +\sum_{i=n+1}^N(p-c_i)\cdot\exp(\alpha_i-\beta p)\biggr) \\
        \cdot\left(1+\sum_{j=1}^n\exp(\alpha_j-(\beta+\gamma)(m+c_j)+\gamma p\right)\left.+\sum_{j=n+1}^N\exp(\alpha_j-\beta p)\right)^{-1}. 
    \end{gathered}
}
However, for a given $n$, the problem is not jointly concave in $p$ and $m$, so we need an efficient algorithm. 
Let $R(\mathcal{N},\mathbf{p};r_l)=x$, we can get:
\eq{
    \begin{aligned}
        \boldsymbol{x}= & \Gamma(x;\gamma)  \\
        := & \max_{\mathbf{p}}\sum_{i\in\mathcal{N}}\Big\{(p_{i}-c_{i}-x)\mathrm{exp}\Big(\alpha_{i}-(\beta+\gamma)p_{i}+\gamma\mathrm{min}p_{k}\Big)\Big\}. 
    \end{aligned}
}
Note that $\Gamma(x;\gamma)$ is decreasing in $x$, the effecient algorithm contain two parts: first compute $\Gamma(x;\gamma)$, then use the bisection method to search $x^*$ such that $x^*=\Gamma(x^*;\gamma)$.\\
\cite{rooderkerk2011incorporating} is the first paper incorporating context effects into the \tbf{attribute-based} choice model. 
Given participant $h$ (can be understood as the response parameter heterogeneity), item $i$ and the choice set $S$, the utility $z_{hi}^S$ is defined by:
\eq{
    \mathrm{z_{hi}^S=\underbrace{V_{hi}}_{\text{partworth utility}}+\underbrace{VC_{hi}^S}_{\text{context-dependent utility}}+\varepsilon_{hi}^S}
} 
Assume every item can be described along $Q$ attributes, each attribute has $L_q$ levels. 
Denote $X_{iql}$ as the dummy indicating whether item $i$ has level $l$ of attribute $q$, the partworth utility is given by:
\eq{
    \mathrm{V_{hi}=\sum_{q=1}^{Q}\sum_{l=2}^{L_{q}}\beta_{hql}x_{iql}},
} 
and the contextual component covers all context effects: compromise, attraction, and similarity:
\eq{
    \mathrm{VC_{hi}^S=\beta_h^{COM}COM_i^S+\beta_h^{ATT}ATT_i^S+\beta_h^{SIM}SIM_i^S}.
}
When the parameters $\beta$ are all zero, this is reduced to a RUM. 
The utility of no-choice is set as $\mathrm{z_{h0}^S=\beta_{h0}+\varepsilon_{h0}^S.}$
The measure of the three effects is based on the Euclidean distance within the attribute space, $x_{kq}$ is the level of attribute $q$ for product $k$, define the middle point $M$ as:
\eq{
    \mathrm{x_{Mq}^S=\frac{\min_{k\in S}x_{kq}+\max_{k\in S}x_{kq}}2,\quad q=1,...,Q.}
} 
the closer one item from $M$, the larger the compromise effect, so $COM_i^s=-d_{iM}^S$. 
To measure the attraction effect, we need the \tit{preference vector} to normalize the effect, which is a $Q$-dimensional vector:
\eq{
    \mathrm{v_{preference}^S=}\left[\left(\max_{\mathrm{k\in S}}\mathrm{x_{k1}}-\min_{\mathrm{k\in S}}\mathrm{x_{k1}}\right)...\left(\max_{\mathrm{k\in S}}\mathrm{x_{kQ}}-\min_{\mathrm{k\in S}}\mathrm{x_{kQ}}\right)\right]
}
Now the attraction effect can be defined as $d_{ij,preference}^S$ if item $i$ dominates item $j$ otherwise $-d_{ij,preference}^S$. 
The similarity effect is given by $\mathrm{SIM_i^S=\min_{j\in S/\{i\}}d_{ij,position}^S}$, where $V_{position}^S$ is a vector independent of $v_{preference}^S$. 
The authors further include the interaction effect between attraction effect and the similarity effect.
Empirical results show that under most cases the new model has a higher out-of-sample hit rate than the simple RUM model.\\
\cite{yousefi2020choice} proposes the CMNL (contextual MNL) model incorporating three context effects and its assortment optimization. 
The choice model assumes the deterministic utility $f_i^S$ for item $i$ within assortment $S$ can be divided into two parts:
\eq{
    f_i^S=\mu_i+\sum_{j\in S}\alpha_{ji},i\in S,
}
where $\mu_i$ is the \tit{baseline utility}, and $\alpha_{ji}$ is used to capture the three context effect of product $j$ on $i$. 
The utility of no-purchase is normalized to $f_0^s=0$. 
Thus, the probability of selecting $j$ among $S$ is given by:
\eq{
    P_j(\mathcal{S})=
    \begin{cases}
        \frac{\exp\left(\mu_j+\sum_{i\in S}\alpha_{ij}\right)}{1+\sum_{l\in S}\exp\left(\mu_l+\sum_{i\in S}\alpha_{il}\right)}&\quad\mathrm{~if~}j\in\mathcal{S},\\
        0&\quad\mathrm{~if~}j\notin\mathcal{S},&
    \end{cases}
}
Denote $x$ as the vector indicating whether $i$ is in $S$, the probability can be written in:
\eq{
    P_j(x)=\frac{x_j\exp\left(\mu_j+\sum_{i=1}^Nx_i\alpha_{ij}\right)}{1+\sum_{l=1}^Nx_l\exp\left(\mu_l+\sum_{i=1}^Nx_i\alpha_{il}\right)}.
}
Matrix $A\in\RR^{n\times n}$ is the matrix for $\alpha_{ji}$, now the assortment optimization can be formulated as:
\eq{
    \max_{x\in\{0,1\}^N}r(x,\mu,A)=\max_{x\in\{0,1\}^N}\sum_{i=1}^Nx_ir_iP_i(x),
}
where $r_i$ is the revenue for product $i$. 
When $r_i=1,\forall i$, this problem becomes a CTROP (Click Through Rate Optimization Problem), which is widely used in online recommendations:
\eq{
    \max_{x\in\{0,1\}^N}M(x,\mu,A)=\max_{x\in\{0,1\}^N}\sum_{i\in\mathcal{N}}x_iP_i(x).
}
The CMNL behaves well in empirical validation (using AIC and BIC scores), but both the AOP and CTROP are NP-hard. \\
In some scenarios, the assortment may have a `star' product, consumers would over-evaluate its utility, this is called the \rt{focal effect}. 
\cite{kovach2022focal} design a new choice model (focal luce model: FLM) that considers this phenomenon:
\eq{
    v(i,\mathcal{S})=v_i+\log\left(1+\delta(\mathcal{S})\cdot\mathbf{1}\{i\in F(\mathcal{S})\}\right)+\epsilon_i,
}
where $F(\cdot)$ is the \tit{focal function} indicating the star products given the set $\cS$, and $\delta(\cdot)$ is the \tit{distortion function} indicating the over-evaluation for the star products. 
\re{
    \tab{
        \item FLM is not a RUM (random utility model);
        \item FLM does not describe the focal function and the distortion function.
    }
}
\cite{jiang2023assortment} consider the assortment optimization under the FLM:
\eq{
    \max_{\mathcal{S}\subseteq\mathcal{N}}f(\mathcal{S}):=\frac{\sum_{i\in\mathcal{S}}r_iu_i(\mathcal{S})}{\sum_{i\in\mathcal{S}_+}u_i(\mathcal{S})}
}
The first scenario they consider is the ranking-based focal effect, let $\sigma(i,\cS)$ be the rank of item $i$'s utility in $\cS$, the top-k focal function:
\[
    F(\mathcal{S})=\{i:\sigma(i,\mathcal{S})\leq\min\{K,|\mathcal{S}|\},i\in\mathcal{S}\},
\]
and the distortion function is set to be a constant: $\delta(\cS)=\delta$.\\
\cite{cao2022network} consider the assortment optimization in the airline industry using the spiked-MNL choice model. 
`Spiked' means that each product $j$ has two utility $w_j,v_j$ with $w_j\ge v_j\ge0$, if $j$ is the cheapest among the assortment, the utility is $w_j$, otherwise $v_j$. 

\clearpage
\section{Data-Driven Operations Management}

\cite{Deng2023} provide a data-driven framework for firms to allocate budget portfolios on online cross-channel advertisement.
They obtain data from the selling and advertising records of a globally-known CPG company on Tmall. There are 4 channels:
\tab{
    \item Banner;
    \item Recommender;
    \item SEO-product;
    \item SEO-store.
}
And there are 3 advertisement effects:
\tab{
    \item promotional effect;
    \item cross-period spillover effect;
    \item cross-product spillover effect.
}
through a reduced-form regression using the pricing records of a non-competing but analogous market as IV:
\eq{
    \begin{gathered}
        r_i^t \begin{aligned}=\alpha_{i,0}+\sum_{j\in\mathcal{J}}\alpha_{i,j}s_{i,j}^{t}+\sum_{j\in\mathcal{J}}\hat{\alpha}_{i,j}(s_{i,j}^{t})^{2}+\sum_{j\in\mathcal{J}}\left(\alpha_{i,j}^{pre}\delta_{pre}^{t}+\alpha_{i,j}^{pro}\delta_{pro}^{t}+\alpha_{i,j}^{pos}\delta_{pos}^{t}\right)s_{i,j}^{t}+\end{aligned} \\
        \sum_{j\in\mathcal{J}}\left(\beta_{i,j}^{\ell1}s_{i,j}^{t-1}+\beta_{i,j}^{\ell2}s_{i,j}^{t-2}+\beta_{i,j}^{\ell3}s_{i,j}^{t-3}\right)+\sum_{k\in\mathcal{I}\setminus\{i\}}\sum_{j\in\mathcal{J}}\gamma_{k,j}s_{k,j}^{t}+\boldsymbol{\Theta}_{i}\cdot\boldsymbol{\Omega}_{i}^{t}+\epsilon_{i},\quad i\in\mathcal{I}.
    \end{gathered}
}
where $r_i^t$ is the revenue for product $i$ at time $t$, $s_{i,j}^t$ is the spending of budges on product $i$ through channel $j$, $s_{k,j}^t$ measures the cross-channel spillover effect. The regression
results are significant. What's more, the authors utilize the parameters of the regressions and optimize them in a sequential game:
\fig{deng2023}{The Optimization Problem}{0.5}
The simulation results give positive feedback.

\clearpage
\section{Analytical Modeling (Game Theory Style)}
\cite{dai2020conspicuous} study a problem of the doctor's pathway behavior considering the reputational payoff.
\ass{\cite{dai2020conspicuous}}{
    \tab{
        \item A client has a state $\theta$, $\theta=1$ means positive, there is a prior on $\theta$ (Bernoulli with parameter $\alpha$, and $\alpha$ is common knowledge);
        \item The expert has a private signal $S_e\in\{0,1\}$ (two types of experts: $e\in\{l,h\}$), the precision is $\rho_e$: $\Pr(s_e=0|\theta=0)=\Pr(s_e=1|\theta=1)=\rho_e$. Assume that $rho_h>\rho_l>0.5$;
        \item After the observing $\alpha$ and the private signal $S_e$, there are two decisions: $t\in\{0,1\}$ denotes the testing decision, $a\in\{0,1\}$ denotes the diagnosis decision;
        \item The client's utility depends on the state and expert's diagnosis decision: TP: $B$; FP: $-D$; TN: $0$l FN: $-d$.
        \item Test $t=1$ would incur a cost $c$ for the client ($c<d$);
        \item Using the Bayesian rule, we can calculate the posterior belief of the expert: 
    }
    \eq{
        b_e(\alpha|\mathrm{s}_e)\triangleq
        \begin{cases}
            \dfrac{\alpha\rho_e}{\alpha\rho_e+(1-\alpha)(1-\rho_e)}&\text{ if }s_e=1,\\
            \dfrac{\alpha(1-\rho_e)}{\alpha(1-\rho_e)+(1-\alpha)\rho_e}&\text{ if }s_e=0.
        \end{cases}
    }
    \tab{
        \item We can use the posterior belief to calculate the expected utility for the clients given $S_e$ and $t$, a purely altruistic expert would decide whether to conduct the test $t$ based on the expected utility of the client;
        \item The expert's type is personal information, but peers can update their beliefs $\beta(t,a)$ about the expert's type, the reputational utility for the expert is $r\cdot \beta(t,a)$;
        \item The expert's expected payoff is given by:
    }
    \eq{
        u_e=\phi U+(1-\phi)r\beta(t,a)
    }
}
When the expert's type is common knowledge, it's intuitive to come up with $\bar{\alpha}_S^e$ and $\underline{\alpha}_s^e$ that the expert provides:
\lis{
    \item positive diagnosis without performing the test if $\alpha>\bar{\alpha}_S^e$;
    \item perform the test if $\bar{\alpha}_S^e\ge\alpha\ge\underbar{\alpha}_S^e$;
    \item negative diagnosis without performing the test if $\alpha<\underbar{\alpha}_S^e$.
}
In the asymmetric-information case, assuming the peers' prior belief of $e=h$ is $\gamma$, the authors prove that under certain circumstances ($\underbar{B}\le B\le\bar{B}$), the system has a single separating NE: the type-l expert conduct the test and the type-h expert doesn't. 
This model assumes that given the expert's type, the precision $\rho_e$ is fixed.  
\cite{adida2023impact} uses a similar framework to analyze whether the payment scheme would affect the expert's pathway behavior.

\clearpage
\chapter{Miscellaneous} \label{chap:miscellaneous}

\section{Notes on Tools} \label{sec:notestools}

\subsection{LaTeX Shortcuts}

There are 6x6 colors in the preset preamble:\\
\begin{center}
    \begin{tabular}{|*{6}{>{\centering\arraybackslash}m{2cm}|}}
        \hline
        \cellcolor{aa}\textcolor{black}{aa} &
        \cellcolor{ab}\textcolor{black}{ab} &
        \cellcolor{ac}\textcolor{black}{ac} &
        \cellcolor{ad}\textcolor{black}{ad} &
        \cellcolor{ae}\textcolor{black}{ae} &
        \cellcolor{af}\textcolor{black}{af}
        \\
        \hline
        \cellcolor{ba}\textcolor{black}{ba} &
        \cellcolor{bb}\textcolor{black}{bb} &
        \cellcolor{bc}\textcolor{black}{bc} &
        \cellcolor{bd}\textcolor{black}{bd} &
        \cellcolor{be}\textcolor{black}{be} &
        \cellcolor{bf}\textcolor{black}{bf}
        \\
        \hline
        \cellcolor{ca}\textcolor{black}{ca} &
        \cellcolor{cb}\textcolor{black}{cb} &
        \cellcolor{cc}\textcolor{black}{cc} &
        \cellcolor{cd}\textcolor{black}{cd} &
        \cellcolor{ce}\textcolor{black}{ce} &
        \cellcolor{cf}\textcolor{black}{cf}
        \\
        \hline
        \cellcolor{da}\textcolor{black}{da} &
        \cellcolor{db}\textcolor{black}{db} &
        \cellcolor{dc}\textcolor{black}{dc} &
        \cellcolor{dd}\textcolor{black}{dd} &
        \cellcolor{de}\textcolor{black}{de} &
        \cellcolor{df}\textcolor{black}{df}
        \\
        \hline
        \cellcolor{ea}\textcolor{black}{ea} &
        \cellcolor{eb}\textcolor{black}{eb} &
        \cellcolor{ec}\textcolor{black}{ec} &
        \cellcolor{ed}\textcolor{black}{ed} &
        \cellcolor{ee}\textcolor{black}{ee} &
        \cellcolor{ef}\textcolor{black}{ef}
        \\
        \hline
        \cellcolor{fa}\textcolor{black}{fa} &
        \cellcolor{fb}\textcolor{black}{fb} &
        \cellcolor{fc}\textcolor{black}{fc} &
        \cellcolor{fd}\textcolor{black}{fd} &
        \cellcolor{fe}\textcolor{black}{fe} &
        \cellcolor{ff}\textcolor{black}{ff}
    \end{tabular}
\end{center}

\no Using \verb=\href{URL}{text}= to refer a \href{EurekaTangChen.github.io}{website}.\\
\no Using \verb=\eq= to write equation, \verb=\tab= to get an unordered list,
\verb=\lis= to get an ordered list.\\
\eq{E=mc^2} \label{eq:Example}
\tab{
    \item item 1;
    \item item 2;
    \item item 3.
}
\lis{
    \item item 1;
    \item item 2;
    \item item 3.
}

\begin{tcblisting}{colback=red!5!white,colframe=red!75!black,listing side text,
        title=Format of Words,fonttitle=\bfseries}
    \hl{highlighted}, \ul{underlined}, \st{strikethrough}\\
    \rt{red}, \yt{yellow}, \bt{blue}, \gt{green}
\end{tcblisting}
\begin{tcblisting}{colback=red!5!white,colframe=red!75!black,listing side text,
        title=Shortcuts,fonttitle=\bfseries}
    \RR, \NN, \ZZ, \QQ\\
    \bA, \bB, \bC, \bD
\end{tcblisting}
\begin{tcblisting}{colback=red!5!white,colframe=red!75!black,listing side text,
        title=Shortcuts,fonttitle=\bfseries}
    \tbf{Text}, \tit{Text}\\
    \cA, \cB, \cC, \cD
\end{tcblisting}
\begin{tcblisting}{colback=red!5!white,colframe=red!75!black,listing side text,
        title=Emoji,fonttitle=\bfseries}
    \emogood, \emobad, \emocool, \emoheart, \emotree
\end{tcblisting}

\no Use \verb=\ass, \ax, \thm, \co, \pro, \defi, \re, \key, \ex, \proo= to
\no use preset tcolorboxes template.
\ass{Example}{\lis{
        \item item 1;
        \item item 2;
        \item item 3.
    }}
\ax{Exmaple}{Test} \label{ax:Example}
\thm{Exmaple}{Test}
\co{Exmaple}{Test}
\pro{Exmaple}{Test} \label{pro:Example}
\defi{Exmaple}{Test}
\re{Expamle}
\key{Example}
\ex{Problem example}{}
\proo{proposition \ref{pro:Example}}{The Formal Proof}

\no Using \verb=\label, \ref= to refer to chapters \ref{chap:miscellaneous},
sections \ref{sec:notestools}, equations \ref{eq:Example}, and boxes \ref{ax:Example}.\\
\no Using \verb=\cite= to cite the literature in APA style. For example:
\cite{Angrist}\\
\no Using \verb=\sep= to insert a horizontal line with words in the middle:\\
\sep{Compilation}
Put photos in the \textit{pic} file and use \verb|\fig| to show it.
\fig{temp}{Example.png}{0.6}
Using \verb=\alg= to write the pseudo code:
\alg{Example Code}{
    \Require $n \geq 0$
    \Ensure $y = x^n$
    \State $y \gets 1$
    \State $X \gets x$
    \State $N \gets n$
    \While{$N \neq 0$}
    \If{$N$ is even}
    \State $X \gets X \times X$
    \State $N \gets \frac{N}{2}$  \Comment{This is a comment}
    \ElsIf{$N$ is odd}
    \State $y \gets y \times X$
    \State $N \gets N - 1$
    \EndIf
    \EndWhile
}

\no Using \verb=\begin{python}=, \verb=\begin{stata}=, \verb=\begin{shell}= to write codes:
\begin{python}
    print("Hello!")
\end{python}

\begin{stata}
    ss install reghdfe
\end{stata}

\begin{shell}
    git push origin main
\end{shell}

\newpage
\listoffigures

\newpage
\listofalgorithms

\bibliographystyle{apalike}
\bibliography{reference}
\end{document}