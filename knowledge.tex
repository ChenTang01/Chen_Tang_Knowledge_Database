\documentclass[12pt]{report}
\input{chentang}

\title{\textbf{Chen Tang's \break Knowledge Database}}
\author{\textbf{ChenTang@link.cuhk.edu.cn}}
\date{Last updated on \today}

\begin{document}

\maketitle

\chapter*{Preface}
\addcontentsline{toc}{chapter}{Preface}
\begin{center}
    {\Large The following is a compendium of my academic notes
        spanning various domains.
        I present these notes
        publicly to share my methodological framework for
        managing and structuring an individual's knowledge
        networks.\\
        The inevitability of encountering
        occasional errors is acknowledged.\\
        \bigskip
        \textbf{This notebook will undergo continuous updates.}}
\end{center}

\tableofcontents

\chapter{Mathematics, Statistics \& Optimization}

\section{Calculus and Linear Algebra}

\subsection{Keys of Calculus}

\subsection{Keys of Linear Algebra}

The main contents of this subsection are notes from \cite{deisenroth2020mathematics}.
The properties of Matrix Multiplication:
\tab{
    \item Associativity: $(AB)C=A(BC)$;
    \item Distributivity: $A(C+D)=AC+AD$;
    \item Identity Multiplication: $I_mA=A,AI_n=A$
}
Only square matrixes ha an inverse matrix, and the inverse is unique. If a matrix has an inverse, then it's called \tbf{regular/invertible/nonsingular}.
The properties of inverses and transposes:
\tab{
\item $(AB)^{-1}=B^{-1}A^{-1}$;
\item $(A+B)^{-1}\neq A^{-1}+B^{-1}$;
\item $(AB)^\top=B^\top A^\top $;
\item $(A+B)^\top=A^\top+B^\top $.
}
To find the inverse matrix, gaussian elimination can be applied: $[A\mid I]=[I\mid A^{-1}]$.\\
\sep{Solutions of Linear Systems}
\tbf{Reduced Row-Echelon Form Matrix}:
\eq{
    \boldsymbol{A}=\begin{bmatrix}1&3&0&0&3\\0&0&1&0&9\\0&0&0&1&-4\end{bmatrix}
}
There are \tit{pivot (basic variables)} and \tit{free variable}, and the column of free variable is dependent on pivots. The steps to find solutions of linear systems:
\lis{
    \item Find a particular solution for $Ax=\boldsymbol{b}$ by setting all free variable zero;
    \item Find all solutions for $Ax=0$;
    \item Add them up.
}
There is an iterative method to solve large-scale linear equations:
define error = $\|x^{(k+1)}-x_*\|$, then optimize the function $x^{(k+1)}=Cx^{(k)}+d$ and iterate it.\\
\sep{Vector Spaces, Basis and Rank}
\defi{Vector Space and Subspace}{
    A \tbf{Vector Space} $V=(\mathcal{V},+,\cdot)$ is a set $\cV$ with two operations:
    \lis{
        \item $+:\mathcal{V}\times\mathcal{V}\to\mathcal{V}$;
        \item $\cdot:\mathbb{R}\times\mathcal{V}\to\mathcal{V}$
    }
    For $\mathcal{U}\subseteq\mathcal{V}$ and $\mathcal{U}\neq\emptyset$, then $U=(\cU,+,\cdot)$ is a vector subspace.\\
    \re{
        \tab{
            \item $V=(\mathcal{V},+)$ is an \tit{Abelian group};
            \item The subspace needs to satisfy \tit{closure}, $\lambda x \in U, x+y\in U$.
        }
    }
}
If $v=\sum_{i=1}^{k}\lambda_ix_i$, then $v$ is a linear combination of $(x_1,x_2,\cdots,x_k)$. If \tbf{not} all values of a solution are
0, then it's called a non-trivial solution. For $\sum_{i=1}^{k}\lambda_ix_i=0$, if the non-trivial solution exists, then the vectors are called \tbf{linearly dependent}.
\defi{Basis and Rank}{
    \tab{
        \item \tbf{Generating Set}: if all vectors in $V$ can be expressed as a linear combination of $\mathcal{A}=\{\boldsymbol{x}_{1},\ldots,\boldsymbol{x}_{k}\}\subseteq\mathcal{V}$,
        then $\cA$ is a generating set of $V$;
        \item \tbf{Span}: The set of linear combinations of $\cA$ is its span;
        \item \tbf{Basis}: The minimal generating set (linearly independent) of a vector space $V$ is called its basis;
        \item \tbf{Rank}: the number of linearly independent column vectors in a matrix $\cA \in \RR&{m\times n}$.
    }
    \re{
        \tab{
            \item $\operatorname{rk}(A)=\operatorname{rk}(A^{\top})$;
            \item A matrix $A\in\mathbb{R}^{n\times n}$ is invertible if and only if $rk(A)=n$;
            \item The span of a matrix is also called its \tbf{image}, $dim(U)=rk(A)$;
            \item $Ax=b$ only has solution if and only if $rk(A)=rk(A\mid b)$;
            \item The solution (kernel, null space) to $Ax=0$ has a dimension of $n-rk(A)$;
        }
    }
}
\defi{Linear Mappings}{
    For vector spaces $V,W$, a mapping $\Phi:V\rightarrow W$ is called linear mapping if:
    \eq{
        \forall x,y\in V\forall\lambda,\psi\in\mathbb{R}:\Phi(\lambda x+\psi\boldsymbol{y})=\lambda\Phi(x)+\psi\Phi(\boldsymbol{y})
    }
}
\sep{Determinant and Trace}
\tbf{Determinant} is used to decide whether a matrix is invertible, denoted by $del(A)$ or $|A|$, its geometric meaning is
the signed volume. \tit{Laplace Expansian} can be used to compute the determinant for large matrixes.\\
If $det(A)=0$, then it's non-invertible. otherwise, it's invertible.\\
\tbf{Trace} is defined as $Tr(A)=\sum_{i=1}^n a_{ii}$. Both determinant and trace can only be applied to the squared matrix.
\sep{Eigenvalue and Postive-Definiteness}
Consider matrix $A\in \RR^{n\times n}$ as a linear mapping rather than a simple matrix, $A$ can map a n-dimensional space
to another n-dimensional space.
\fig{Mona-Lisa}{
    A shear mapping example
}{0.3}
The Mona Lisa example is a linear mapping of two-dimensional space, but during the mapping, there are some vectors (such as the blue vector)
that are only modified in length rather than direction. This is the idea of the eigenvector.
\defi{Eigenvalue and Eigenvector}{
    For a square matrix $A$, if there exists a scalar $\lambda$ and a vector $v\in \RR^n$, such that $A\cdot v=\lambda\cdot v$.
    Which would lead us to $(A-\lambda I)\cdot v=\tbf{0}$.
    \re{
        The following statements are equivalent:
        \tab{
            \item $\lambda$ is an eigenvalue of $A\in \RR^{n\times n}$;
            \item $rk(A-\lambda I)<n$;
            \item $det(A-\lambda I)=0$.
        }
    }
}
The definiteness is similar to the idea of eigenvalue. Consider $X^T MX$ as the inner product of $X$ and $MX$, where
$MX$ can be thought as a transformed version of $X$. If $X^T MX>0$ for all $X$,
then we can get $\cos \theta=\frac{X^T \cdot MX}{||X||\cdot ||MX||}>0$, which means for all vectors, the linear mapping $M$ can map
the vector to an angle less than 90 degrees.\\
So to show whether a matrix is positive definite, calculate all the eigenvalues of the matrix. If all values are larger than 0, then it's positive definite/
\re{
    \tab{
        \item $Tr(A)=\sum_i^n \lambda_i$;
        \item $\det(A)=\prod_{i=1}^n\lambda_i$.
    }
}

\subsection{Norms}

\defi{Norm}{
The norm $||\codt||$ is a mapping $V\to \RR$, which assigns each vector a length that satisfy:
\tab{
    \item \tit{Absolutely homogeneous}: $\|\lambda x\|=|\lambda|\|x\|$;
    \item \tit{Triangle inequality}: $\lVert x+y\rVert\leqslant\lVert x\rVert+\lVert y\rVert$;
    \item \tit{Positivity}: $||x||\ge0$ and $\|\boldsymbol{x}\|=0\iff\boldsymbol{x}=\boldsymbol{0}$.
}
The $L_p$ norm: $\|x\|_p=\left(|x_1|^p+|x_2|^p+\cdots+|x_n|^p\right)^{1/p}$.\\
\co{$L_p$ norm}{
    \lis{
        \item \tbf{Manhattan Norm}: $\|\boldsymbol{x}\|_1:=\sum_{i=1}^n|x_i|$;
        \item \tbf{Euclidean Norm}: $\|\boldsymbol{x}\|_2:=\sqrt{\sum_{i=1}^nx_i^2}=\sqrt{\boldsymbol{x}^\top\boldsymbol{x}}$ (mostly denoted as $||\cdot||$);
        \item \tbf{Infinity Norm}: $\|x\|_\infty=\max\left\{|x_1|,|x_2|,\ldots,|x_n|\right\}$.
    }
}
\proo{Infinity Norm}{
By squeeze theorem:\\
First show that $\|x\|_p=(\sum_i|x_i|^p)^{\frac1p}\leq(\sum_i\max_i|x_i|^p)^{\frac1p}=n^{\frac1p}\max_i|x_i|\to\max_i|x_i|=\|x\|_\infty $,\\
Next show that $\|x\|_p=(\sum_i|x_i|^p)^{\frac1p}\geq(\max_i|x_i|^p)^{\frac1p}=\max_i|x_i|=\|x\|_\infty $.
}

}
\sep{Taking derivatives of Norm}
\ex{$X\in \RR^m, f(x):\RR^m\to\RR^n, g(x)=||x||_2$, what is $\nabla g(X)$?}{
    First, $g(X)=\|f(X)\|_2=\sqrt{\sum_{i=1}^nf_i(X)^2}$;
    \eq{
        \nabla g(X)=\frac12\left(\sum_{i=1}^nf_i(X)^2\right)^{-\frac12}\left(\sum_{i=1}^n2f_i(X)\nabla f_i(X)\right)=\frac{J_f(X)^Tf(X)}{\|f(X)\|_2}
    }
}


\subsubsection{Matrix Norms}
\defi{Induced Matrix Normss}{
Let $A\in\RR^{W\times V}$ be a mapping from vector space $\cV$ to $\cW$.
\eq{
\|A\|_{\cV,\cW}:=\max_{x\neq0}\frac{\|Ax\|_{\cW}}{\|x\|_{\cV}}.
}
This measures how much the matrix $A$ can stretch an $n$ dimensional vector at most. Because norms are homogeneous to scaling, we also have:
\eq{
\|A\|_{\mathcal{V},\mathcal{W}}=\sup_{\|v\|_{\mathcal{V}=1}}\|Av\|_{\mathcal{W}}.
}
Besides the 3 satisfactions listed above, there are two more properties that a matrix norm needs to satisfy:
\tab{
    \item \tit{Sub-multiplicative:} $\|AB\|\leq\|A\|\cdot\|B\|\text{ for all }A,B$;
    \item \tit{Compatible with vector norm}: $\|Ax\|\leq\|A\|\cdot\|x\|\text{ for all }A \text{ and vector }x$.
}
\proo{Sub-multiplicative}{
    \eq{
        \|AB\|=\max_{\|v\|=1}\|ABv\|\le\max_{\|v\|=1}\|A\|\|Bv\|=\|A\|\|B\|.
    }
}
For a matrix $A\in\RR^{m\times n}$:
\eq{
\|A\|_1:=\max_{j\in\{1,\ldots,n\}}\sum_{i=1}^m|a_{ij}|, \|A\|_\infty:=\max_{i\in\{1,\ldots,m\}}\sum_{j=1}^n|a_{ij}|.
}
The $\|\cdot\|_1$ is the \bt{maximum absolute column sum}, the $\|\cdot\|_{\infty}$ is the \bt{maximum absolute row sum}.\\
The \tbf{spectral norm}:
\eq{
    \|A\|:=\|A\|_2:=\sqrt{\lambda_{\max}(A^\top A)}.
}
The \tbf{Frobenius Norm}:
\eq{
    \|A\|_F=\left(\sum_{i=1}^m\sum_{j=1}^n|a_{ij}|^2\right)^{1/2}=\sqrt{\operatorname{tr}(A^\top A)}.
}
}

\clearpage
\section{Probability Theory}

The main contents of this section are from \cite{Ross2014}.

\subsection{Probability Distribution}
\sep{Common Discrete Distribution}
\tbf{Bernoulli}(p)\\
\tit{pmf}: $P(X=x\mid p)=p^{x}(1-p)^{1-x};\quad x=0,1;\quad0\leq p\leq1$\\
\tit{mean}: $EX=p$; \tit{variance}: $VarX=p(1-p)$\\
\tab{
    \item A Bernoulli trial (named after James Bernoulli) is an experiment with only
    two possible outcomes;
    \item Bernoulli random variable $X = 1$ if “success” occurs and $X = 0$ if “failure”
    occurs where the probability of a “success” is $p$.
}
\tbf{Binomial}(n,p)\\
\tit{pmf}: $P(X=x\mid n,p)=\binom{n}{x}p^{x}(1-p)^{n-x},\quad x=0,1,\ldots,n;\quad0\leq p\leq1$\\
\tit{mean}: $EX=np$; \tit{variance}: $np(1-p)$\\
\tab{
    \item A Binomial experiment consists of $n$ independent identical Bernoulli trials;
    \item $X=\sum_{i=1}^nY_i$, where $Y_1,\cdots,Y_n$ are $n$ identical, independent Bernoulli random variables.
}
\tbf{Poisson}($\lambda$)\\
\tit{pmf}: $P(X=x\mid\lambda)=\frac{e^{-\lambda}\lambda^x}{x!};\quad x=0,1,\ldots;\quad0\leq\lambda<\infty $\\
\tit{mean}: $EX=\lambda$; \tit{variance}: $Var X=\lambda$\\
\tab{
    \item A Poisson distribution is typically used to model the probability distribution
    of the number of occurrences (with $\lambda$ being the intensity rate) per unit
    time or per unit area;
    \item Binomial pmf approximates Poisson pmf.
    Poisson pmf is also a limiting distribution of a negative binomial distribution;
    \item A useful result: By Taylor series expansion: $e^{\lambda}=\sum_{x=0}^{\infty}\frac{\lambda^x}{x!}$.
}
\ass{Poisson Process}{
    \lis{
        \item Let $X(\Delta)$ be the number of events that occur during an interval $\Delta$;
        \item The events are independent: if $\Delta_1,\cdots,\Delta_n$ are disjoint intervals,
        then $X(\Delta_1),\cdots,X(\Delta_n)$ are independent;
        \item $X(\Delta)$ only depends on the length of $\Delta$;
        \item The probability that exactly one event occurs in a small interval of length $\Delta t$ equals $\lambda \Delta t+\tit{o}(\Delta t)$;
        \item Poisson distribution is \hl{not} memoryless, but its interval (exponential distribution) is memoryless.
    }
}
\tbf{Geometric}($p$)\\
\tit{pmf}: $P(X=x\mid p)=p(1-p)^{x-1};\quad x=1,2,\ldots;\quad0\leq p\leq1$\\
\tit{mean}: $\frac{1}{p}$; \tit{variance}: $\frac{1-p}{p^2}$\\
\tab{
    \item The experiment consists of a sequence of independent trials;
    \item \emotree The property of memoryless: $P(X>s\mid X.t)=P(X>s-t)$.
}
\tbf{Negative Binomial}($r,p$)\\
\tit{pmf}: $P(X=x\mid r,p)=\binom{r+x-1}{x}p^r(1-p)^x,\quad x=0,1,\ldots;\quad0\leq p\leq1$\\
\tit{mean}: $EX=\frac{r(1-p)}{p}$; \tit{varaince}: $\frac{r(1-p)}{p^2}$\\
\tab{
    \item assume there are many independent and identical experiments, to observe the $r$th success, $X$ is the number of games to see the failure;
}
\tbf{Hypergeometric}($N,M,K$)\\
\tit{pmf}: $\begin{aligned}P(X=x\mid N,M,K) & =\frac{\binom{M}{k}\binom{N-M}{K-x}}{\binom{N}{K}};
               \quad x=0,1,2,\ldots,K;                                                \\M-(N-K)&\le x\le M;\quad N,M,K\ge0\end{aligned}$\\
\tit{mean}: $EX=\frac{KM}{N}$; \tit{varaince}: $\frac{KM}{N}\frac{(N-M)(N-K)}{N(N-1)}$\\

\sep{Common Continuous Distribution}
\tbf{Uniform}($a,b$)\\
\tit{pdf}:$f(x\mid a,b)=\frac{1}{b-a}$; \tit{mean}:$EX=\frac{b+a}{2}$; \tit{variance}: $Var X=\frac{(b-a)^2}{12}$.\\
\tbf{Exponential}($\beta$)\\
\tit{pdf}: $f(x\mid \beta)=\frac{1}{\beta}e^{-x/\beta},0\le x< \infty, \beta>0$; \tit{mean}: $EX=\beta$; \tit{variance}: $VarX=\beta^2$.\\
\tbf{Gamma}($\alpha,\beta$)\\
\tit{pdf}:$f(x\mid\alpha,\beta)=\frac{1}{\Gamma(\alpha)\beta^\alpha}x^{\alpha-1}e^{-x/\beta},\quad0\leq x<\infty,\quad\alpha,\beta>0$;
\tit{mean}:$\alpha\beta$; \tit{variance}:$\alpha\beta^2$.\\
\tab{
\item The \tit{gamma function} is defined as $\Gamma(\alpha)=\int_0^\infty t^{\alpha-1}e^{-t}dt$;
\item $\Gamma(\alpha+1)=\alpha\Gamma(\alpha), \alpha>0$;
\item $\Gamma(n)=(n-1)!,\quad\text{ for any integer }n>0$.
}
\tbf{Normal}($\mu,\sigma^2$)\\
\tit{pdf}:$f\left(x\mid\mu,\sigma^2\right)=\frac{1}{\sqrt{2\pi}\sigma}e^{-(x-\mu)^2/\left(2\sigma^2\right)},\quad-\infty<x<\infty$;
\tit{mean}:$\mu$; \tit{variance}:$\sigma^2$.\\
\ex{for $f(x)=\frac{\beta\alpha^{\beta}}{x^{\beta+1}},\quad\alpha<x<\infty,\quad\alpha>0,\quad\beta>0$: \\
    \tab{
        \item (a) Verify $f(x)$ is a pdf;
        \item (b) Derive the mean and variance of this distribution;
        \item (c) Prove that the variance does not exist if $\beta\leq2$.
    }}{
    (a)
    \eq{\begin{aligned}\int_{\alpha}^{\infty}f(x)dx&=\int_{\alpha}^{\infty}\frac{\beta\cdot \alpha^{\beta}}{x^{\beta+1}}dx=-x^{\beta\cdot\alpha^{\beta}|\alpha}\\&=0+\alpha^{-\beta}\cdot\alpha^{\beta}=1\end{aligned}}
    (b)
    \eq{\begin{aligned}\\Ex=\int_{\alpha}^{\infty}xf(x)dx=\int_{\alpha}^{\infty}\frac{\beta\cdot \alpha^{\beta}}{X^{\beta}}dx\\=\frac{\beta}{-\beta+1}\cdot\alpha^{\beta}\cdot x^{-\beta+1}|_{\alpha}^{\infty}=\frac{\beta\cdot\alpha}{\beta-1}\end{aligned}}
    \eq{\begin{aligned}Ex^{2}&=\int_{\alpha}^{\infty}\frac{\beta\cdot\alpha^{\beta}}{x^{\beta-1}}dx=\frac{\beta}{-\beta+2}\cdot\alpha^{\beta}\cdot x^{-\beta+2}|_{\alpha}^{\infty}\\&=\frac{\alpha^{2}\beta}{\beta-2}\end{aligned}}
    \eq{\operatorname{Var}X=EX^2-(EX)^2=\frac{\beta\alpha^2}{(\beta-1)^2(\beta-2)}}
    (c)\\
    If $\beta<2$, then the variance is negative.
}
\fig{type of distribution}{Type of distribution}{}

\section{Statistical Inference}
This section is mainly the notes from \cite{casella2021statistical}
\fig{statistical inference}{The scope of statistical inference}{}
Statistics uses observed data to \hl{inference} the statistical model.

\subsection{Exponential Families}
\defi{Exponential Families
}{A family of pdfs or pmfs is called an \tit{exponential family} if:
    \eq{
        f(x|\boldsymbol{\theta})=h(x)c(\boldsymbol{\theta})\exp\left(\sum_{i=1}^kw_i(\boldsymbol{\theta})t_i(x)\right)
    }
    Where $h(x)\le0,c(\boldsymbol{\theta})\le0$, and $h(x),t_i(x)$ don't depend on $\boldsymbol{\theta}$. $c(\boldsymbol{\theta}),
        w_i(\boldsymbol{\theta})$ don't depend on $x$.
    \tab{
        \item Continuous: normal, gamma, beta, exponential;
        \item Discrete: binomial, poisson, nagative binomial;
        \item $\boldsymbol{\theta}=\theta_1,\theta_2, \codts,\theta_d$, $k$ must $\ge d$;
        \item If $k=d$, then it's a \tit{full exponential family}, if $k>d$, then it's a \tit{curved exponential family} (For example,
        most normal distributions are \tit{full exponential family}, but normal distribution satisfy $\mu=\sigma^2$ is a \tit{curved exponential family}).
    }
}
To verify a pdf is an exponential family, identify the function $h(x),c(\boldsymbol{\theta}),t_i(x),w_i(\boldsymbol{\theta})$, then
verify these functions satisfy the condition above.
\ex{
    Show that Binomial, Poisson, Exponential and Normal distribution belongs to the exponential families.
}{
    \tbf{Binomial}:\\
    \eq{
        \begin{gathered}
            f(x|p) =\binom nxp^x(1-p)^{n-x}=\binom nx(1-p)^n\left(\frac p{1-p}\right)^x \\
            =\begin{pmatrix}n\\x\end{pmatrix}(1-p)^n\exp\left(x\log\left(\frac{p}{1-p}\right)\right),
        \end{gathered}
    }
    Among which $\begin{pmatrix}n\\x\end{pmatrix}$ is $h(x)$, $(1-p)^n$ is $c(\theta)$, $x$ is $t_1(x)$, and $\log\left(\frac{p}{1-p}\right)$ is $w_i(\theta)$.
    Note that $f(x\mid p)$ is only in exponential families when $0<p<1$.\\
    \tbf{Poisson}:\\
    \eq{
        \begin{aligned}
             & f(x|\lambda)=\frac{\lambda^{x}e^{-\lambda}}{x!}=\frac{1}{x!}e^{-\lambda}\exp\left(x\log(\lambda)\right) \\
             & \text{then}                                                                                             \\
             & h(x)=\frac{1}{x!},c(\lambda)=e^{-\lambda},t(x)=x\mathrm{~and}w(\lambda)=\log(\lambda).
        \end{aligned}
    }
    \tbf{Exponential}:\\
    \eq{
        \begin{aligned}
             & f(x|\beta)=\frac{1}{\beta}\exp\left(-\frac{x}{\beta}\right)                   \\
             & \text{then}                                                                   \\
             & h(x)=1,c(\beta)=\frac{1}{\beta},t(x)=x\mathrm{~and}w(\beta)=-\frac{1}{\beta}.
        \end{aligned}
    }
    \tbf{Normal}:\\
    \eq{
        \begin{aligned}
             & f(x|\mu,\sigma^{2})=\frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{(x-\mu)^{2}}{2\sigma^{2}}\right)=\frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{x^{2}}{2\sigma^{2}}+\frac{x\mu}{\sigma^{2}}-\frac{\mu^{2}}{2\sigma^{2}}\right) \\
             & \text{then}                                                                                                                                                                                                                 \\
             & \begin{aligned}h(x)=1,c(\mu,\sigma)=\frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{\mu^{2}}{2\sigma^{2}}\right),\end{aligned}                           \\
             & t_{1}(x)=-\frac{x^{2}}{2},w_{1}(\mu,\sigma)=\frac{1}{\sigma^{2}},t_{2}(x)=x\mathrm{~and~}w_{2}(\mu,\sigma)=\frac{\mu}{\sigma^{2}}.
        \end{aligned}
    }
}
\ex{Show the following are exponential families:
\tab{
    \item Gamma family with either $\alpha,\beta$ is unknown or both unknown;
    \item Beta family with either $\alpha,\beta$ is unknown or both unknown;
    \item Negative Binomial family when $r$ is unkown.
}}{
\tbf{Gamma $\alpha$ unkown}:
\eq{f(x|\alpha;\beta)=e^{-x/\beta}\frac1{\Gamma(\alpha)\beta^\alpha}\exp((\alpha-1)\log x)}
thus $h(x)=e^{-x/\beta},x>0;c(\alpha)=\frac1{\Gamma(\alpha)\beta^\alpha};w_1(\alpha)=\alpha-1;t_1(x)=\log x.$\\
\tbf{Gamma $beta$ unknown}:
\eq{f(x|\beta;\alpha)=\frac1{\Gamma(\alpha)\beta^\alpha}x^{\alpha-1}e^{\frac{-x}\beta}}
thus $h(x)=\frac{x^{\alpha-1}}{\Gamma(\alpha)},x>0;c(\beta)=\frac1{\beta^\alpha};w_1(\beta)=\frac1\beta; t_1(x)=-x.$\\
\tbf{Gamma both unknown}:
\eq{f(x|\alpha,\beta)=\frac1{\Gamma(\alpha)\beta^\alpha}\exp((\alpha-1)\log x-\frac x\beta)}
thus $h(x)=I_{\{x>0\}}(x);c(\alpha,\beta)=\frac{1}{\Gamma(\alpha)\beta^\alpha};w_1(\alpha)=\alpha-1;t_1(x)=\log x;w_2(\alpha,\beta)=-1/\beta;t_2(x)=x.$\\
\tbf{Beta $\alpha$ unknown}:\\
$h(x)=(1-x)^{\beta-1}I_{[0,1]}(x),\quad c(\alpha)=\frac{1}{B(\alpha,\beta)},\quad w_1(x)=\alpha-1,\quad t_1(x)=\log x$\\
\tbf{Beta $\beta$ unkown}:\\
$h(x)=x^{\alpha-1}I_{[0,1]}(x),c(\beta)=\frac{1}{B(\alpha,\beta)},w_1(\beta)=\beta-1,t_1(x)=\log(1-x)$\\
\tbf{Beta both unknown}:\\
$h(x)=I_{[0,1]}(x),c(\alpha,\beta)=\frac{1}{B(\alpha,\beta)},w_1(\alpha)=\alpha-1,t_1(x)=\log x,w_2(\beta)=\beta-1,t_2(x)=\log(1-x).$\\
\tbf{Negative Binomial}:\\
\eq{h(x)=\left(\begin{matrix}r+x-1\\x\end{matrix}\right)I_{\NN}(x),\quad c(p)=\left(\frac{p}{1-p}\right)^r,\quad w_1(p)=\log(1-p),\quad t_1(x)=x.}
}
\thm{
    Expectation and Variance of Exponential Families
}{
    If X is a random variable that satisfies any distribution from the exponential families, then:
    \eq{
        \begin{aligned}
             & 1.\quad\operatorname{E}\left(\sum_{i=1}^{k}\frac{\partial w_{i}(\boldsymbol{\theta})}{\partial\theta_{j}}t_{i}(X)\right)=-\frac{\partial}{\partial\theta_{j}}\log\big(c(\boldsymbol{\theta})\big);                                                                                                                           \\
             & 2.\quad\mathrm{Var}\left(\sum_{i=1}^{k}\frac{\partial w_{i}(\boldsymbol{\theta})}{\partial\theta_{j}}t_{i}(X)\right)=-\frac{\partial^{2}}{\partial\theta_{j}^{2}}\log\left(c(\boldsymbol{\theta})\right)-\mathrm{E}\left(\sum_{i=1}^{k}\frac{\partial^{2}w_{i}(\boldsymbol{\theta})}{\partial\theta_{j}^{2}}t_{i}(X)\right).
        \end{aligned}
    }
}
\ex{Derive the mean and variance for binomial and normal distribution using the above theorem.}{
    \tbf{Binomial}:\\
    \eq{
        \begin{aligned}
             & h(x)=\begin{pmatrix}n\\x\end{pmatrix},c(p)=(1-p)^n,t(x)=x\mathrm{and}w(p)=\log\biggl(\frac{p}{1-p}\biggr).                                     \\
             & \mathrm{Then},                                                                                                                                 \\
             & \frac{\mathrm{d}}{\mathrm{d}p}w(p)=\frac{\mathrm{d}}{\mathrm{d}p}\log\left(\frac{p}{1-p}\right)=\frac{1}{p(1-p)},                              \\
             & \frac{\mathrm{d}^{2}}{\mathrm{d}p^{2}}w(p)=-\frac{1}{p^{2}}+\frac{1}{(1-p)^{2}}=\frac{2p-1}{p^{2}(1-p)^{2}},                                   \\
             & \frac{\mathrm{d}}{\mathrm{d}p}\log\big(c(p)\big)=\frac{\mathrm{d}}{\mathrm{d}p}n\log(1-p)=-\frac{n}{1-p},                                      \\
             & \frac{\mathrm{d}^{2}}{\mathrm{d}p^{2}}\log\left(c(p)\right)=-\frac{n}{(1-p)^{2}}.                                                              \\
             & \text{Therefore, from Theorem 3.4.2, we have}                                                                                                  \\
             & \operatorname{E}\left(\frac1{p(1-p)}X\right)=\frac n{1-p}\Rightarrow\operatorname{E}(X)=np,                                                    \\
             & \mathrm{Var}\left(\frac{1}{p(1-p)}X\right)=\frac{n}{(1-p)^2}-\mathrm{E}\left(\frac{2p-1}{p^2(1-p)^2}X\right)\Rightarrow\mathrm{Var}(X)=np(1-p)
        \end{aligned}
    }
    \tbf{Normal}:\\
    \eq{
        \begin{aligned}
             & \text{For Normal Distribution, we have}                                                                                                                                                                                                          \\
             & h(x)=1,c(\mu,\sigma)=\frac1{\sqrt{2\pi}\sigma}\exp\left(-\frac{\mu^2}{2\sigma^2}\right),                                                                                                                                                         \\
             & t_{1}(x)=-\frac{x^{2}}{2},w_{1}(\mu,\sigma)=\frac{1}{\sigma^{2}},t_{2}(x)=x\mathrm{and}w_{2}(\mu,\sigma)=\frac{\mu}{\sigma^{2}}.                                                                                                                 \\
             & \mathrm{Then},                                                                                                                                                                                                                                   \\
             & \begin{aligned}\frac{\partial w_1(\mu,\sigma)}{\partial\mu}=\frac{\partial(1/\sigma^2)}{\partial\mu}=0,\end{aligned}                                                                                                                             \\
             & \begin{aligned}\frac{\partial w_2(\mu,\sigma)}{\partial\mu}=\frac{\partial(\mu/\sigma^2)}{\partial\mu}=\frac{1}{\sigma^2},\end{aligned}                                                                                                          \\
             & \begin{aligned}\frac{\partial w_1(\mu,\sigma)}{\partial\sigma}=\frac{\partial(1/\sigma^2)}{\partial\sigma}=-\frac{2}{\sigma^3},\end{aligned}                                                                                                     \\
             & \frac{\partial w_2(\mu,\sigma)}{\partial\sigma}=\frac{\partial(\mu/\sigma^2)}{\partial\sigma}=-\frac{2\mu}{\sigma^3},                                                                                                                            \\
             & \frac{\partial}{\partial\mu}\log\left(c(\mu,\sigma)\right)=\frac{\partial}{\partial\mu}\left(-\frac{\log(2\pi)}{2}-\log(\sigma)-\frac{\mu^{2}}{2\sigma^{2}}\right)=-\frac{\mu}{\sigma^{2}},                                                      \\
             & \frac\partial{\partial\sigma}\log\left(c(\mu,\sigma)\right)=\frac\partial{\partial\sigma}\left(-\frac{\log(2\pi)}2-\log(\sigma)-\frac{\mu^2}{2\sigma^2}\right)=-\frac1\sigma+\frac{\mu^2}{\sigma^3}.                                             \\
             & \operatorname{E}\left(\frac{1}{\sigma^{2}}X\right)=\frac{\mu}{\sigma^{2}}\operatorname{and}\operatorname{E}\left(-\frac{2}{\sigma^{3}}\left(-\frac{X^{2}}{2}\right)-\frac{2\mu}{\sigma^{3}}X\right)=\frac{1}{\sigma}-\frac{\mu^{2}}{\sigma^{3}}, \\
             & \text{which implies}                                                                                                                                                                                                                             \\
             & \operatorname{E}(X)=\mu,\operatorname{E}(X^{2})=\mu^{2}+\sigma^{2}\mathrm{and}\mathrm{Var}(X)=\mathrm{E}(X^{2})-(\mathrm{E}X)^{2}=\sigma^{2}
        \end{aligned}
    }
}
\defi{
    The indicator function
}{
    I_A(x)=\begin{cases}1,&\text{if }x\in A\\0,&\text{if }x\notin A\end{cases}
}
\ex{
Show that $f(x\mid \theta)=\frac{1}{\theta}exp(1-\frac{X}{\theta})$ is \tbf{NOT} an exponential family.
}{\eq{
f(x|\theta)=\frac1\theta\exp\left(1-\frac x\theta\right)I_{[\theta,\infty)}(x)
}
At here $h(x)=I_{[\theta,\infty)}(x)$, which is not independent with $\theta$.}
\defi{
    Reparameterization of Exponential Families
}{
    \eq{
        f(x|\boldsymbol{\eta})=h(x)c^*(\boldsymbol{\eta})\exp\left(\sum_{i=1}^k\eta_it_i(x)\right)
    }
    Where $h(x)$ and $t_i(x)$ are identical with the original parameterization. $\boldsymbol{\eta}=(\eta_1,\eta_2,\cdots,\eta_n)$ and
    $\eta_i=w_i(\bold{\theta})$. And to make it a pdf (integrates to 1):
    \eq{
        c^*(\boldsymbol{\eta})=\left[\int_{-\infty}^\infty h(x)\exp\left(\sum_{i=1}^k\eta_it_i(x)\right)dx\right]^{-1}
    }
}

\subsection{Scale and Location}
step 1: define the standard pdf $f(Z)$, for example: $f(Z)=e^{-Z},Z\ge0$;\\
step 2: find the relationship between $X$ and $X$: $\sigma Z+\mu=X$, where $\sigma$ is the scale parameter and $\mu$ is the location parameter;\\
step 3: replace $Z$ using $X$: $f(x)=\frac{1}{\sigma}f(\frac{x-\mu}{\sigma})$ is a distribution transformed from the standard pdf.
\thm{Sacle-Location Family}{If f(x) is any pdf, for $\mu, \sigma>0$, then the function
    $g(x\mid \mu,\sigma)=\frac{1}{\sigma}f(\frac{x-\mu}{\sigma})$ is a valid pdf.
    \proo{}{
        \eq{
            \begin{aligned}g(x|\mu,\sigma)&=\frac{1}{\sigma}f\left(\frac{x-\mu}{\sigma}\right)\geq0\\\\\int_{-\infty}^{\infty}g(x|\mu,\sigma)dx&=\int_{-\infty}^{\infty}\frac{1}{\sigma}f\left(\frac{x-\mu}{\sigma}\right)dx\xrightarrow{\left(y=\frac{x-\mu}{\sigma}\right)}\int_{-\infty}^{\infty}f(y)dy=1.\end{aligned}
        }
    }
    \re{
        \tab{
            \item For discrete R.V.(pmf), the above theorem doesn't hold. (ignore the $\frac{1}{\sigma}$);
            \item The $\sigma$ is the scale parameter, the $\mu$ is the location parameter.
        }
    }}\label{thm:ScaleLocation}
\ex{\begin{align}
    \quad\int_{\mathbf{X}}(x_1\mid\theta) & = \frac1\theta\exp\{1-\frac x\theta\},\quad x \ge \theta                   \\
                                          & =\frac1\theta\exp\{-\frac{x-\theta}\theta\}\cdot\mathrm{I}_{[0,\infty)}(x)
\end{align} Is $\theta$ a scale parameter or a location parameter?}{
It depends on the standard pdf:\\
If the standard pdf is $f_{Z}(z)=\exp\{-z\}\cdot\mathcal{I}_{[0,+\infty)}(z)$:\\
then $\theta$ serves as both parameters because $f_X(x)=\frac1\theta\exp\{-\frac{x-\theta}\theta\}\cdot I_{[0,+\infty)}(\frac{x-\theta}\theta)$.\\
Else if the standard pdf is $f_{Z}(z)=\exp\{1-z\}\cdot\mathcal{I}_{[0,+\infty)}(z)$:\\
then $\theta$ is only a scale parameter because $f_X(x)=\frac1\theta\exp\{1-\frac{x}\theta\}\cdot I_{[0,+\infty)}(\frac{x-\theta}\theta)$.
}
\thm{For any pdf $f(\cdot)$, and $\sigma, \mu>0$. Then $X$ is a random variable with pdf $\frac{1}{\sigma}f\left(\frac{x-\mu}{\sigma}\right)$
    \tbf{if and only if} there is a random variable $Z$ with pdf $f(z)$ and $X=\sigma Z+\mu$.}{
    \proo{Rigorous Proof}{The ncessity of the proof can be found in \ref{thm:ScaleLocation}, here the sufficiency need }
    \proo{Intuitive Proof}{}
}

\subsection{Data Reduction}

The idea of data reduction is to summarize or reduce the data $X_1,X_2,\cdots,X_n$ to get the information of the unknown parameter $\theta$.\\
There are many sample point $\bold{x}=(x_1,x_2,\cdots,x_n)$, which are realizations (observations) of the random variable $\bold{X}=(X_1,X_2,\cdots,X_n)$.
A \tbf{Statistic} $T(\bold{X})$ is a form of data reduction, or a summary of the data, $T(\bold{x})$ is an observation of $T(\bold{X})$. $\cX$ is
the \tbf{sample space}. $\mathcal{T}=\{t:t=T(\mathbf{x}),\mathrm{~for~}\mathbf{x}\in\mathcal{X}\}$ is the image of $cX$ under $T(\bold{X})$.
$T(\bold{X})$ partition the sapmle sapce $\cX$ into sets $A_t=\{\mathbf{x}:T(\mathbf{x})=t,\mathbf{x}\in\mathcal{X}\}$.
\ex{
    Give a two-dimensional example for random variable, sample point, Statistic, observation of Statistic, sample space, image and $A_t$.
}{
    Assume $\bold{X}=X_1,X_2$, among which $X_1,X_2$ are Bernoulli R.V. with $p=0.5$. $\bold{x}=(0,1)$ is a sample point.\\
    The sample space $\cX$ is $(0,0), (0,1), (1,0), (1,1)$, set $T(\bold{X})$ as the statistic, then the image $\cT$ is $(0,1,2)$.\\
    $A_1(\bold{x})=((1,0),(0,1))$, $A_2(\bold{x})=(1,1)$, $A_0(\bold{x})=(0,0)$.
}

\subsection{Moment Generating Function}
The \tbf{Moment Genarating Function} (mgf) is:
\eq{
    M_X(t)=\mathbb{E}\left(e^{tX}\right).
}
If taking the $n$ order derivatives of the mgf by $t$, and let $t=0$, then:
\eq{
M_X^{(n)}(t)|_{t=0}=\mathbb{E}\left(X^n\right).
}

\subsection{Concentrtion Inequality}
\begin{center}
    \rt{\tit{Averages of independent random variables concentrate around their expectations.}}
\end{center}
\emogood Denote $\overline{X}=\frac{1}{n}\sum_{i=1}^{n}X_i$, where $X_1,\cdots,X_n$ are generated \tit{i.i.d.}, the general form of concentration inequality:
\eq{
    \mathbb{P}(|\overline{X}-\mathbb{E}[\overline{X}]|\geq\varepsilon(n))\leq1-\delta(n)
}
where $\varepsilon(n)$ and $\delta(n)$ converge to $0$ when $n\to\infty$.
\thm{Markov Inequality}{
    For a positive random variable $X$:
    \eq{
        \mathbb{P}(X\geq t)\leq\frac{\mathbb{E}[X]}{t}=O\left(\frac{1}{t}\right).
    }
    \proo{}{
        \eq{
            \mathbb{E}[X]=\int_0^\infty xp(x)dx\geq\int_t^\infty xp(x)dx\geq t\int_t^\infty p(x)dx=t\mathbb{P}(X\geq t).
        }
    }
}
\thm{Chebyshev's inequality}{
    Let $Var(X)=\sigma^2$, then:
    \eq{
        \mathbb{P}\Big(|X-\mathbb{E}[X]|\geq t\sigma\Big)\leq\frac{1}{t^2}=O\left(\frac{1}{t^2}\right).
    }
    \proo{}{
        \begin{aligned}
            \mathbb{P}(|X-\mathbb{E}[X]|\geq t\sigma) & =\mathbb{P}(|X-\mathbb{E}[X]|^{2}\geq t^{2}\sigma^{2})               \\
                                                      & \leq\frac{\mathbb{E}(|X-\mathbb{E}[X]|^2)}{t^2\sigma^2}=\frac1{t^2}.
        \end{aligned}
    }
    \co{}{
        If we consider $\widehat{\mu}_n=\frac{1}{n}\sum_{i=1}^nX_i$ with variance $\frac{\sigma^2}{n}$, applying the Chebyshev's inequality would get:
        \eq{
            \mathbb{P}\left(|\widehat{\mu}_n-\mu|\geq\frac{t\sigma}{\sqrt{n}}\right)\leq\frac{1}{t^2}.
        }
    }
}
\lem{Chernoff Method}{
\emocool This trick is extremely important.\\
Assume the mgf for $X$ is finite for all $|t|\le b,b>0$. Then we can use mgf to produce a tail bound by Markov inequality:
\eq{
\mathbb{P}((X-\mu)\geq u)=\mathbb{P}(\exp(t(X))\geq\exp(t(u+\mu)))\leq\frac{\mathbb{E}[\exp(tX)]}{\exp(t(u+\mu))}
}
Now consider $t$ as a parameter, to achieve the tightest bound, we can tune $t$ that satisfies:
\eq{
    \mathbb{P}((X-\mu)\geq u)\leq\inf_{0\leq t\leq b}\exp(-t(u+\mu))\mathbb{E}[\exp(tX)].
    }
    This is called the \tbf{Chernoff Bound}. The Chernoff bound "plays" nicely with summations, assume $X_i$ are independent:
    \eq{
        M_{X_1+\cdotp\cdotp\cdotp+X_n}(\lambda)=\prod_{i=1}^n M_{X_i}(\lambda),
    }
    this means that when we calculate a Chernoff bound of a sum of i.i.d. variables, we need  need only calculate the moment generating function for \tit{one} of them, assume $n$ zero-mean independent variables:
    \eq{
        \begin{aligned}
            \mathbb{P}\bigg(\sum_{i=1}^nX_i\geq t\bigg) & \leq\frac{\prod_{i=1}^n\mathbb{E}\left[\exp(\lambda X_i)\right]}{e^{\lambda t}} \\
                                                        & =(\mathbb{E}[e^{\lambda X_1}])^ne^{-\lambda t},
        \end{aligned}
    }
}
\thm{Gaussisn Tail Bound}{
    Suppose $X\sim N(\mu,\sigma^2)$, the mgf is $M_X(t)=\mathbb{E}[\exp(tX)]=\exp(t\mu+t^2\sigma^2/2)$, to apply the Chernoff bound we can compute:
    \eq{
        \inf\limits_{t\ge0}\exp(-t(u+\mu))\exp(t\mu+t^2\sigma^2/2)=\inf\limits_{t\ge0}\exp(-tu+t^2\sigma^2/2),
    }
    when $t=\frac{u}{\sigma^2}$, we can obtain:
    \eq{
        \mathbb{P}(X-\mu\geq u)\leq\exp(-u^2/(2\sigma^2)).
    }
    This is just the right side of the distribution, taking the absolute value:
    \eq{
        \mathbb{P}(|X-\mu|\geq u)\leq2\exp(-u^2/(2\sigma^2)).
    }
    Now consider $\overline{X}$, we can obtain:
    \eq{
        \mathbb{P}(|\widehat{\mu}-\mu|\geq t\sigma/\sqrt{n})\leq2\exp(-t^2/2).
    }
    \re{
        Comparing the Gaussian tail bound with the corollary of Chebyshev's inequality, we can find with probability $1-\delta$:
        \tab{
            \item Chebyshev tells that $|\widehat{\mu}-\mu|\leq\frac{\sigma}{\sqrt{n\delta}}$;
            \item Gaussian tells that $|\widehat{\mu}-\mu|\leq\sigma\sqrt{\frac{2\ln(2/\delta)}{n}}$, which decrease faster when $n\to\infty$.
        }
    }
}
\thm{Bounded RVs: Hoeffding's inequality}{
\lem{Hoeffding's Lemma}{
Let $X$ be a random variable bounded in $[a,b]$, let $X^\prime$ to be an independent copy of $X$, then:
\eq{
\mathbb{E}_X[\exp(\lambda(X-\mathbb{E}_X[X]))]=\mathbb{E}_X[\exp(\lambda(X-\mathbb{E}_{X^{\prime}}[X^{\prime}]))]\overset{(i)}{\operatorname*{\leq}}\mathbb{E}_X[\mathbb{E}_{X^{\prime}}\exp(\lambda(X-X^{\prime}))],
}
where $i$ follows Jensen's inequality. Let $S\in\{-1,1\}$ to be a random sign variable, then $S(X-X^\prime)$ has the same distribution as $X-X^\prime$:
\eq{
    \begin{aligned}
        \begin{aligned}\mathbb{E}_{X,X'}[\exp(\lambda(X-X'))]\end{aligned} & =\mathbb{E}_{X,X^\prime,S}[\exp(\lambda S(X-X^\prime))]                                         \\
                                                                           & =\mathbb{E}_{X,X^{\prime}}[\mathbb{E}_S[\exp(\lambda S(X-X^{\prime}))\mid X,X^{\prime}]].\right \\
                                                                           & \le \exp{\frac{\lambda^2(X-X^\prime)^2}{2}}                                                     \\
                                                                           & \le \exp{\frac{\lambda^2(b-a)^2}{2}}
    \end{aligned}
}
}
Now use the Chernoff bound and Hoeffding's lemma to show:
\eq{
    \begin{aligned}
        \begin{aligned}\mathbb{P}\left(\frac{1}{n}\sum_{i=1}^n(X_i-\mathbb{E}[X_i])\geq t\right)\end{aligned} & \begin{aligned}=\mathbb{P}\left(\sum_{i=1}^n(X_i-\mathbb{E}[X_i])\geq nt\right)\end{aligned}                                                                                      \\
                                                                                                              & \leq\mathbb{E}\left[\exp\left(\lambda\sum_{i=1}^n(X_i-\mathbb{E}[X_i])\right)\right]e^{-\lambda nt}                                                                               \\
                                                                                                              & =\left(\prod_{i=1}^n\mathbb{E}[e^{\lambda(X_i-\mathbb{E}[X_i])}]\right)e^{-\lambda nt}{\operatorname*{\leq}}\left(\prod_{i=1}^ne^{\frac{\lambda^2(b-a)^2}8}\right)e^{-\lambda nt}
    \end{aligned}
}
Rewriting this and minimize over $\lambda\ge 0$, we have
\eq{
    \begin{aligned}\mathbb{P}\left(\frac1n\sum_{i=1}^n(X_i-\mathbb{E}[X_i])\geq t\right)\leq\min_{\lambda\geq0}\exp\left(\frac{n\lambda^2(b-a)^2}8-\lambda nt\right)=\exp\left(-\frac{2nt^2}{(b-a)^2}\right)\end{aligned}
}
}

\subsection{Convergence}

\clearpage
\section{Convex Optimization}

\subsection{Linear Programming}
The main contents of this section are notes from \cite{luenberger1984linear}, \cite{bertsimas1997introduction}.
The \tit{standard form} of the linear programming:
\eq{
    \begin{array}{rl}\text{minimize}&c_1x_1+c_2x_2+\ldots+c_nx_n\\\text{subject to}&a_{11}x_1+a_{12}x_2+\ldots+a_{1n}x_n=b_1\\&a_{21}x_1+a_{22}x_2+\ldots+a_{2n}x_n=b_2\\&\vdots\quad\vdots\\&a_{m1}x_1+a_{m2}x_2+\cdots+a_{mn}x_n=b_m\\\text{and}&x_1\geqslant0,x_2\geqslant0,\ldots,x_n\geqslant0,\end{array}
}
or the above equations can be concisely written in:
\eq{
    \begin{array}{ll}\text{minimize}&\mathbf{c}^T\mathbf{x}\\\text{subject to}&\mathbf{A}\mathbf{x}=\mathbf{b}\quad\text{and}\quad\mathbf{x}\geqslant\mathbf{0}.\end{array}
}
\sep{Convertion to standard form LP}
\tbf{Slack Variable}\\
For this kind of LP formation:
\eq{
    \begin{array}{rl}\text{minimize}&c_1x_1+c_2x_2+\ldots+c_nx_n\\\text{subject to}&a_{11}x_1+a_{12}x_2+\ldots+a_{1n}x_n\le b_1\\&a_{21}x_1+a_{22}x_2+\ldots+a_{2n}x_n\le b_2\\&\vdots\quad\vdots\\&a_{m1}x_1+a_{m2}x_2+\cdots+a_{mn}x_n\le b_m\\\text{and}&x_1\geqslant0,x_2\geqslant0,\ldots,x_n\geqslant0,\end{array}
}
The above formulation can be transformed into the following standard form:
\eq{
    \begin{array}{rlr}\text{minimize}&c_1x_1+c_2x_2+\cdots+c_nx_n\\\text{subject to}&a_{11}x_1+a_{12}x_2+\cdots+a_{1n}x_n+y_1=b_1\\&a_{21}x_1+a_{22}x_2+\cdots+a_{2n}x_n+y_2=b_2\\&\vdots\quad\vdots\\&a_{m1}x_1+a_{m2}x_2+\cdots+a_{mn}x_n+y_m=b_m\\\text{and}&x_1\geqslant0,x_2\geqslant0,\ldots,x_n\geqslant0,\\\text{and}&y_1\geqslant0,y_2\geqslant0,\ldots,y_m\geqslant0.\end{array}
}
Now the constraint would be modified from $\tbf{A}$ to $\tbf{[A, I]}$, and the number of unknowns is changed from $n$ to $n+m$.\\
\tbf{Surplus Variable}\\
Similar to the slack variable, formulation like:
\eq{
a_{i1}x_1+a_{i2}x_2+\cdots+a_{in}x_n\geqslant b_i
}
can be transformed into:
\eq{
a_{i1}x_1+a_{i2}x_2+\cdots+a_{in}x_n-y_i=b_i
}\\
\tbf{Free Variable}\\
For some variables without the constraint of the sign, there are two methods to transform it into the standard form. One is
to $x_i=u_i-v_i$, where both $u_i$ and $v_i$ are larger or equal to zero. Another method can be used when the following condition holds:\\
\eq{
    a_{1}x_1+\cdots + a_{i}x_i+\cdots+a_{n}x_n=b_i
}
where $x_i$ is the free variable, thus we can replace $x_i$ by $b_i-\sum_{j\ne i}^n a_jx_j$, which can eliminate one variable and one constraint simultaneously.\\
One important example of the linear programming model is the \tbf{maximal flow problem}:
\fig{A network with capacities}{A network with capacities}{}
This problem can be formulated into:
\eq{
    \begin{aligned}
         & \text{minimize} \text{f}                                               \\
         & \text{subject to} \sum_{j=1}^nx_{1j}-\sum_{j=1}^nx_{j1}-f=0            \\
         & \sum_{j=1}^nx_{ij}-\sum_{j=1}^nx_{ji}\quad=0,               & i\neq1,m \\
         & \sum_{j=1}^nx_{mj}-\sum_{j=1}^nx_{jm}+f=0                              \\
         & 0\leq x_{ij}\leq k_{ij},\quad\quad\text{for all }i,j
    \end{aligned}
}
\sep{Basic Solutions}
The system of linear equalities:
\eq{
    \mathbf{A}\mathbf{x}=\mathbf{b}
}
where $\tbf{A}$ is a $m\times n$ constraint matrix and $\tbf{x}$ is the $n \times 1$ decision variables.
\ass{
    Full Rank Assumption
}{The $m\times n \tbf{A}$ has $m<n$, and the $m$ rows of $\tbf{A}$ are linearly independent}.
This assumption makes the linear equalities always have at least one basic solution.
At least $m$ linearly independent columns $\to \tbf{B}$, then would get:
\eq{
\mathbf{B}\mathbf{x}_{\mathbf{B}}=\mathbf{b}
}
\defi{Basic Feasible Solution}{$\mathbf{x}=(\mathbf{x}_{\mathbf{B}},\mathbf{0})$ is the \tbf{basic solution}, the components of
$\mathbf{x}$ related to the columns of $\mathbf{B}$ are \tbf{basic variables}\\
If the solution also satisfies $\mathbf{x}\ge0$, then it's called a \tbf{basic feasible solution}.\\
If some of the basic variables have value zero, then it's called a \tbf{degenerate basic solution}.\\
Similar to the definition above, we have \tbf{degenerate basic feasible solution}.}
\thm{Fundamental Theorem of Linear Programming}{
    Given a linear program in standard form, where $\tbf{A}$ is an $m\times n$ matrix of rank $m$:
    \lis{
        \item if there is a feasible solution, there is a basic feasible solution;
        \item if there is an optimal feasible solution, there is an optimal basic feasible solution.
    }
    \proo{}{
        (1)\\
        If there is a feasible solution $\mathbb{x}$, the solution satisfy:
        \eq{x_1\mathbf{a}_1+x_2\mathbf{a}_2+\cdots+x_n\mathbf{a}_n=\mathbf{b}}
        Assume there are $p$ of variable $x_i>0$, then the following equation holds:
        \eq{x_1\mathbf{a}_1+x_2\mathbf{a}_2+\cdots+x_p\mathbf{a}_n=\mathbf{b}}
        Then there are two cases:
        \lis{
            \item if $\mathbf{a}_{1},\mathbf{a}_{2},\ldots,\mathbf{a}_{p}$ are linearly independent, then $p\le m$, which means $\mathbf{x}$ is already a basic solution;
            \item otherwise, there would be a non-trivial solution for $y_1\mathbf{a}_1+y_2\mathbf{a}_2+\cdots+y_p\mathbf{a}_p=\mathbf{0}$, which
            means $(x_1-\varepsilon y_1)\mathbf{a}_1+(x_2-\varepsilon y_2)\mathbf{a}_2+\cdots+(x_p-\varepsilon y_p)\mathbf{a}_p=\mathbf{b}$.\\
            Then for any value of $\epsilon$, $\mathbf{x}-\varepsilon\mathbf{y}$ is a solution but may violate the signal constraint.\\
            Mention that there is at least one $y_i$ that is negative or positive, thus there is at least one $x_i$ decreasing when we increase the $\epsilon(\epsilon > 0)$,
            thus we set $\varepsilon=\min\{x_i/y_i:y_i>0\}$, which would bring us to a new feasible solution but the number of zeros is larger.\\
            Through such iteration, we can get at least one basic feasible solution.
        }\\
        (2)\\
        The idea is the same as the first one. In case one it's obvious. in case two, we need to prove for any $\epsilin$, $\mathbf{x}-\varepsilon\mathbf{y}$ is still optimal.\\
        Note the new value is $\mathbf{c}^T\mathbf{x}-\varepsilon\mathbf{c}^T\mathbf{y}$. Because $\varepsilon$ can be both positive and
        negative, thus $\mathbf{c}^T\mathbf{y}=0$, which makes $\mathbf{x}-\varepsilon\mathbf{y}$ still as optimal solution.
    }
    \re{
        This theorem reduce the original problem to the size of $\begin{pmatrix}n\\m\end{pmatrix}=\frac{n!}{m!(n-m)!}$.
    }}
\sep{Relationship with the Convex Optimization}
\defi{Extreme Point}{A point $\tbf{x}$ in a convex set $\cC$ is an \tit{extreme point} if there are \tbf{no} two distinct
    $\tbf{x}_1,\tbf{x}_2\in\cC$ such that $\mathbf{x}=\alpha\mathbf{x}_{1}+(1-\alpha)\mathbf{x}_{2}$ for some $\alpha, 0<\alpha<1$}
\thm{Equivalence of extreme points and basic solutions}{Let \tbf{A} be an $m \times n$ matrix with rank $m$, Let \tbf{K} denote the \tit{convex polytope}
consisting all vector $\tbf{x}$ satisfying $\mathbf A\mathbf x=\mathbf b,\mathbf x\geqslant\mathbf0$.\\
A vector \tbf{x} is an extreme point of \tbf{K} if and only if \tbf{x} is a basic feasible solution.
\proo{}{
(1) BFS $\to$ extreme point:\\
Suppose $\textbf{x}=(x_1,x_2,\ldots,x_m,0,0,\ldots,0)$ is a BFS, it satisfies $x_1\mathbf{a}_1+x_2\mathbf{a}_2+\cdots+x_m\mathbf{a}_m=\mathbf{b}$.
If $\mathbf{x}=\alpha\mathbf{y}+(1-\alpha)\mathbf{z}$, since the value in $\mathbf{y}, \mathbf{z}$ is larger than 0, then we have:
\eq{y_1\mathbf{a}_1+y_2\mathbf{a}_2+\cdots+y_m\mathbf{a}_m=\mathbf{b}\\z_1\mathbf{a}_1+z_2\mathbf{a}_2+\cdots+z_m\mathbf{a}_m=\mathbf{b}}
Because the vectors $\mathbf{a}_1,\mathbf{a}_2,\ldots,\mathbf{a}_m$ are linearly independent, then we can get $\mathbf{x}=\mathbf{y}=\mathbf{z}$,
which means that $\tbf{z}$ is an extreme point.\\
(2) Extreme point $\to$ BFS:\\
Assume that $\tbf{x}$ has $k$ components larger than zero, then we have:
$y_1\mathbf{a}_1+y_2\mathbf{a}_2+\cdots+y_k\mathbf{a}_k=0$. Assume that $\mathbf{a}_1, \mathbf{a}_2, \cdots, \mathbf{a}_k$ are
linearly dependent, which would lead to:
\eq{
    y_1\mathbf{a}_1+y_2\mathbf{a}_2+\cdots+y_k\mathbf{a}_k=0
}
Define $\mathbf{y}=(y_{1},y_{2},\ldots,y_{k},0,0,\ldots,0)$, it's obvious to see the following can exist:
\eq{
    \mathbf x+\epsilon\mathbf y\geqslant0,\quad\mathbf x-\epsilon\mathbf y\geqslant0
}
Contradicts that $\mathbf{x}$ is an extreme point $\to$ $\mathbf{a}_1, \mathbf{a}_2, \cdots, \mathbf{a}_k$ are
linearly independent $\to$ $\mathbf{x}$ is a BFS.
}}
\co{}{\lis{
        \item If the convex set $\tbf{K}$ is nonempty, there is at least one extreme point;
        \item If there is a finite optimal solution to a linear programming problem, there is a finite optimal solution which is an extreme point of the constraint set.;
        \item The constraint set $\tbf{K}$ possesses at most a finite number of extreme points;
        \item If \tit{K} is bounded, then it's a \tit{convex polyhedron}.
    }}
\subsubsection{Simplex Method}
The standard form linear programming can be transformed to the \tbf{canonial form} (reduced row-echelon form/tabular method
). The canonical form provides basic variables and non-basic variables. \tbf{Pivot equations} can transform a non-basic variable
into a basic variable.
\eq{
    \begin{cases}\bar a_{ij}'=\bar a_{ij}-\frac{\bar a_{iq}}{\bar a_{pq}}\bar a_{pj},i\neq p\\\bar a_{pj}'=\frac{\bar a_{pj}}{\bar a_{pq}}.\end{cases}
}

\subsection{Convexity and Continuity}
\defi{Differentiability}{
    A mapping $F:\mathbb{R}^n\to\mathbb{R}^m$ is said to be \tit{differentiable} at $x$ if there is a
    function: $DF:\mathbb{R}^n\to\mathbb{R}^{m\times n}$ such that:
    \eq{
        \lim_{h\to0}\frac{\|F(x+h)-F(x)-DF(x)\cdot h\|}{\|h\|}=0.
    }
    The matrix $DF(x)\in \RR^{m\times n}$ is the \rt{Jacobian Matrix}.\\
    If $F$ is a $\mathbb{R}^n\to\mathbb{R}$ mapping, then the \tbf{gradient} can be expressed as:
    \eq{
        \nabla f(x)=Df(x)^\top=\begin{pmatrix}\frac\partial{\partial x_1}f(x)\\\vdots\\\frac\partial{\partial x_n}f(x)\end{pmatrix}
    }
    The \tbf{Hessian Matrix} (symmetric matrix) can be expressed by:
    \eq{
        \nabla^2f(x)=H_f(x)=\begin{pmatrix}\frac{\partial f}{\partial x_1\partial x_1}(x) & \frac{\partial f}{\partial x_1\partial x_2}(x) & \cdots                                         & \frac{\partial f}
               {\partial x_1\partial x_n}(x)                                                                                                                                        \\\cdot&\cdot&\cdots&\cdot\\\cdot&\cdot&\cdot&\cdot\\\cdot&\cdot&
               \cdot                                          & \cdot                                                                                                               \\\cdot&\cdot&\cdots&\cdot\\\frac{\partial f}{\partial x_n\partial x_1}(x)&\frac{\partial f}
               {\partial x_n\partial x_2}(x)                  & \cdots                                         & \frac{\partial f}{\partial x_n\partial x_n}(x)\end{pmatrix}
    }
}
\ex{Calculate the gradient of $f:\mathbb{R}^n\to\mathbb{R},\quad f(x)=\frac12x^\top Ax+b^\top x+c$, where $A\in\mathbb{R}^{n\times n}\text{ be symmetric}$.}{
    Use the definition of Differentiability, first calculate $f(x+h)-f(x)$:
    \eq{\begin{align}f(x+h)-f(x) & =\frac12(x+h)^TA(x+h)+b^T(x+h)+c-f(x)                     \\
                         & = (x+h)^TAx+(x+h)^TAh+b^Tx+b^Th-\frac12x^\top Ax+b^\top x \\
                         & = \frac12h^TAx+\frac12x^TAh+\frac12h^TAh+b^Th             \\
                         & = \frac12(h^TAx)^T+\frac12x^TAh+\frac12h^TAh+b^Th         \\\\
                         & = X^TAh+b^Th
        \end{align}

    }
    Thus $\nabla f(x)=Ax+b$.\\
    If $A$ is not symmetric, then $\nabla f(x)=\frac{A^T+A}{2}x+b$.}
\re{
    \tbf{FONC (First-Order Necessary Conditions)}\\
    If $x^{\star}$ is a local minimizer of the unconstrained problem, then we must have $\nabla f(x^*)=0.$}
\thm{SONC (Second Order Necessary Condition)}{
If $x^*$ is a local minimizer of $f$, then it holds that:
\lis{
\item $\nabla f(x^*)=0$;
\item $\mathrm{For~all~}d\in\mathbb{R}^n{:}d^\top\nabla^2f(x^*)d\geq0$ ($\nabla^2f(x^*)$ is positive semidefinite);
}
\defi{Saddle Point}{
    A point satisfying FONC is a \tbf{stationary point}, a stationary point with an indefinite Hessian matrix is called \tbf{saddle point}.
}
}
\fig{An example of saddle point}{An example of saddle point}{}
\thm{SOSC (Second Order Sufficient Conditions)}{
\lis{
\item $\nabla f(x^*)=0$;
\item  $\mathrm{For~all~}d\in\mathbb{R}^n{:}d^\top\nabla^2f(x^*)d>0$ ($\nabla^2f(x^*)$ is positive definite);
}
Then $x^*$ is a \tit{strict local minimum} of $f$.
\proo{}{
    By \tit{taylor expansion}, $f(x^*+td)=f(x^*)+\frac12t^2d^\top\nabla^2f(x^*)d+o(t^2)>f(x^*)$.
}
}
\defi{Coercivity}{
    A continuous function $f:\mathbb{R}^n\to\mathbb{R}$ is said to be \tbf{coercive} if:
    \eq{
        \lim_{\|x\|\to\infty}f(x)=+\infty
    }
    What's more, if $f$ is a coercive function, then the level set $L_{\leq\alpha}:=\{x\in\mathbb{R}^n:f(x)\leq\alpha\}$ is \tit{compact}
    and has at least one \tit{global minimizer}.
}

\sep{Convexity}
\defi{Convex Sets and Functions}{
    \tbf{Convex Sets}\\
    A set $X\subseteq\mathbb{R}^n$ is \tbf{convex} if for any $x,y\in X$, and any $\lambda \in [0,1]$, we have $\lambda x+(1-\lambda)y\in X$.\\
    For example, the half space $H:=\{x\in\mathbb{R}^n:a^\top x\leq b\}$ and the closed ball $B_r(a):=\{x\in\mathbb{R}^n:\|x-a\|\leq r\}$ are both convex sets.\\
    \co{Intersection of Convex Sets}{
        The intersection of convex sets is a convex set. For example, the \tit{Polyhedral Sets}: $\{x\in\mathbb{R}^n:Ax\leq b\}$.
    }
    \tbf{Convex Functions}\\
    A function $f$ is said to be \tit{convex} on a \tit{convex sets} $X$ is for every $x,y\in X$ and any $0\le \lambda \le 1$:
    \eq{
        f(\lambda x+(1-\lambda)y)\leq\lambda f(x)+(1-\lambda)f(y)
    }
    Examples are The Euclidean norm $f(x)=\|x\|=\sqrt{x^\top x}$ and affine-linear functions $f(x)=a^\top x+b$.\\
    If a function $f$ has the property $f(\lambda\mathbf{x}+(1-\lambda)\mathbf{y})<\lambda f(\mathbf{x})+(1-\lambda)f(\mathbf{y})$,
    then it's said to be \tbf{strictly convex}.\\
    \tbf{Strongly Convex} (with parameter $\mu$)\\
    \eq{
        f(\lambda x+(1-\lambda)y)+\frac{\mu\lambda(1-\lambda)}2\|y-x\|^2\leq\lambda f(x)+(1-\lambda)f(y)
    }
    This is equivalent to $f-\frac{\mu}{2}\lVert \cdot \rVert^2$.
    \lem{General Composition}{
        Let $h:X\to \RR$ be \tit{convex} and $g:Y\to \RR$ be \tit{convex} and \tbf{non-decreasing}. Then $f(x)=g(h(x))$ is convex.
    }
}
\re{Operations that preserve convexity
    \tab{
        \item Nonnegative weighted sums;
        \item Pointwise maximum;
        \item Composition of non-decreasing functions;
        \item power of non-negative function.
    }}
\thm{Convexity and Differentiability}{
    $f$ is convex \tbf{if and only if}:
    \eq{
        f(y)-f(x)\geq\nabla f(x)^\top(y-x),\quad\forall\mathrm{~}x,y\in X
    }
    Or
    \eq{
        h^\top\nabla^2f(x)h\geq0,\quad\forall\mathrm{~}h\in\mathbb{R}^n,\quad\forall\mathrm{~}x\in X
    }
}
\thm{Convexity and Optimality}{
    If $f$ is a convex function and $X$ is a convex set, then for the problem $\min f(x)\quad\mathrm{s.t.}\quad x\in X$, we have:
    \tab{
        \item Every local minimizer is also a global minimizer;
        \item If $f$ is strongly convex, then it has at most one global minimizer, which is the stationary point.
    }
}
\lem{Convexity and Level Sets}{
    Let $f$ be a convex function, then for any $\alpha$, the level set $\mathcal{L}_{\leq\alpha}=\{x:f(x)\leq\alpha\}$ is a convex set.\\
    Linear constraints are always convex.
}
\ex{Is the optimization problem:
    \eq{
        \begin{aligned}\mathrm{maximize}_{x,y,z}&&\mathrm{xyz}\\\mathrm{s.t.}&&x+2y+3z\leq3\\&&x,y,z\geq0\end{aligned}
    }
    convex or not?}{
    All constraints are convex since they are linear. The objective $xyz$ is not concave. However,
    we can turn the above problem into an equivalent form:
    \eq{
        \begin{aligned}\mathrm{maximize}_{x,y,z}&&\log xyz\\\mathrm{s.t.}&&x+2y+3z\leq3\\&&x,y,z\geq0\end{aligned}
    }
    This problem is convex since $\log xyz = \log x +\log y+\log z$, which is a concave-preserved operation on concave functions.
}

\subsubsection{Lipschitz Continuity}
\defi{Lipschitz Continuous \& Smooth}{
    The $\nabla f$ is called \rt{Lipschitz continuous} if:
    \eq{
        \|\nabla f(x)-\nabla f(y)\|\leq L\|x-y\|,\quad\forall\mathrm{~}x,y\in\mathbb{R}^n,
    }
    where $L$ is the \rt{Lipschitz constant} and $f$ is called \rt{Lipschitz smooth}.\\
    \ex{Consider $f(x)=\frac12x^\top Ax+b^\top x+c$}{
        \eq{
            \begin{aligned}\|\nabla f(x)-\nabla f(y)\|&=\|(Ax+b)-(Ay+b)\|\\&=\|A(x-y)\|\leq\|A\|\cdot\|x-y\|.\end{aligned}
        }
        So $f(x)$ is Lipschitz Smooth with $L=\|A\|$.
    }
    \lem{Descent Lemma}{
        Let $f$ be Lipschitz Smooth with $L$, then:
        \eq{
            f(y)\leq f(x)+\nabla f(x)^\top(y-x)+\frac L2\|y-x\|^2\quad\forall\left.x,y\in\mathbb{R}^n.\right.
        }
        The descent lemma provides a quadratic upper bound for $f$.
        \proo{}{
            By Taylor's Expansion, the vector $z_t = x+t(y-x)$ can be expressed by $f(y)=f(x)+\int_0^1\langle\nabla f(z_t),y-x\rangle dt$.
            Subtracting $\langle\nabla f(z_t),y-x\rangle$ on each side, we have:
            \eq{
                \begin{aligned}
                    \begin{aligned}f(y)-f(x)-\langle\nabla f(x),y-x\rangle\end{aligned}    & \begin{aligned}=\quad\int_0^1\langle\nabla f(z_t)-\nabla f(x),y-x\rangle dt\end{aligned} \\
                    \begin{aligned}&|f(y)-f(x)-\langle\nabla f(x),y-x\rangle|\end{aligned} & =\quad\left|\int_0^1\langle\nabla f(z_t)-\nabla f(x),y-x\rangle dt\right|                \\
                                                                                           & \leq\quad\int_0^1|\langle\nabla f(z_t)-\nabla f(x),y-x\rangle|dt                         \\
                                                                                           & \leq\quad\int_0^1\|\nabla f(z_t)-\nabla f(x)\|_2\cdot\|y-x\|_2dt                         \\
                                                                                           & \leq\quad L\int_0^1t\|x-y\|_2^2dt                                                        \\
                                                                                           & =\quad\frac L2\|x-y\|_2^2.
                \end{aligned}
            }
        }
    }
}
\thm{Lipschitz Continuity via Hessians}{
    Let $f$ be a twice cont. differentiable function. Then, the following two conditions are equivalent:
    \tab{
        \item $f$ is Lipschitz smooth with constant $L$;
        \item $\|\nabla^2 f(x)\|\le L$ for any $x\in\RR^n$.
    }
}

\subsection{Single Variable Problem}
\tbf{Bisection method} uses the idea that the local minimizer must satisfy the FONC: $f^{\prime}(x)=0.$
\fig{Bisection Methods}{Bisection Method}{0.5}
The bisection method can only help us find an approximate \tit{stationary point}. When $f$ is convex, this is an approximate global minimizer of $f$.\\
The iteration of the bisection method takes at most $\log_2(\frac{x_r-x_\ell}\epsilon)$ steps.\\
\tbf{Golden section method} can optimize those problems whose first derivative is difficult to obtain, but it requires us to know the range of the local optimal solution.\\
\fig{Golden Section Method}{Golden Section Method}{0.5}
If we set $\phi=\frac{3-\sqrt{5}}2$, then we can save one function evaluation during each iteration.\\
A necessary condition for the minimizer is $g(x)=f^\prime(x)=0$. \tbf{Newton's Method in single dimension} approximates $g$ using a first-order
Taylor expansion for each $x^k$: $g(x)\approx g(x^k)+g^{\prime}(x^k)(x-x^k).$ By setting the right-hand side to 0 and solving:
\meq{
    x^{k+1}=x^k-\frac{g(x^k)}{g^{\prime}(x^k)};\\
    x^{k+1}=x^k-\frac{f^{\prime}(x^k)}{f^{\prime\prime}(x^k)}.
}
where we assume $g^\prime(x^k)\ne0$ at each step. However, Newton's method may not converge for every initial point.
\thm{Quadratic Convergence of Newton's method in 1-D case}{
If $g$ is twice continuous and $x^\star$ is a root of $g$ (global minimizer) and $g^\prime (x^\star)\ne0$. When $|x^0-x^\star|$ is sufficiently small, the sequence
by Newton's method $x^{k+1}=x^k-\frac{g(x^k)}{g^{\prime}(x^k)}$ will satisfy:
\eq{
|x^{k+1}-x^*|\leq C|x^k-x^*|^2.
}
}
Another interpretation of Newton's method: consider a second-order Taylar expansion for $f$:
\eq{
f(x)\approx f(x^k)+f^{\prime}(x^k)(x-x^k)+\frac12f^{\prime\prime}(x^k)(x-x^k)^2.
}
The Newtion's method is a minimizer for the quadratic model. If the original objective function is quadratic, then Newton's method converges in one step.

\subsection{High Dimensional Search}
Consider:
\eq{
\mathrm{minimize}_{x\in\mathbb{R}^n}\quad f(x).
}
We typically use iterative steps of the form:
\eq{
    x^{k+1}=x^k+\alpha_kd^k.
}
The $d^k\in \RR^n$ is called a \rt{descent direction} of $f$ if $\nabla f(x)^\top d<0.$ The
\rt{step size} $\alpha_k$ may be chosen in accordance with some line search (\tit{one-dimensioanl}) rules.\\
\fig{Abstract Descent Method}{Abstract Descent Method}{0.5}
A popular stopping criterion is $\|\nabla f(x^k)\|\leq\text{to}1$ with \rt{tolerance} $tol>0$.
\defi{Steepest Descent Directions}{
    Let $f$ be convex and let $x\in \RR^n$ be given with $nabla f(x)\ne 0$. Let $d\in\RR^n$ denote
    the solution of:
    \eq{
        \min_{\|\boldsymbol{d}\|=1}\nabla f(\mathbf{x})^\top\boldsymbol{d}.
    }
    Every vector of the form $s=\lambda d, \lambda>0$, is called \tbf{steepest descent direction}.\\
    By Cauchy-Schwarz inequality: $|\nabla f(x)^{T}d|\leq||\nabla f(x)||\cdot||d||$, which implies
    $\nabla f(x)^{T}d\ge -||\nabla f(x)||\cdot||d||$, thus, we can have $\nabla f(x)^{T}d\ge -||\nabla f(x)||$.
    So $d^\star=-\nabla f(x)/\|\nabla f(x)\|.$\\
    We can choose the descent directions $d=-\lambda \nabla f(x),\lambda>0$.
}
\re{
    \tab{
        \item The steepest descent norm depends on the chosen norm (here it is Euclidean norm);
        \item The gradient $\nabla f(y)$ is \tit{perpendicular} to the level set and it points towards a
        direction where $f$ is decreasing the most.
    }
}

\subsubsection{Step Size Strategies}
There is one simple step size strategy: \tbf{constant step size}, by which we choose $\alpha_k=\bar{\alpha}$ for all $k$.
\sep{Exact Line Search}
We choose $\alpha_k$ such that:
\eq{
    \alpha_k=\arg\min_{\alpha\geq0}f(x^k+\alpha d^k).
}
Golden section methods can solve $\alpha_k$. In some cases, there are analytical solutions for $\alpha_k$.
\pro{The "zig-zag" path of ELS}{
The directions between two consecutive steps are perpendicular:
\eq{
(d^{k+1})^\top d^k=0,\\
(x^{k+2}-x^{k+1})^T(x^{k+1}-x^k)=0.
}
\proo{}{
    For the $k$th iteration, the $alpha_k$ need to satisfy $\alpha_k=\arg\min_{\alpha\geq0}f(x^k+\alpha d^k)$. Denote $\phi(\alpha)=f(x^k+\alpha d^k)$,
    so $\phi^{\prime}(\alpha)=\nabla f(x^k+\alpha d^k)^T d^k$. When $alpha=0$, $\phi^{\prime}(\alpha)=-\|\nabla f(x^k)\|^2<0$. To minimize the formula, we
    need to set $\phi^{\prime}(\alpha)=0$, then:
    \eq{
        \nabla f(x^k+\alpha^k d^k)^\top d^k=0\\
        (d^{k+1})^\top d^k=0.
    }
}
}
\sep{Backtracking Line Search}
Let $\sigma,\gamma \in (0,1)$ be given, choose $\alpha_k$ as the largest element from $(1,\sigma,\sigma^2,\sigma^3,\cdots)$ such that:
\eq{
    f(x^k+\alpha_kd^k)-f(x^k)\leq\gamma\alpha_k\cdot\nabla f(x^k)^\top d^k.
}
This is called \tbf{\rt{Armijo condition}}.\\
Procedure:
\lis{
    \item Start with $\alpha=1$ or $\alpha=s$;
    \item If $f(x^k+\alpha d^k)\leq f(x^k)+\gamma\alpha\cdot\nabla f(x^k)^\top d^k,$ choose $\alpha_k=\alpha$, else set $\alpha=\sigma\alpha$ and repeat.
}
By Taylor expansion, for small $\alpha$:
\eq{
    f(x^k+\alpha d^k)\approx f(x^k)+\alpha\nabla f(x^k)^\top d^k<f(x^k)+\gamma\alpha\cdot\nabla f(x^k)^\top d^k.
}
\fig{Armijo Rule}{Armijo Rule}{0.4}
\sep{Diminishing Step Sizes}
Choose $\alpha_k\to0$ and $\sum_{k=0}^{\infty}\alpha_k=\infty$, this is frequently used in stochastic programming.

\subsection{Gradient Descent Algorithm}
\fig{Gradient Descent Method}{Gradient Descent Method}{0.7}
\defi{Accumulation Point}{
    A point $x$ is an \tbf{accumulation point} of $\{x^k\}_k$ if for every $\epsilon>0$, there are infinitely many numbers $k$ with $x^k\in B_\varepsilon(x)$.
    \re{
        \tab{
            \item If $x$ is an \tbf{accumulation point} of $\{x^k\}_k$, then there is a subsequence of $\{x^k\}_k$ that converges to $x$;
            \item If $\{x^k\}_k$ converges to some $x\in\RR^n$, then $x$ is the unique accumulation point of $\{x^k\}_k$;
            \item A bounded sequence has at least one accumulation point.
        }
    }
}
\defi{Global Convergence}{
    The gradient method can find the stationary point \tbf{independent} of the chosen initial point.
    \thm{Global Convergence}{
        Let $f:\RR^n\to\RR$ be continuously differentiable, $\{x^k\}_k$ be the sequence generated by gradient method for:
        \eq{
            \min_xf(x)\quad\mathrm{s.t.}\quad x\in\mathbb{R}^n,
        }
        with \tit{exact line search} or \tit{Armijo rule} step size strategies, $\{f(x^k)\}_k$ has the properties:
        \tab{
            \item it's nonincreasing;
            \item every accumulation point of $\{f(x^k)\}_k$ is a stationary point of $f$.
        }
        \re{
            \tab{
                \item It does not guarantee the existence of an accumulation point or convergence to a single limit point;
                \item If $f$ is \tit{coercive} (bounded), then $\{f(x^k)\}_k$ must have an accumulation point.
            }
        }
    }
}
\thm{Convergence under Lipschitz Continuity}{
    Let $f:\RR^n\to\RR$ be Lipschitz Smooth, $\{x^k\}_k$ be the sequence generated by gradient method for:
    \eq{
        \min_xf(x)\quad\mathrm{s.t.}\quad x\in\mathbb{R}^n,
    }
    with \tit{constant step size} $\bar{\alpha}\in(0,\frac{2}{L})$, \tit{exact line search}, \tit{diminishing} step size
    or \tit{Armijo rule} step size strategies, $\{f(x^k)\}_k$ either:
    \tab{
        \item converge to $-\infty$;
        \item or converge to a finite value that $\|\nabla f(x^k)\|\to0.$
    }}
\co{Complexity Bounds}{
Complexity bounds characterize the behavior and progress of an algorithm during the very first iterations.\\
If $f$ is Lipschitz smooth with $L$ and lower bounded by $\bar{f}$, then it holds:
\eq{
    \min_{i=0,\cdots,T-1}\|\nabla f(x^{i})\|\leq\frac{f(x^{o})-\overline{f}}{\sqrt{\beta T}}=\mathcal{O}(1/\sqrt{T})\quad\forall T.
}
here $\{x^k\}$ are generated by constant step size and $\bar{\beta}=\bar{\alpha}(1-\frac{L\bar{\alpha}}{2})$.
\re{
    \tab{
        \item We need to perform the gradient method for $T>\frac{1}{tol^2}$ iterations to ensure $\min_{i=0,\cdots,T-1}\|\nabla f(x^{i})\|\le tol$;
        \item If $f$ is convex and the problem has a solution, then the whole sequence converge to a global minimum;
        \item Convergence can be ensured if the stationary points are \gt{isolated};
        \item This works for backtracking, constant \& diminishing step sizes.
    }
}
\proo{}{
Recall the descent lemma:
\eq{
    f(y)\leq f(x)+\nabla f(x)^\top(y-x)+\frac L2\|y-x\|^2\quad\forall\left.x,y\in\mathbb{R}^n.\right.
}
Set $y=x^{k+1}$ and $x=x^k$, so $x^{k+1}-x^k=-\bar{\alpha}\nabla f(x^k)$, reducing the descent lemma to:
\eq{
    f(x^{k+1})\le f(x^k)-(1-\frac{L\bar{\alpha}}{2}\bar{\alpha}\cdot \|\nabla f(x^k)\|^2).
}
Denote $\bar{beta}=\bar{\alpha}(1-\frac{L\bar{\alpha}}{2})$, we have $\bar{\beta}\|\nabla f(x^k)\|^2\le f(x^{k})-f(x^{k+1})$. Then sum this for $k=0,\cdots,T-1$:
\eq{
    \bar{\beta}\cdot \sum_{k=0}^{T-1}\|\nabla f(x^k)\|^2 \le f(x^0)-f(x^T)\le f(x^0)-\bar{f}.
}
For the left part:
\eq{
    \bar{\beta}\cdot \sum_{k=0}^{T-1}\|\nabla f(x^k)\|^2 \ge \bar{\beta}\cdot \sum_{k=0}^{T-1}\min_{i=0,\cdots,T-1}\|\nabla f(x^k)\|^2=T\cdot \min_{i=0,\cdots,T-1}\|\nabla f(x^k)\|^2.
}
Finally, we have:
\eq{
    \min_{i\to0,\ldots,T\to1}\|\nabla f(x^{i})\|\leq\frac{f(x^{0})-\overline{f}}{\sqrt{\beta T}}=\mathcal{O}(1/\sqrt{T})\quad\forall T.
}
}
}
\defi{Linear Convergence}{
$\{x^k\}_k$ converges linear with rate $\eta\in(0,1)$ to $x^\star \in \RR^n$ if there is $\cL > 0$ such that:
\eq{
\|x^{k+1}-x^\star\|\le \eta\cdot\|x^k-x^\star\|, \forall k\g\cL
}
\re{
    \tab{
        \item linear convergence with $\cL=0$ implies that $\|x^k-x^\star$\| \le \eta^k \|x^0-x^\star\|.
    }
}
\thm{Rates for Strongly Convex Problems}{
Let $f$ be Lipschitz smooth with $L$ and suppose there is $\mu>0$ such that:
\eq{
    \mu\|\boldsymbol{h}\|^2\le\boldsymbol{h}^\top\nabla^2f(\boldsymbol{x})\boldsymbol{h}(\le\|\boldsymbol{h}\|^2)\forall\boldsymbol{h},\forall\boldsymbol{x}.
}
Then $\{x^k\}_k$ converges linearly to $x^\star$ if it's generated by the gradient method. Further, for $\eta=1-2M_{\mu}$, it follows:
\eq{
    f(x^k)-f(x^*)\leq\eta^k\cdot[f(x^0)-f(x^*)].
}
, where
\eq{
    M=\begin{cases}\tilde\alpha(1-\frac{L\bar\alpha}2)&\text{constant step size:}\bar\alpha\in(0,\frac2L),\\\frac1{2L}&\text{exact line search},\\\gamma\min\{1,\frac{2\sigma(1-\gamma)}L\}&\text{Armijo line search}.\end{cases}
}
The optimal rate can be $\eta=1-\frac\mu L=1-\frac1\kappa, \kappa=\frac{L}{\mu}$. The larger $\kappa$, the lower the convergence rate.
}
}

\subsection{Newton's Method}
To solve $\min_{x\in\mathbb{R}^n}f(x)\mathrm{~with~}f:\mathbb{R}^n\to\mathbb{R}$ using Newton's method, we approximate the function by its second-order Taylor
expansion at $x^k$:
\eq{
    f(x)\approx f(x^k)+\nabla f(x^k)^\top(x-x^k)+\frac12(x-x^k)^\top\nabla^2f(x^k)(x-x^k).
}
Minimizing this leads to:
\eq{
    x=x^k-(\nabla^2f(x^k))^{-1}\nabla f(x^k).
}
where $d^k = -(\nabla^2f(x^k))^{-1}\nabla f(x^k).$ is called the Newton direction. It's easy to find that $\nabla f(x)^\top\boldsymbol{d}=-\nabla f(x)^\top(\nabla^2f(x))^{-1}\nabla f(x)$,
so whether $\nabla f(x)^T d\le0$ depends on whether $\nabla^2 f(x)$ is positive semi-definite.
\fig{The Newton Method}{The Newton Method}{0.7}
\thm{Quadratic Convergence of Newton's Method}{
Let $f$ be twice continuously differentiable and let $x^\star$ be a local minimizer of $f$. For some $\epsilon>0$, if in $B_{\epsilon}(x^\star)$:
\tab{
    \item $f$ is strongly convex with $\mu$;
    \item $f$ is Lipschitz smooth with $L$.
}
Then $\{x^k\}_k$ follows:
\eq{
\|x^{k+1}-x^*\|\leq\frac L{2\mu}\|x^k-x^*\|^2.
}
In addition, if $\|x^0-x^*\|\leq\frac{\mu\min\{1,\varepsilon\}}L$:
\eq{
    \|x^k-x^*\|\leq\frac{2\mu}L\left(\frac12\right)^{2^k},\quad k=0,1,2,\ldots
}
}
\re{
    Comparison between gradient descent method and Newton's method:
    \tab{
        \item Newton takes fewer steps of iterations, but each iteration is more expensive;
        \item Newton requires more memory to store the Hessian matrix
    }
}

\subsection{Large-Scale Optimization}

\subsubsection{Inpainting}
Assume we have access to a binary mask matrix $\in \RR^{m\times n}$ with:
\eq{
    \text{Mask}_{ij}=\begin{cases}1&\text{the pixel }(i,j)\text{is not damaged},\\0&\text{the pixel }(i,j)\text{is damaged}.&\end{cases}
}
Then define $\text{mask}:=vec(\text{Mask}_{ij})\in\RR^{mn}, s:=\sum_{i=1}^{mn}\text{mask}_i$ as the number of undamaged pixels,
and $\mathcal{I}:=\{i:\mathrm{mask}_i=1\}=\{q_1,q_2,\cdots,q_s\}$. Then we can define the \gt{slection matrix}:
\eq{
    $A=\begin{bmatrix}-&e_{q_1}^\top&-\\&\vdots&\\-&e_{q_s}^\top&-\end{bmatrix}\in\mathbb{R}^{s\times mn},\quad[\mathbf{e}_j]_i=\begin{cases}1&\mathrm{~if~}i=j,\\0&\mathrm{~otherwise},&\end{cases}\quad\forall j\in\mathcal{I}.$
}
Let $U\in \RR^{m\times n}$ be the \tit{original undamaged image}, and $u=vec(U)\in\RR^{mn}$. Then we can formalize $b=Au\in\RR^s$ as
the information of all undamaged pixels in $U$. Just solving $Ax=b$ can produce infinite solutions. Instead, we inpaint via \tbf{total variation minimization}:
\eq{
\min_x\frac12\|Ax-b\|^2+\mu\sum_{i=1}^{m-1}\sum_{j=1}^{n-1}\|D_{(i,j)}\boldsymbol{x}\|_2,
}
where $\mu$ is a regularization parameter and $D_{(i,j)}\in\mathbb{R}^{2\times mn}$ is the \tit{image gradient} at pixel ($i,j$).
$D_{(i,j)}x=(\delta_1,\delta_2)^\top $, where
\eq{
\delta_1=X_{(i+1)j}-X_{ij}\quad\mathrm{and}\quad\delta_2=X_{i(j+1)}-X_{ij}.
}
However, the L2-norm $\|\cdot\|_2$ is not smooth at $0$, hence, we use \tbf{Huber-norm} to smooth the objective function:
\meq{
    \varphi_{\mathrm{hub}}(\mathbf{y})=\begin{cases}\frac1{2\delta}\|\mathbf{y}\|^2&\mathrm{if~}\|\mathbf{y}\|\leq\delta,\\\|\mathbf{y}\|-\frac12\delta&\mathrm{if~}\|\mathbf{y}\|>\delta,&\end{cases}\delta>0;\\
    \min_x\frac12\|Ax-b\|^2+\mu\sum_{i=1}^{m-1}\sum_{j=1}^{n-1}\varphi_{\mathrm{hub}}(D_{(i,j)}x).
}
This is called \tbf{\rt{TV-Huber problem}}.
\tab{
    \item TV-Huber problem is convex;
    \item Newton's method can not be applied to this problem (Huber norm is not twice continuously differentiable).
}

\clearpage
\chapter{Economics and Econometrics}

\section{Development Economics}

\subsection{Models of Development Economics}

\subsection{Clan Culture}

\defi{Clan Culture}{
    A clan is a consolidated kin group made up of component
    families that trace their patrilineal descent from a common ancestor.
}
\sep{History of Clans}
"Modern" clan originated in the Song Dynasty (860-1279 CE). At that time Neo-Confucian ideology was formed,
which provided the theoretical basis as well as clan organization structure design. The characteristics
of "modern" clan culture:
\lis{
    \item The families of a clan lived in the same or several nearby communities;
    \item Common properties and organized routine group activities, resource pooling;
    \item Compilation of genealogies;
    \item Own internal governance structures.
}
Currently although China has been transitioning for a long time from a traditional society to a modern society, clan culture is still
prevalent and has a broad impact on the lives of Chinese people, especially in rural areas.

\cite{bertrand2006role} shows the positive correlation between the fraction of family control among listed firms and family
ties using cross-country level data.
\cite{cheng2021clan} uses IV (the minimum distance to two prominent neo-Confucian academies,
the Kaoting Academy (Kaoting Shuyuan) and the Xiangshan Academy (Xiangshan Shuyuan)) to identify that
clan culture causes higher firm ownership concentration.
\fig{clan indensity}{Clan Culture Intensity}{}
Potential reasons that culture affects the concentration of family ownership:
\lis{
    \item Clan culture fosters high trust within the family and low trust in outsiders(\textbf{short-radius trust attitude}).
    According to agency-cost-based theories,
    family ownership can be concentrated in such a situation.
    \item \textit{Resources Pooling}: commom property ownership.
    \item \textit{Amenity Potential}: other things constant, owners subject to stronger influences of clan culture
    could have a higher utility.
}
\cite{zhang2020clans} estimates the effect of clan on entrepreneurship. He finds that clan leads to a higher
occurrence of entrepreneurship by helping overcome financing constraints and escape from local governments' "grabbing hand."
\cite{zhang2019family} investigates the relationship between the low take-up rate of social pensions and the clan culture intensity. In
his article, dummy variable $temple_c$ (whether community $c$ has ancestral temple) is constructed as the proxy variable for the strength of clan
culture. Some interesting insights are obtained:
\lis{
    \item Clan culture is positively related to adults raising children for support in their old age;
    \item Clan culture is associated with a larger number of children being born and a higher probability of having sons;
    \item Clan culture is associated with a higher coresidence rate between old parents and adult sons;
    \item Clan culture is associated with a higher likelihood of receiving financial transfers from non-coresident children;
    \item Clan culture is associated with a lower likelihood of participating in rural pension programs.
}
\cite{cao2022clans}
There is much research about clan culture outside of Mainland China. \cite{yang2019family} found the concave relationship the between the \textbf{heterogeneity}
of clan family and the provision of public goods. This finding implies that group homogeneity yields not only benefits, but also some possible costs.
re{Common Control Variables in Clan Research Identification(Individual Level):
        \lis{
            \item \textit{Hukou} Status;
        }
        Regional Level:
        \lis{
            \item Distance to the sea;
        }}
\sep{Data resources in Clan Research}
\re{\tab{
        \item \textit{Comprehensive Catalogue of the Chinese Genealogy} can be used to construct the strength of clan culture;
        \item
    }}

\clearpage
\section{Reduced-Form Identification}
The main contents of this chapter are the notes of \cite{angrist2014mastering}, \cite{angrist2009mostly}.

\subsection{Randomized Trials}
\key{ATT, ATE, Counter-factual World, Potential Outcome}
The outcome is $Y_i$, the potential outcome is $Y_{1i},Y_{0i}$:
\eq{Y_i=\begin{cases}Y_{1i}\quad&ifD_i=1\\Y_{0i}\quad&ifD_i=0\end{cases}\\Y_i=Y_{0i}+(Y_{1i}-Y_{0i})D_i}
\textit{Naive Comparison}:
\eq{\begin{aligned}\{\mathbf{E}[Y_i|D_i=1]-\mathbf{E}[Y_i|D_i=0]\} & =\{\mathbf{E}[Y_{1i}|D_i=1]-
               \mathbf{E}[Y_{0i}|D_i=1]\}                                                     \\&+\{\mathbf{E}[Y_{0i}|D_i=1]-\mathbf{E}[Y_{0i}|D_i=0]\}\end{aligned}}
\tab{\item observed difference: $\mathbf{E}[Y_i|D_i=1]-\mathbf{E}[Y_i|D_i=0]$;
    \item ATT: $\mathbf{E}[Y_{1i}|D_i=1]-\mathbf{E}[Y_{0i}|D_i=1]$;
    \item selection bias: $\mathbf{E}[Y_{0i}|D_i=1]-\mathbf{E}[Y_{0i}|D_i=0]$.
}
\no The existence of the selection bias is due to the dependence between $D_i$ and the \textbf{potential} outcome $Y_{1i},Y_{0i}$. Randomization can make
$D_i \perp Y_{1i},Y_{0i}$:
\eq{E[Y_{1i}|D_i=1]=E[Y_{1i}|D_i=0]=E[Y_{1i}]\\
E[Y_{0i}|D_i=1]=E[Y_{0i}|D_i=0]=E[Y_{0i}]}
\no So selection $=\mathbf{E}[Y_{0i}|D_i=1]-\mathbf{E}[Y_{0i}|D_i=0]=0$. \emocool Besides, by randomization, $ATT=ATE$.
Randomization example:
\tab{
    \item Rand HIE experiment: whether the insurance program makes people healthier;
    \item STAR: the effects of class size on education;
    \item OHP: this group experiment is not perfect because the group is not a determinant of whether to receive the treatment, but the treatment group does have
    a higher probability to get the treatment (\textbf{Instrumental Variable} can handle this situation);
}
\re{\tab{
        \item By randomization, the individual differences still exist;
        \item Checking for balance is an important step in randomization;
        \item The most critical idea of randomization is \textbf{Other Things Equal}(\textit{ceteris paribus});
        \item Randomization was invented by \textit{Ronald Aylmer Fisher} in 1925.
    }}

\subsection{Regression and Matching}
\key{\tab{
        \item CEF, CEF decomposition, ANOVA;
        \item regression justification,
    }}
\no Conditional Expectation Function (CEF) is a \hl{population} concept:
\eq{\begin{gathered}
        E[Y_i|X_i=x]=\int tf_y(t|X_i=x)dt \\
        E[Y_{i}|X_{i}=x]=\sum_{t}tP(Y_{i}=t|X_{i}=x)
    \end{gathered}}
\lem{The law of iterated expectations}{
    \eq{E\left[\mathrm{y}_i|\mathrm{X}_i=x\right]=\int tf_{\boldsymbol{y}}\left(t|\mathrm{X}_i=x\right)dt.}
    \proo{}{
        \eq{\begin{aligned}
                E\{E\left[\mathrm{y}_{i}|\mathrm{X}_{i}\right]\} & =\quad\int E\left[\mathrm{y}_{i}|\mathrm{X}_{i}=u\right]g_{x}(u)du                                                                                                                    \\
                                                                 & =\quad\int\left[\int tf_{\boldsymbol{y}}\left(t|\mathrm{X}_{i}=u\right)dt\right]g_{\boldsymbol{x}}(u)du                                                                               \\
                                                                 & \text{=} =\int\int tf_{y}\left(t|\mathrm{X}_{i}=u\right)g_{x}(u)dudt                                                                                                                  \\
                                                                 & =\quad\int t\left[\int f_{\boldsymbol{y}}\left(t|\mathrm{X}_{i}=u\right)g_{\boldsymbol{x}}(u)du\right]dt=\int t\left[\int f_{\boldsymbol{x}\boldsymbol{y}}\left(u,t\right)du\right]dt \\
                                                                 & =\quad\int tg_{y}(t)dt.
            \end{aligned}}
    }
}
\no \emoheart \hl{3 important property of CEF}:
\thm{CEF Decompostion Property}{
\eq{Y_i=E[Y_i|X_i]+\epsilon_i}
where $\epsilon_i$ is mean independent of $X_i$, and $X_i$ is uncorrelated with any function of $X_i$.
\proo{}{
Take the expectation of $X_i$ at both sides:
\eq{
    \begin{align}
        E[Y_i|X_i]        & = E[E[Y_i|X_i]|X_i]+E[\epsilon_i|X_i] \\
        E[\epsilon_i|X_i] & = E[Y_i|X_i] - E[Y_i|X_i]  = 0
    \end{align}
}
\eq{E[\epsilon_i]=\int_{X_i}f_x(t)E[\epsilon_i|X_i]dt=\int_{X_i}0dt=0=E[epsilon_i|X_i]}
}
}
\no \hl{\textbf{This means that $Y_i$ can be decomposed into 2 parts: explaind by $X_i$ and terms uncorrelated with $X_i$.}}
\thm{CEF Prediction Property}{
    CEF is the best estimator of $Y_i$ in the MMSE sense, which means:
    \eq{E\left[\mathrm{y}_i|\mathrm{X}_i\right]=\arg\min_{m(\mathrm{X}_i)}
        E\left[\left(\mathrm{Y}_i-m\left(\mathrm{X}_i\right)\right)^2\right]}
    \proo{}{
        \eq{
            \begin{array}{rcl}\left(\mathrm{Y}_i-m\left(\mathrm{X}_i\right)\right)^2&=&\left(\left(\mathrm{Y}_i-E\left[\mathrm{Y}_i|\mathrm{X}_i\right]\right)+\left(E\left[\mathrm{Y}_i|\mathrm{X}_i\right]-m\left(\mathrm{X}_i\right)\right)\right)^2\\&=&\left(\mathrm{Y}_i-E\left[\mathrm{Y}_i|\mathrm{X}_i\right]\right)^2+2\left(E\left[\mathrm{Y}_i|\mathrm{X}_i\right]-m\left(\mathrm{X}_i\right)\right)\left(\mathrm{Y}_i-E\left[\mathrm{Y}_i|\mathrm{X}_i\right]\right)\\&&+\left(E\left[\mathrm{Y}_i|\mathrm{X}_i\right]-m\left(\mathrm{X}_i\right)\right)^2\end{array}
        }
        The formula has the lowest constant value by setting $m\left(\mathrm{X}_i\right)=$ CEF.
    }}
\thm{ANOVA Theorem}{
\eq{\operatorname{V}(Y_i)=E[\operatorname{V}(Y_i|X_i)]+\operatorname{V}(E[Y_i|X_i])}
}
\no This indicates that the variance of $Y_i$ can be decomposed into two parts:
\lis{
    \item the variance of the CEF;
    \item the variance of the residual;
}
\re{\tab{
        \item The CEF property \hl{dosen't rely on any assumption}! It has nothing to do with regression right now;
        \item If $X_i$ is not mean independent of $Y_i$, then by ANOVA theorem, the variance of the outcome variable controlled
        by $X_i$ could be smaller;
    }}
\sep{CEF and (Population) Regression}
\eq{\begin{aligned}\beta&=\arg\min_bE[(Y_i-X_i'b)^2]\\1storder&:E[X_i(Y_i-X_i'b)]=0\\solution&:\beta=E[X_iX_i']^{-1}E[X_iY_i]\end{aligned}}
\thm{Regression Anatomy}{
\eq{\beta_k = \frac{Cov(Y_i,\tilde{X_{ki}})}{V(\tilde{X_{ki}})}}
\co{Bivariate Case}{
    \eq{\beta = \frac{Cov(Y_i,\tilde{X_i})}{V(\tilde{X_{i}})}}
}
\proo{}{Substitute\eq{
    Y_i=\alpha+\beta_1x_{1i}+\cdots+\beta_kx_{ki}+e_i
}
$\tilde{x}_{ki}$ is uncorrelated with $e_i$ and other covariates by construction, thus $Cov(\tilde{x}_{ki},x_{ki})=Var(\tilde{x}_{ki})$,
thus $Cov(Y_i,\tilde{x_{ki}}=\beta_k x_{ki})$.
}
\re{The regression anatomy shows that each $\beta_k$ in multi-regression is the bivariate slope after "partialing out"
    all the other regressors.}
}
\emogood Why the population regression coefficient is what we are interested in (Link with CEF):
\thm{Regression Justification}{\lis{
        \item Suppose the CEF is linear, then the population regression function is it;
        \item In any condition, $X^'_i\beta$ is the best predictor of $Y_i$ in a MMSE sense;
        \item The function $X^'_i\beta$ provides the MMSE linear approximation to $E[Y_i|X_i]$.\\
        \eq{\beta=\arg\min_bE\{(E[\mathrm{Y}_i|\mathrm{X}_i]-\mathrm{X}_i'b)^2\}}}
    \proo{}{Suppose $E[\mathrm{Y}_i|\mathrm{X}_i]=X_i^'\beta^{*}$. By regression decomposition theorem:
        \eq{\begin{align}
                E[X_i(Y_i-X_i'\beta^{*})]                             & = 0             \\
                \beta^{*}                  = E[X_iX_i']^{-1}E[X_iY_i] & = \tilde{\beta}
            \end{align}
        }
        \eq{
            \begin{aligned}
                \left(\mathrm{Y}_{i}-\mathrm{X}_{i}^{\prime}b\right)^{2} =\quad\{(\mathrm{y}_i-E[\mathrm{y}_i|\mathrm{X}_i])+(E[\mathrm{y}_i|\mathrm{X}_i]-\mathrm{X}_i^{\prime}b)\}^2 \\
                =\quad(\mathrm{y}_i-E[\mathrm{y}_i|\mathrm{X}_i])^2+(E[\mathrm{y}_i|\mathrm{X}_i]-\mathrm{X}_i^{\prime}b)^2                                                            \\
                +2(\mathrm{y}_i-E[\mathrm{y}_i|\mathrm{X}_i])(E[\mathrm{y}_i|\mathrm{X}_i]-\mathrm{X}_i^{\prime}b).
            \end{aligned}
        }}
    \co{}{\tab{
            \item For the saturated model, the population linear regression is the CEF;
            \item For the single dummy variable, the coefficient is the mean probability of receiving treatment;
        }

    }
}

\sep{From Regression to Causality}

\subsection{Instrumental Variables}

\subsubsection{Local Average Treatment Effect}

\subsection{Asymptotic Analysis}

\subsection{Empirical Classics}

\subsubsection{Returns to Schooling}
It's been a long history to study the effects of education on the salary. Counterfactuals here are multi-faceted: instead of the
single contrast $Y_{1i}-Y{0i}$, it is more often to study every possible schooling choice $Y_{n,i}-Y_{m,i}$, where $n,m$ are the
years of education.
\cite{mincer1974schooling} uses U.S. census data to do the following regression:
\eq{
    \ln Y_i = \alpha+\rho S_i+\beta_1 X_i +\beta_2 X_i^2+e_i,
}
where $X_i=Age_i-S_i-6$, defined as the \tit{potential experience}, and $S_i$ is the year of education. The result shows that one-year more eduction would bring 10.7\% more earnings.\\
This result may be biased because it doesn't consider an individual's ability, which is correlated both with earnings and education.\\
To handle the ability bias, \cite{griliches1977estimating} includes IQ as a control variable to eliminate the influence of personal
ability, the new $\rho$ estimated is 5.9\%, which is smaller than expected.\\
In the process of handling ability bias, be aware of the bad control: which is controlling the occupation. This is because the occupation
is a variable \tit{after} the education years, and some effect of the education is through occupation, controlling occupation would
underestimate the effect. \\
The above regressions don't consider other variables, one approach to handle this is by comparing the twins
(\cite{ashenfelter1994estimates}, \cite{ashenfelter1998income}), since each person of the twins is almost identical in IQ,
family background, etc. The long regression is written as:
\eq{
\ln Y_{i,f} = \alpha+\rho S_{i,f}+\lambda A_{i,f}+e_{i,f},
}
where subscript $i,f$ stand for individual and family respectively, $i=1,2$ indexes twin siblings. If we consider the regression within a family:
\eq{
\ln Y_{1,f} = \alpha + \rho S_{1,f} +\lambda A_f +e_{1,f}\\
\ln Y_{2,f} = \alpha + \rho S_{2,f} +\lambda A_f +e_{2,f}
}
Subtracting them would lead to:
\eq{
\ln Y_{1,f}-\ln Y_{2,f}=\rho (S_{1,f}-S_{2,f}) + e_{1,f}-e_{2,f}
}
This regression gives an estimation of 6.2\% for $\rho$. But in practice, the collected data encounters the misreported issue, which can bring the \tbf{Attenuation bias}.
\proo{Attenuation Bias}{
    $C(\cdot)$ denotes the covariance among two variables.
    Consider the regression:
    \eq{
        Y_i=\alpha+\beta S^\star_i + e_i
    }
    But the data for $S^\star_i$ is misreported, the observed $S_i=S^\star_i+m_i$, where we assume that $\bE[m_i]=0,C(S^\star_i,m_i)=0$.
    We assume the real parameter $\beta=\frac{C(Y_i,S^\star_i)}{V(S^\star_i)}$, and the biased parameter is $\beta_b=\frac{C(Y_i,S_i)}{V(S_i)}$:
    \eq{
        \begin{aligned}
            \beta_b & = \frac{C(Y_i,S_i)}{V(S_i)}                                           \\
                    & = \frac{C(\alpha+\beta S^\star_i+e_i, S_i^\star+m_i)}{V(S_i)}         \\
                    & = \frac{C(Y_i, S_i^\star)}{V(S_i)}=\beta \frac{V(S^\star_i)}{V(S_i)}.
        \end{aligned}
    }
    Because $V(S_i)=V(S^\star_i)+V(m_i)$, which means $\beta_b=r\beta$, where
    \eq{
        r(V(S^\star_i))=\frac{V(S^\star_i)}{V(S^\star_i)+V(m_i)}<1.
    }
    \co{Adding Covariates Can Exacerbate Attenuation Bias}{
        When adding covariates $X_i$, we would get:
        \eq{
            \beta_b=\frac{V(\tilde{S_i^\star})}{V(\tilde{S_i^\star})+V(m_i)}\beta=r(V(\tilde{S_i^\star}))\beta
        }
        Be aware that $V(\tilde{S_i^\star})<V(S^\star_i)$ because $V(S^\star_i)=V(\gamma X_i + \tilde{S_i^\star})$. And $r(\cdot)$ is a decreasing function, so adding
        covariates would exacerbate attenuation bias.
    }
}

\clearpage
\chapter{Data Science}

\section{Cloud Computing}

This section is mainly the notes of \cite{hwang2017cloud}.

\subsection{Principles of Cloud Computing System}
\key{\tab{
        \item HTC, HPC, physical machine, virtual machine, VM clusters;
        \item IaaS, PaaS, SaaS;
        \item Amdahl's Law, Gustafson's Law, cloud availability.
    }}
\defi{HPC and HTC}{
    \tab{
        \item \textbf{HTC} refers to \textit{highly-throughput computing} system built with parallel and distributed computing technologies;
        \item \textbf{HPC} refers to \textit{highly-performance computing} system in terms of raw speed in batch processing.
    }
}
\fig{Cloud Computing Technology Convergence}{Cloud Computing Technology Convergence}{}
\defi{Cloud}{A cloud is a pool of virtualized computer resources. A cloud can host a variety of different workloads,
    including batch-style backend jobs and interactive, user-facing applications.\\
    Some view the clouds as computing clusters with modest changes in \tit{virtualization}.}
\ex{\lis{
        \item Distinguish between physical machines and virtual machines.
        \item What are the fundamental differences between CPU and GPU as building blocks in a modern computer, or datacenter, or a cloud system?
        \item What are the fundamental differences between traditional data centers and modern cloud platforms?
        \item Differences between HPC and HTC.
    }}{\tbf{1}\\
    \lis{
        \item PM runs on hardware, VM runs on virtualized layer such as hypervisor or container;
        \item The resource of PM is fixed, for VM it can be dynamically adjusted;
    }
    (2)\\
    CPU has several cores and low latency, good for serial processing; GPU has many cores, high throughput, good for parallel processing.\\
    (3)\\
    One is HTC, built with many servers and the computing services are fixed; Cloud is HTC, built with virtualized clusters and more flexible.\\
    (4)\\
    HPC tasks need large amounts of computing power for short periods, whereas HTC tasks also require large amounts of computing, but for much longer times;\\
    HPC utilizes centralized high-performance processors to deal with a single large task, HTC utilizes less powerful processors from many servers to deal with the task that requires high throughput.\\
}
\fig{from HPC systems and clusters to p2p}{From HPC systems and clusters to grids, p2p networks, clouds and IoT}{}
Basic cloud service models are \tit{infrastructure as a service} (IaaS) or infrastructure cloud,
\tit{platform as a service} (PaaS) or platform cloud,
and \tit{software as a service} (SaaS) or application cloud. Their differences with on-premise computing are listed below:
\fig{comparing three cloud service models}{Comparing three cloud service models with on-premise computing}{}
\tab{
    \item \tbf{IaaS}: AWS, GoGrid;
    \item \tbf{PaaS}: Google App Engine, Microsoft Azure, Salesforce;
    \item \tbf{SaaS}: CRM, ERP, HR, Hadoop, Google Docs.
}
A physical cluster is a collection of servers (PMs) interconnected by a physical network. \tbf{virtual clusters} are built with multiple
VMs installed at PM belong to one or more physical clusters.
\fig{physical clusters and virtual clusters}{physical cluster and virtual cluster}{}
The virtual clusters have the following interesting properties:
\lis{
    \item Multiple VMs running with different OSs can be deployed on the same physical node;
    \item A VM runs with a guest OS, which is often different than the host OS that manages the resources
    in the PM, where the VM is implemented;
    \item Using VMs can greatly enhance server utilization and application flexibility;
    \item VMs can be colonized (replicated) in multiple servers for fault tolerance and disaster recovery;
    \item The size (number of nodes) of a virtual cluster can grow or shrink dynamically;
    \item the failure of any physical nodes may disable some VMs installed on the failing
    nodes but the failure of VMs will not pull down the host system.
}
\sep{Cloud Scalability}
\emogood There are two fundamental issues on cloud performance: \tbf{Scalability} and \tbf{Availability}.
The total execution time of the program is calculated by $\alpha \bT+(1-\alpha)\bT/n$.
\thm{Amdahl's Law}{
The total execution time of the program is calculated by $\alpha \bT+(1-\alpha)\bT/n$.
\eq{
Speedup\ factor=S=T/[\alpha T+(1-\alpha)T/n]=1/[\alpha+(1-\alpha)/n]
}
\tab{
    \item The communication time and I/O time are excluded in this formula;
    \item When $n\to\infty$, the $S$ can have the upper bound $\frac{1}{\alpha}$, thus $\alpha$ is the \tit{sequential bottleneck} here;
    \item This speedup is called \tit{fixed-workload speedup};
    \item The \tit{cluster efficiency} is defined by $E=S/n=\frac{1}{\alpha n+1-\alpha}$; (
    efficiency means that one more cluster can reduce how much time to process
    )
    \item Large sequential bottleneck would lead to many idle servers in the cluster;
}
}
\thm{Gustafson's Law}{
By fixing the parallel execution time at level $W$:
\eq{S^{\prime}=W^{\prime}/W=[\alpha W+(1-\alpha)nW]/W=\alpha+(1-\alpha)n}
\tab{
\item The efficiency is obtained by: $E^{\prime}=S^{\prime}/n=\alpha/n+(1-\alpha)$, this means one more cluster
can increase how much workload;
\item For a fixed workload, use Amdahl's Law; for a scaled problem, apply Gustafson's Law.
}
}
\sep{Cloud Availability}
\thm{Cluster Availability}{
    \eq{
        \text{Clusater Availability}=MTTF/(MTTF+MTTR)
    }
    \tab{
        \item \tit{MTTF}: mean time to failure;
        \item \tit{MTTR}: mean time to repair.
    }
}
\ex{
    There is a double-redundancy cluster, the MTTF is 200 units while the MTTR is 5 units, calculate the availability of this system.
}{
    The availability of each server is $200/(200+5)=97.5\%$, the failure rate of the whole system is $1-(1-97.5\%)^2=0.625\%$
}
Consider the use of a cluster of $n$ homogeneous servers in a system, the system is available when more than $k$ machine is running,
then the system availability can be expressed by:
\eq{
    \begin{aligned}
        \text{A} & =\sum_{i=k}^{n}\left(\begin{array}{c}n\\i\end{array}\right)p^{i}\left(1-p\right)^{n-i}                                                               \\
                 & =\left(\begin{array}{c}n\\k\end{array}\right)p^k\left(1-p\right)^{n-k}+\left(\begin{array}{c}n\\k+1\end{array}\right)p^{k+1}\left(1-p\right)^{n-k-1} \\
                 & +\cdots+\left(\begin{array}{c}n\\n-1\end{array}\right)p^{n-1}\left(1-p\right)^1+\left(\begin{array}{c}n\\n\end{array}\right)p^n\left(1-p\right)^0,
    \end{aligned}
}
\sep{CPU and GPU}
\fig{CPU and GPU Architecture}{CPU and GPU Architecture}{}
\tab{
    \item CPU has high flexibility for different applications. \tit{Von Neumann bottleneck}: memory access is slow compared to calculation.
    \item GPU has high throughput, it works well on applications with \tit{massive parallelism}.
}

\subsection{Virtual Machines}
A VM is essentially built as a software package that can be loaded into a host computer to
execute certain user applications. Once the jobs are done, the VM package can be removed from the host computer.
The host acts like a “hotel” to accommodate different “guests” at different timeframes. \\
\tbf{VMM} (virtual machine monitor) can be used to build virtual machine for the guest OS, it can have the following operations:
\fig{VMM Operations}{VMM Operations}{}
There are five levels of virtualization, among which only 2 are valuable:
\tab{
    \item \tbf{Hypervisor}: virtualization on top of bare-metal hardware, an example is XEN.
    \item \tbf{Container}: virtualization on operating system level, isolated containers of user app with isolated resources.
}
\re{
    \tbf{Difference between hypervisor and container}\\
    \lis{
        \item \tit{Virtualization Level}: hardware-level virtualization vs. operating system-level virtualization;
        \item \tit{Resource Isolation}: strong isolation between VMs vs. lighter resource isolation;
        \item \tit{Performance, Startup time, Management}: high vs. low;
    }
}
\tbf{Unikernel}: combines the advantages of hypervisor and container. Don't rely on a host OS, don't need guest OS for every VM.
High performance and start quickly.
\fig{virtual machine architecture}{virtual machine architecture}{}
\subsection{Hadoop and Spark}
\lis{
    \item \tbf{MapReduce}: is a parrallel programming model designed by Google;
    \item \tbf{Haddop}: is a parallel programming platform (library) to implement the MapReduce model;
    \item \tbf{Spark}: is a computing engine for big data supporting more operations than MapReduce.
}
\sep{MapReduce Model}
\fig{MapReduce Model}{MapReduce Model}{50}
Steps of MapReduce:
\lis{
    \item \tbf{Data Partitioning}: split the input data files into multiple \tit{blocks};
    \item \tbf{Fork Out} the user pogram to masters (nne copy of the user program runs on the master mode);
    \item Assign \tbf{Map Function} and \tbf{Reduce Function};
    \item Read the partitioned data blocks into map workers;
    \item Perform the Map Function;
    \item \tbf{Sort and Group} intermediate key-value pairs before forwarding them to reduce workers;
    \item Perform the reduction function using the grouped pairs;
    \item Output the results.
}
\fig{Word Count Using MapReduce}{Word Count Using MapReduce}{0.4}
For an $n\times n$ matrix multiplication, the complexity is $O(n^3)$, using $N$ clusters, the ideal speedup is $N$ and the maximum speed up is $n^2$.
\fig{MapReduce for Parallel Matrix Multiplication}{MapReduce for Parallel Matrix Multiplication}{0.4}
Different types of ReducForm:
\lis{
    \item Map-only: map() $\to$ output: document conversion, brute force searches;
    \item Classic MapReduce: map() $\to$ reduce() $\to$ output: distributed search and sort;
    \item Iterative MapReduce: some advanced linear algebra and machine learning.
}
\sep{Benchmark for Distributed System}
\tbf{1.} PageRank Algorithm with MapReduce: PageRank is an iterative algorithm used to calculate the importance of web pages. At time $0$, the PR
score for every page is $1$ or $\frac{1}{N}$ by default. At time $t$, the score is given by:
\eq{
    PR(p_i;t+1)=\frac{1-d}N+d\sum_{p_j\in M(p_i)}\frac{PR(p_j;t)}{L(p_j)}.
}
And the matrix notation is $\mathbf{R}(t+1)=d\mathcal{M}\mathbf{R}(t)+\frac{1-d}N\mathbf{1}.$
To use MapReduce model to solve a PageRank algorithm, we need an iterative MapReduce model, in which multiple reducers are processed.
\fig{PageRank using MapReduce}{PageRank using MapReduce}{0.8}
\tbf{2.} Terasort: is a standard benchmark on Hadoop. It covers the following steps:
\lis{
    \item Input: random data generated by TeraGen on HDFS;
    \item Map: generate trie to ensure KV pair to be partitioned to specific reduce server;
    \item Shuffle & sort: sort the output from mapper on local disk of each mapper server;
    \item Reduce: local sort each reducer input and write results to HDFS;
    \item Record performance metrics such as sorting time and throughput;
    \item Compare the performance of different systems or configurations;
}
\tbf{3. } Hibench: is a benchmark toolkit that covers the following metrics: speed, HDFS bandwidth, utilization (CPU, memory, IO).
\sep{Apache Hadoop}
The hadoop architecture contains three components:
\tab{
    \item Hadoop Distributed File System (HDFS): store the data in multiple blocks;
    \item MapReduce Engine;
    \item YARN: process job requests and manage cluster resources.
}
Standing on a higher level, there is a master-slave pattern for every component, among which \tit{master} is the control node and
\tit{slave} is the working node.
\tab{
    \item HDFS: NameNode - DataNode;
    \item MapReduce: Jobtracker - Tasktracker;
    \item YARN: ResourceManager - NodeManager.
}
\sep{Apache Spark}
Two main abstractions of Spark:
\lis{
    \item \tbf{RDD}: Resilient Distributed Datasets, a data structure that is failure-tolerant. Two kind
    of modification: \tit{transformation}: turn a RDD into a new RDD; \tit{action}: compute a RDD and return a result;
    \item \tbf{DAG}: A scheduler that enables in-memory-computation.
}
\fig{Spark Architecture}{Spark Architecture}{0.3}
The difference between Spark and Hadoop:
\tab{
    \item Speed: Spark usually has a higher speed;
    \item Data Processing Models: Hadoop uses the MapReduce model for batch processing. Spark supports batch processing (similar to MapReduce),
    real-time streaming processing (Spark Streaming), interactive querying (Spark SQL);
    \item Data Caching: Spark allows data to be cached in memory, Hadoop relies on intermediate disk.
}
\fig{Hadoop vs Spark}{Hadoop vs Spark}{0.3}

\clearpage
\section{Machine Learning}

\clearpage
\section{Deep Learning}

\clearpage
\section{Causal Inference with Machine Learning}

\subsection{Foundations of Causal Inference}

\cite{peters2017elements} provide an overview of causal inference from a fundamental science perspective. \cite{spirtes2010introduction}
summarize the development of causal inference and machine learning interface from a computer science perspective.\\
There are two main causal frameworks: \tbf{Structural Causal Model} and \tbf{Potential Outcome Framework}.

\subsubsection{Structural Causal Model}
\tbf{SCM} (Structural Causal Model) can be divided into two subgroups: \tit{causal graph} and \tit{structural euqation}.

\defi{}{}


\clearpage
\section{Reinforcement Learning}

This section is mainly the notes of \cite{thrun2000reinforcement} and \cite{agarwal2019reinforcement}.

\subsection{Introduction and MDP}

\key{\tab{
        \item Exploration, Exploitation, Reward, Policy, Value Function;
        \item
    }}

\no The four elements of reinforcement learning:
\lis{
    \item \textit{Policy}: agent's way to interact with the environment;
    \item \textit{Reward Signal}: on each time step, the environment sends to the agent a single number;
    \item \textit{Value Function}: specify what is good in the long run, which is the discount value of the rewards;
    \item \textit{Model}: mimics behaviors of the environment;
}

\re{"Deadly Trials"\lis{
        \item The balance between \textbf{Exploration} and \textbf{Exploitation};
        \item Reinforcement learning is difficult to generalize;
        \item Delayed consequences may cause RL algorithm to perform poorly.
    }}

\no \textbf{MDP} is a mathematical framework to model \tit{discrete-time} sequential decision process, denoted by the tuple
$(\cS,\cA,\cT,\cR,\rho_0,\gamma)$:
\tab{
    \item $\cS$: the state space, which is the states for the \tbf{entire} environment(MOBA games may can not directly be modeled by MDP);
    \item $\cA$: the action space. $\cA$ can depend on $s \in \cS$;
    \item $\cT:\cS\times\cA\to\Delta(\cS)$:the environment transition probability function;
    \item $\cR:\cS\times\cA\to\Delta(\RR)$: the reward function;
    \item $\rho_0\in\Delta(\cS)$: the initial state distribution;
    \item $\gamma \in [0,1]$ is the discount factor.
}
The goal is to optimize:
\eq{
    \mathbb{E}_{s_t,a_t,r_t,t\geq0}\left[R_0\right]=\mathbb{E}_{s_t,a_t,r_t,t\geq0}\big[\sum_{t=0}^{\infty}\gamma^tr_t\big]
}
\re{
    \tab{
        \item The $\Delta(\cdot)$ may not be deterministic, but some random distribution;
        \item Among the above tuple, $\cS,\cT,\cR,\rho_0$ can not be modified by the agent, to train a good policy, $\cA,\gamma$ is the key;
        \item RL is more like infants rather than adults;
        \item The reward function is the way of communicating with the agent \tit{what} to do, not \tit{how} to do;
        \item The \tit{trajectory} of the MDP sequence: $S_0,A_0,R_1,S_1,A_1,\cdots$.
    }
}
\fig{agent-environment interaction}{The agent-environment interaction in a Markov decision process}{0.3}
\thm{Dynamics of MDP}{\meq{
p(s',r|s,a)\doteq\Pr\{S_t=s',R_t=r\mid S_{t-1}=s,A_{t-1}=a\}\\
\sum_{s^{\prime}\in\mathcal{S}}\sum_{r\in\mathcal{R}}p(s^{\prime},r|s,a)=1,\text{ for all }s\in\mathcal{S},a\in\mathcal{A}(s)
}}
\co{Some formula derived from the dynamics theorem}{
\eq{
p(s'|s,a)\doteq\Pr\{S_t=s'\mid S_{t-1}=s,A_{t-1}=a\}=\sum_{r\in\mathbb{R}}p(s',r|s,a)
}
\eq{
r(s,a)\doteq\mathbb{E}[R_t\mid S_{t-1}=s,A_{t-1}=a]~=~\sum_{r\in\mathbb{R}}r\sum_{s'\in\mathcal{S}}p(s',r|s,a)
}
\eq{
r(s,a,s')~\doteq~\mathbb{E}[R_t\mid S_{t-1}=s,A_{t-1}=a,S_t=s']~=~\sum_{r\in\mathbb{R}}r\frac{p(s',r|s,a)}{p(s'|s,a)}
}
}
\defi{Some useful function:}{
    The act value function given policy $\pi$:
    \eq{Q^{\pi}(s,a)=\mathbb{E}_{s_t,a_t,r_t,t\geq0}\big[\sum_{t=0}^{\infty}\gamma^tr(s_t,a_t)\mid s_0=s,a_0=a\big]}
    The expected return at state $s$ given policy $\pi$:
    \eq{V^\pi(s)=\mathbb{E}_{a\sim\pi(s)}\left[Q^\pi(s,a)\right]}
    The \tbf{advantage function}:
    \eq{A^\pi(s,a)=Q^\pi(s,a)-V^\pi(s)}
    The temporal-difference error:
    \eq{\delta_t=r_t+\gamma V(s_{t+1})-V(s_t)}
}
If we denote $G_t=R_{t+1}+\gamma G_{t+1}$, then we have:
\thm{Bellman Equation}{
    \eq{
        \begin{aligned}
            v_{\pi}(s) & \doteq\mathbb{E}_\pi[G_t\mid S_t=s]                                                                                   \\
                       & =\mathbb{E}_\pi[R_{t+1}+\gamma G_{t+1}\mid S_t=s]                                                                     \\
                       & =\sum_a\pi(a|s)\sum_{s^{\prime}}\sum_rp(s^{\prime},r|s,a)\Big[r+\gamma\mathbb{E}_\pi[G_{t+1}|S_{t+1}=s^{\prime}]\Big] \\
                       & =\sum\pi(a|s)\sum p(s^{\prime},r|s,a)\Big[r+\gamma v_\pi(s^{\prime})\Big],\quad\text{for all }s\in\mathcal{S},
        \end{aligned}
    }
    If we consider a $n$-state game, assume the reward $r\in \RR^n$ is deterministic. Then we denote $P$ as the transition matrix
    for a corresponding strategy and $V\in\RR^n$ as the value function vector. Finally, we can get the Bellman equation in matrix form:
    eq{
            V=r+\gamma PV.
        }
}
A policy $\pi$ is better $\pi'$ if and only if $v_{\pi}(s)\le v_{\pi'}(S) \text{for all } s \in \cS$. There is always at list one
\tit{optimal policy} denoted by $\pi_*$ (doesn't hold for partially observed MDP). They share the same state-value function, called the \tit{optimal state-value function}:
$v_*(s)\doteq\max_\pi v_\pi(s)$. The optimal policy also shares the same \tit{optimal action-value function}:
$q_*(s,a)\doteq\max_{\pi}q_\pi(s,a)=\mathbb{E}[R_{t+1}+\gamma v_*(S_{t+1})\mid S_t=s,A_t=a]$. Then we can get the \tbf{Bellman optimality function}
in an action-value function sense:
\thm{Bellman optimality function}{
    \eq{
        \begin{gathered}
            q_{*}(s,a) \begin{array}{rl}{=}&{\mathbb{E}\Big[R_{t+1}+\gamma\max_{a^{\prime}}q_{*}(S_{t+1},a^{\prime})\Big|S_{t}=s,A_{t}=a\Big]}\end{array} \\
            =\quad\sum_{s^{\prime}.r}p(s^{\prime},r|s,a)\Big[r+\gamma\operatorname*{max}_{a^{\prime}}q_{*}(s^{\prime},a^{\prime})\Big].
        \end{gathered}
    }
}
\fig{different reinforcement learning agents}{Classification of different reinforcement learning agents}{0.25}
\defi{The optimal policy $\pi^*$}{A policy $\pi^*$ is an optimal policy if for every policy $\pi$ and every $s\in\cS$, we have:
    \eq{V^{\pi^*}(s)\geq V^{\pi}(s)}
    \re{
        \tab{
            \item If we have the full information of the game (MDP framework), then the optimal policy is always deterministic;
            \item The optimal policy is always stochastic when there are \tbf{minimax} structure (e.g. protection information);
            \item Whether the optimal policy is stochastic or deterministic has nothing to do with the stochasticity of the game.
        }
    }}

\subsection{Bandit Algorithms}

The regret for bandit games is defined as :
\eq{
    \overline{R}_t=\sum_{i=1}^m\mathbb{E}[N_{t,i}]\Delta_i
}
Where $N_{t,i}=\sum_{t^{\prime}=0}^{t}\mathbb{1}\{a_{t^{\prime}}=i\}$ and $\Delta_{i}=\mu^{*}-\mu_{i}$.\\
\sep{Greedy Algorithms}
\fig{the greedy algorithm}{The greedy algorithm}{}
This algorithm achieves a regret at most $O(T)$.\\
\sep{The $\epsilon$-greedy algorithm}
\fig{The epsilon greedy algorithm}{The epsilon greedy algorithm}{}
The lower bound of $\epsilon$ greedy: $\overline{R}_t\geq\frac1m(\Delta_2+\cdots+\Delta_m)\varepsilon(T-m)$, where $\epsilon\le \epsilon_t$ for all $t$;
\thm{The upper bound of $\epsilon$ greedy}{
    \overline{R}_T\leq C'\sum_{i\geq2}\left(\Delta_i+\frac{\Delta_i}{\Delta_{\mathrm{min}}^2}\log\max\left\{e,\frac{T\Delta_{\mathrm{min}}^2}m\right\}\right)
    \proo{}{
        This proof contains two parts: part 1 is about the cost of exploration, and part 2 is about the suboptimal condition during exploitation.\\
        First, the exploration: $\overline{R}_t=\frac1m(\Delta_2+\cdots+\Delta_n)\epsilon$, by setting $\epsilon_t$ = $\frac{1}{t}$: $\overline{R}_t=\frac1m(\Delta_2+\cdots+\Delta_m)O(\log T)$.\\
        Then prove the probability of selecting the suboptimal arms is very thin.
    }
}
\sep{The explore-then-commit algorithm (ETC)}
\fig{The ETC algorithm}{The ETC algorithm}{}
This algorithm has an upper bound of $O(\Delta^2 log T)$, or $O(T^{\frac{2}{3}})$.
\sep{The UCB algorithm}
\fig{UCB Algorithm}{UCB Algorithm}{0.5}
The UCB is based on the principle of optimism in the face of uncertainty, which states that:
\begin{center}
    \tbf{One should act as if the environment is as nice as plausibly possible.}
\end{center}
The UCB estimate for each arm is an over-estimate compared with the empirical mean $\hat{\mu}_{i,t-1}=\frac{1}{N_{i,t-1}}\sum_{t'\le t-1}r_{t'}\mathbb{1}\{a_{t'}=i\},$ recall the Chernoff-Hoeffding bound:
\eq{
    \mathbb{P}(\overline{X}-\mathbb{E}[\overline{X}]\leq z)\geq1-\exp(-nz^2/2).
}
Repalcing $z$ to $\sqrt{\frac{2\log(1/\delta)}{N_{i,t-1}}}$, we would get:
\eq{
    \mathbb{P}(\mu_i\geq\hat{\mu}_{i,t-1}+\sqrt{\frac{2\log(1/\delta)}{N_{i,t-1}}})\leq\delta.
}
\sep{Thompson Sampling Algorithm}
Assume that the reward distributions of different arms belong to the same family with respective parameters:
\eq{
    r(i)\sim p(x\mid\theta_i).
}
When estimating $\theta$, the posterior is:
\eq{
    p(\theta\mid x)=\frac{p(x\mid\theta)p(\theta)}{\int_{\theta'}p(x\mid\theta')p(\theta')d\theta'}.
}
If the posterior $p(\theta|x)$ is in the same probability distribution family as the prior $p(\theta)$, then they are called conjugate distributions.
The Bernoulli-Beta is important for Bernoulli Bandits, for $\theta=\{\alpha,\beta\}$:
\eq{
p(\theta)=\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}x^{\alpha-1}(1-x)^{\beta-1},
}
where $\Gamma(z)=\int_{0}^{\infty}x^{z-1}\exp(-x)dx$ is the Gamma function. So when $p(\theta)\sim\mathrm{Beta}(\alpha_{0},\beta_{0})$ and
observing $x_1,\ldots,x_{\alpha^{\prime}+\beta^{\prime}}\sim x\mathrm{~i.i.d.}$, with $\alpha^\prime$ ones and $\beta^\prime$ zeros.
\eq{
    \begin{aligned}
        p(\theta\mid x_{1},\ldots,x_{\alpha^{\prime}+\beta^{\prime}}) & =\frac{p(x_1,\ldots,x_{\alpha^{\prime}+\beta^{\prime}}\mid\theta)p(\theta)}{\int_{\theta^{\prime}}p(x_1,\ldots,x_{\alpha^{\prime}+\beta^{\prime}}\mid\theta^{\prime})p(\theta^{\prime})d\theta^{\prime}}                                                 \\
                                                                      & =\frac{\binom{\alpha'+\beta'}{\alpha'}\theta^{\alpha'}(1-\theta)^{\beta'}\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}\theta^{\alpha_0-1}(1-\theta)^{\beta_0-1}}{\int_{\theta'}p(x_1,\ldots,x_{\alpha'+\beta'}\mid\theta')p(\theta')d\theta'} \\
                                                                      & =\frac{\binom{\alpha'+\beta'}{\alpha'}\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}}{\int_{\theta'}p(x_1,\ldots,x_{\alpha'+\beta'}\mid\theta')p(\theta')d\theta'}\theta^{\alpha_0+\alpha'-1}(1-\theta)^{\beta_0+\alpha'-1}                    \\
                                                                      & \sim\mathrm{Beta}(\alpha_0+\alpha^{\prime},\beta_0+\beta^{\prime}).
    \end{aligned}
}
\fig{Thompson sampling for Bernoulli bandits}{Thompson sampling for Bernoulli bandits}{0.5}
TS algorithm can work on any conjugate distributions.
\fig{General Thompson Sampling}{General Thompson Sampling}{0.5}

\subsubsection{Analysis of Bandit Algorithms}
\sep{ETC Algorithm}
\thm{ETC Regret}{
    Assume $r(i)$ is \tit{1-sub-Gaussian} for each $i$, the regret satisfies:
    \eq{
        \overline{R}_T\leq k\sum_{i\in[m]}\Delta_i+(T-mk)\sum_{i\in[m]}\Delta_i\exp\left(-\frac{k\Delta_i^2}{4}\right).
    }
    When $m=2$, taking $k=\lceil\operatorname*{max}\begin{Bmatrix}1,4\Delta_{2}^{-2}\log(T\Delta_{2}^{2}/4)\end{Bmatrix}\rceil$ yields:
    \eq{
        k=\lceil\operatorname*{max}\begin{Bmatrix}1,4\Delta_{2}^{-2}\log(T\Delta_{2}^{2}/4)\end{Bmatrix}\rceil
    }
}
\proo{}{
    \eq{
        \begin{aligned}
            \mathbb{E}\left[N_{T,i}\right] & =k+(T-mk)\mathbb{P}\big(i=\underset{i'}{\operatorname*{\arg\max}}\hat{\mu}_{mk-1,i^{\prime}}\big)           \\
                                           & \leq k+(T-mk)\mathbb{P}(\hat{\mu}_{mk-1,i}\geq\hat{\mu}_{mk-1,1})                                           \\
                                           & =k+(T-mk)\mathbb{P}\big(\hat{\mu}_{mk-1,i}-\mu_{i}-\big(\hat{\mu}_{mk-1,1}-\mu_{1}\big)\geq\Delta_{i}\big).
        \end{aligned}
    }
    By the sub-Gaussian tail bound:
    \eq{
        \mathbb{P}\left(\hat{\mu}_{mk-1,i}-\mu_i-(\hat{\mu}_{mk-1,1}-\mu_1)\geq\Delta_i\right)\leq\exp\left(-\frac{k\Delta_i^2}{4}\right).
    }
    Therefore,
    \eq{
        \begin{aligned}
            \overline{R}_{T} & =\sum_{i=1}^{m}\mathbb{E}\left[N_{T,i}\right]\Delta_{i}                                                                                                                      \\
                             & \begin{aligned}&\leq\sum_{i=1}^{m}\Delta_{i}\left(k+(T-mk)\right.\mathbb{P}\big(\hat{\mu}_{mk-1,i}-\mu_{i}-(\hat{\mu}_{mk-1,1}-\mu_{1})\geq\Delta_{i}\big)\big)\end{aligned} \\
                             & \leq\sum_{i=1}^{m}\Delta_{i}\left(k+(T-mk)\exp\left(-\frac{k\Delta_{i}^{2}}{4}\right)\right).
        \end{aligned}
    }
    By setting $m=2$, we can derive:
    \eq{
        \begin{aligned}
            \overline{R}_{T} & \leq  \Delta_{2}\left(k+(T-mk)\exp\left(-\frac{k\Delta_{2}^{2}}{4}\right)\right) \\
                             & \leq \Delta_{2}\left(k+T\exp\left(-\frac{k\Delta_{2}^{2}}{4}\right)\right).
        \end{aligned}
    }
    To minimize the regret, take a derivative on $k$ and use the first-order condition. When $k=\lceil\operatorname*{max}\left\{1,4\Delta_{2}^{-2}\log(T\Delta_{2}^{2}/4)\right\}\rceil $:
    \eq{
        \begin{aligned}
            \overline{R}_{T} & \leq\Delta_{2}\left(k+T\exp\left(-\frac{k\Delta_{2}^{2}}{4}\right)\right)                                                                                                                                               \\
                             & \leq\Delta_{2}\left(k_{0}+1+T\exp\left(-\frac{k_{0}\Delta_{2}^{2}}{4}\right)\right)                                                                                                                                     \\
                             & \leq\Delta_{2}\left(\frac{4}{\Delta_{2}^{2}}\log\left(\frac{T\Delta_{2}^{2}}{4}\right)+1+T\exp\left(-\frac{\Delta_{2}^{2}}{4}\cdot\frac{4}{\Delta_{2}^{2}}\cdot\log\left(\frac{T\Delta_{2}^{2}}{4}\right)\right)\right) \\
                             & \leq\Delta_{2}\left(\frac{4}{\Delta_{2}^{2}}\log\left(\frac{T\Delta_{2}^{2}}{4}\right)+1+T\cdot\frac{4}{T\Delta_{2}^{2}}\right)                                                                                         \\
                             & \leq\Delta_2+\frac{4}{\Delta_2}+\frac{4}{\Delta_2}\log\left(\frac{T\Delta_2^2}{4}\right).
        \end{aligned}
    }
}
\sep{Recap of Different Algorithms}
\re{
For $\epsilon$-greedy, by choosing $\varepsilon_{t}=\min\{1,Ct^{-1}\Delta_{\mathrm{min}}^{-2}m\}$:
\eq{
    \overline{R}_T\le C'\sum_{i\ge2}\left(\Delta_i+\frac{\Delta_i}{\Delta_{\text{min}}^2}\log\max\left\{e,\frac{T\Delta_{\text{min}}^2}{m}\right\}\right)
}
For ETC under 2-armed bandits, when $T\geq4\sqrt{2\pi e}/\Delta^{2}$, by choosing $k=\lceil\frac{2}{\Delta^{2}}W(\frac{T^{2}\Delta^{4}}{32\pi})\rceil,$
\eq{
    \overline{R}_T\leq O(\frac{1}{\Delta}\log T\Delta^2)+o(\log T)+\Delta。
}
For the two algorithms listed above, to achieve the optimal regret. the information about $\Delta$ is necessary. UCB and TS don't require knowing $\Delta$.\\
For UCB algorithm, by setting $\delta=T^{-2}$,
\eq{
    \overline{R}_{T}\leq3\sum_{i=1}^{m}\Delta_{i}+\sum_{i:\Delta_{i}>0}\frac{16\log T}{\Delta_{i}}.
}
For TS (Bernoulli bandits):
\eq{
    \overline{R}_{T}\le\sum_{i:\Delta_{i}>0}\frac{\mu_{1}-\mu_{i}}{d_{\mathrm{KL}}(\mu_{1}\parallel\mu_{i})}\log T+o(\log T).
}
}

\subsection{Model-Based RL}

There are two streams of reinforcement learning algorithms, \tit{model-based} v.s. \tit{model-free} methods. Model-based algorithms require the knowledge on the
environment(transition matrix $P$ and reward function $r$), or need to estimate $P,r$ during the computations. Model-free algorithms don't rely the estimation
of the environment.
\tbf{Policy Evaluation (Prediction)}\\
\eq{
    \begin{aligned}v_{k+1}(s)\quad&\doteq\quad\mathbb{E}_\pi[R_{t+1}+\gamma v_k(S_{t+1})\mid S_t=s]\\&=\quad\sum_a\pi(a|s)\sum_{s',r}p(s',r|s,a)\Big[r+\gamma v_k(s')\Big]\end{aligned}
}
\fig{Iterative Policy Evaluation}{Iterative Policy Evaluation}{0.5}
The policy evaluation would terminate after finite steps. For any $\epsilon\in(0,\frac{1}{1-\gamma})$, after at least $N\ge\frac{\gamma}{(1-\gamma)^4}\frac{n^2m\log(cnm/\delta)}{\varepsilon^2}$
steps, with at least probability $1-\delta$:
\eq{
    \|P(\cdot\mid s,a)-\hat{P}(\cdot\mid s,a)\|_1\leq(1-\gamma)^2\varepsilon.
}
\thm{Policy Improvement Theorem}{
    For all $s\in\cS$, if:
    \eq{q_\pi(s,\pi^{\prime}(s))\geq v_\pi(s)}
    Then the policy $\pi^{\prime}$ must be better than policy $\pi$.
}
\tbf{Policy Improvement}
\eq{
    \begin{aligned}
        \pi^{\prime}(s) & \begin{aligned}\dot{=}\quad\arg\max_aq_\pi(s,a)\end{aligned}                                                                                        \\
                        & \begin{aligned}=\quad\underset{a}{\operatorname*{argmax}}\mathbb{E}[R_{t+1}+\gamma v_{\pi}(S_{t+1})\mid S_{t}=s,A_{t}=a]\end{aligned}              \\
                        & \begin{aligned}=\quad\arg\max_a\sum_{s^{\prime},r}p(s^{\prime},r|s,a)\Big[r+\gamma v_\pi(s^{\prime})\Big],\end{aligned}
    \end{aligned}
}
\sep{Policy Iteration and Value Iteration}
\fig{Policy Iteration}{Policy Iteration}{0.5}
\fig{Value Iteration}{Value Iteration}{0.5}
Differences between VI and PI:
\re{
    \tab{
        \item In each sweep, VI only updates one-step evaluation and one-step improvement, PI updates
        multiple-step evaluation and one-step improvement;
        \item PI takes fewer rounds but takes more time within each round.
    }
}
\fig{Generalized Policy Iteration}{Generalized Policy Iteration}{0.25}
\sep{Episodic Discrete MDP}
In the episodic setting, in every episode, the learner acts for $H$ steps, starting from a fixed starting state $s_0\sim\rhp_0$, and the process repeats for $K$ episodes.
The agent's goal is to minimize the expected regret:
\eq{
    \overline{R}_K=\mathbb{E}\left[KV^*(s_0)-\sum_{k=0}^{K-1}\sum_{h=0}^{H-1}r(s_h^k,a_h^k)\right].
}

\subsubsection{UCVI: Upper Confidence bound Value Iteration}
In MDP, the exploration is more difficult compared to bandit games, because we can only decide the action rather than navigate to a
particular state which is barely reached before. Using the ideas in UCB from bandit games, we can give an overestimation of value functions
for those states that were barely reached before. By value iteration and policy iteration, this overestimation can navigate the
agent to explore the strange states.
\fig{UCVI}{UCVI}{0.9}
UCVI is an extremely important algorithm. Taking confidence level $\delta=1/KH$, the regrest can achieve a performance of $\overline{R}_T\leq10\sqrt{n^2mH^4K\log(nmH^2K^2)}.$

\subsubsection{PSRL: Posterior Sampling for Reinforcement Learning}
\fig{PSRL}{PSRL}{0.4}
The regret of PSRL: $\overline{R}_T\leq\sqrt{30n^2mH^3K\log(nmHK)}.$

\subsection{Model-free RL}

The model-free framework is important in two types of domains:
\lis{
    \item When the MDP model is unknown, but we can sample trajectories from the MDP;
    \item When the MDP model is known but computing the value function via our model-based control methods is infeasible due to the size of the domain.
}
\subsubsection{Q-Learning}
Recall the model-based value iteration, the model appears in two places. One for the computation of the action value function,
and one for computing all the optimal policies $\max_{a\in A}\left[R(s,a)+\gamma\sum_{s'\in S}P(s'\mid s,a)V(s')\right]$. Both
places could remove the dependency on $P$ by using Q-function $Q(s,a)$ directly.\\
Another feature of Q-learning is the \tit{step size} $\alpha$. In Q-learning, instead of utilizing the Bellman optimality equation directly,
the update only takes $\alpha$ portion of the action value.
\fig{Q-learning}{Q-learning}{0.5}
Where $\delta_t=r+\gamma\max_{a\in\mathcal{A}}Q(s',a')-Q(s,a)$ is known as the \tbf{temporal difference} error (TD).\\
One problem with Q-learning is that the trajectory sampled is subject to the current policy, which lacks exploration. One simple approach is
using $\epsilon$ greedy exploration.
\fig{Q-learning with greedy exploration}{Q-learning with $\epsilon$-greedy exploration}{0.45}
In reinforcement learning algorithms, we tend to overestimate the value function of the states, this problem comes from the fact that we are using our estimate to both choose the
better coin and estimate its value. Consider the state $s$ with two possible actions $a_1, a_2$, and $Q(s,a_1),Q(s,a_2)=0$. Then we have:
\eq{
    \begin{aligned}
        \hat{V}(s) & =\mathbb{E}[\max(\hat{Q}(s,a_1),\hat{Q}(s,a_2))]                \\
                   & \geq\max(\mathbb{E}[\hat{Q}(s,a_1)],\mathbb{E}[\hat{Q}(s,a_2)]) \\
                   & =\max(0,0)                                                      \\
                   & =0=V^{*}(s)\:.
    \end{aligned}
}
Where the inequality is followed by Jensen's inequality.\\
In double Q-learning, we can maintain two independent unbiased estimates, $Q_1$ and $Q_2$ and when we use one to select the maximum, we can use the other to get an estimate of the value of this maximum.
\fig{Double Q-Learning}{Double Q-Learning}{0.5}

\subsubsection{Monte-Carlo Policy Evaluation}
The idea of Monte-Carlo is quite simple, run the game with the policy $\pi$ for many iterations, and average all the rewards to approximate
the value function.
\fig{Monte-Carlo policy evaluation}{Monte-Carlo policy evaluation}{0.5}
Some variations of Monte-Carlo policy evaluation:
\tab{
\item The body of the algorithm can remove $S$ by using $V^{\pi}(s_{j,t})\leftarrow V^{\pi}(s_{j,t})+\frac{1}{N(s_{j,t})}(G_{j,t}-V^{\pi}(s_{j,t})).$;
\item The algorithm only counts the first visit in every episode, there is another \tit{every-visit} version that doesn't use this condition;
\item Similar to Q-learning, there is another incremental updating scheme $V^{\pi}(s_{j,t})\leftarrow V^{\pi}(s)+\alpha(G_{j,t}-V^{\pi}(s))$.
}
\sep{Importance Sampling: Off-Policy Policy Evaluation}
The goal of importance sampling is to estimate the expected value of a function $f(x)$ when $x$ is drawn from the distribution $q$ using only the data $f(x_1)\cdots f(x_n)$,
where $x_i$ are drawn from a different distribution $p$. To estimate $\mathbb{E}_{x\sim q}[f(x)]$:
\eq{
    \begin{aligned}
        \mathbb{E}_{x\sim q}[f(x)] & \begin{aligned}=\int_xq(x)f(x)dx\end{aligned}                \\
                                   & =\int_xp(x)\left[\frac{q(x)}{p(x)}f(x)\right]dx              \\
                                   & =\mathbb{E}_{x\sim p}\left[\frac{q(x)}{p(x)}f(x)\right]      \\
                                   & \approx\sum_{i=1}^n\left[\frac{q(x_i)}{p(x_i)}f(x_i)\right].
    \end{aligned}
}
Now we bring the context back to reinforcement learning. To estimate $V^{\pi_1}(s)=\mathbb{E}[G_t\mid s_t=s]$,
using $n$ trajectories $h_1,\cdots,h_n$ generated by $\pi_2$, the importance sampling estimate can give us:
\eq{
    V^{\pi_1}\left(s\right)\approx\frac1n\sum_{j=1}^n\frac{\mathbb{P}(h_j\mid\pi_1,s)}{\mathbb{P}(h_j\mid\pi_2,s)}G(h_j),
}
where $G(h_j)=\sum_{t=1}^{L_j-1}\gamma^{t-1}r_{j,t}$ is the total discounted reward for each trajectory. We have:
\eq{
    \begin{aligned}
        \begin{aligned}\mathbb{P}(h_j\mid\pi,s=s_{j,1})\end{aligned} & \begin{aligned}&=\prod_{t=1}^{L_j-1}\mathbb{P}(a_{j,t}\mid s_{j,t})\mathbb{P}(r_{j,t}\mid s_{j,t},a_{j,t})\mathbb{P}(s_{j,t+1}\mid s_{j,t},a_{j,t})\end{aligned} \\
                                                                     & \begin{aligned}&=\prod_{t=1}^{L_j-1}\pi(a_{j,t}\mid s_{j,t})\mathbb{P}(r_{j,t}\mid s_{j,t},a_{j,t})\mathbb{P}(s_{j,t+1}\mid s_{j,t},a_{j,t}),\end{aligned}
    \end{aligned}
}
This equation needs information of the model ($P,r$), but by the operation of division, we have:
\eq{
    \begin{aligned}
        V^{\pi_1}(s) & \begin{aligned}&\approx\frac1n\sum_{j=1}^n\frac{\mathbb{P}(h_j\mid\pi_1,s)}{\mathbb{P}(h_j\mid\pi_2,s)}G(h_j)\end{aligned}                                                                                                                                     \\
                     & \begin{aligned}&=\frac1n\sum_{j=1}^n\frac{\prod_{t=1}^{L_j-1}\pi_1(a_{j,t}\mid s_{j,t})\mathbb{P}(r_{j,t}\mid s_{j,t},a_{j,t})\mathbb{P}(s_{j,t+1}\mid s_{j,t},a_{j,t})}{\prod_{t=1}^{L_j-1}\pi_2(a_{j,t}\mid s_{j,t})\mathbb{P}(r_{j,t}\mid s_{j,t},a_{j,t})\mathbb{P}(s_{j,t+1}\mid s_{j,t},a_{j,t})}G(h_j)\end{aligned} \\
                     & =\frac1n\sum_{j=1}^nG(h_j)\prod_{t=1}^{L_j-1}\frac{\pi_1(a_{j,t}\mid s_{j,t})}{\pi_2(a_{j,t}\mid s_{j,t})}.
    \end{aligned}
}
Which maintains the off-policy policy evaluation as a model-free technique.
\sep{Temporal Difference Learning}
Replace the $G_t$ in incremental Monte-Carlo policy evaluation with $r_t+\gamma V^{\pi}\left(s_{t+1}\right),$ and we can get:
\fig{TD Learning}{TD Learning}{0.45}
where the term $\delta_t=r_t+\gamma V^\pi(s_{t+1})-V^\pi(s_t)$ is called \tit{temporal difference}.

\subsubsection{Monte-Carlo Control}
\defi{GLIE: Greedy in the limit of infinite exploration}{
    A policy $\pi$ is \tit{greedy in the limit of infinite exploration} if:
    \lis{
        \item $\lim\limits_{k\to\infty}N_k(s,a)\to\infty\text{with probability}1$;
        \item $\lim\limits_{k\to\infty}\pi_k(a\mid s)=\arg\max\limits_aQ(s,a)\text{with probability}1.$
    }
    One example of GLIE is an $\epsilon$-greedy policy where $\epsilon\to 0$.
}
\fig{Online Monte-Carlo control}{Online Monte-Carlo control}{0.4}
\fig{SARSA}{SARSA}{0.4}
SARSA is short for \tit{state action reward state action}. The difference between SARSA and Q-learning is that SARSA is
an on-policy algorithm, updating the Q-function using the current strategy, while Q-learning is an off-policy learning algorithm.

\subsection{Value Function Approximation}

The methodologies above try to estimate the value function or Q-function for every state and action, but these methods might not apply to those
scenarios with very large state and action spaces. A popular approach for this issue is via \tbf{value function approximation}(VFA).
\eq{
    V^{\pi}(s)\approx\hat{V}(s,\mathbf{w})\quad\mathrm{or}\quad Q^{\pi}(s,a)\approx\hat{Q}(s,a,\mathbf{w}),
}
where $\mathbf{w}$ refers to parameter or wights. Some function approximations are listed below:
\lis{
    \item Linear combinations of features;
    \item Neural networks;
    \item Decision trees.
}

\subsubsection{Linear feature representations}
In linear function representations, we use a feature vector to represent a state:
\eq{
    x(s)=(x_1(s),x_2(s),\ldots,x_d(s))^T,
}
where $d$ is the dimensionality of the feature space, the approximation can be expressed by:
\eq{
    \hat{V}(s,\mathbf{w})=x(s)^T\mathbf{w}=\sum_{j=1}^dx_j(s)\mathbf{w}_j.
}
Then we can define the loss function:
\meq{
J(\mathbf{w})=\mathbb{E}_{s\sim\rho^{\pi}(s)}\left[(V^{\pi}(s)-\hat{V}(s,\mathbf{w}))^2\right];\\
\rho^\pi(s)=\lim_{T\to\infty}\frac{\sum_{t=0}^T\gamma^t\mathbb{P}(s_t=s\mid\pi)}{\sum_{t=0}^T\gamma^t}.
}
where $\rho^\pi(s)$ denotes the cumulative probability under the policy $\pi$. There are several ways to solve this optimization problem:
\lis{
    \item Gradient Descent (SGD);
    \item Policy evaluation with linear VFA;
}
\fig{Monte-Carlo policy evaluation with linear VFA}{Monte-Carlo policy evaluation with linear VFA}{0.45}
The updating formula $\mathbf{w}\leftarrow\mathbf{w}+\alpha(\arg(R(s_t))-\hat{V}(s_t,\mathbf{w}))x(s_t)$ is derived by taking derivative
of $\mathbf{w}$ on $J(\mathbf{w})$:
\eq{
    \begin{aligned}
        \nabla J(\mathbf{w}) & = 2(V^{\pi}(s)-\hat{V}(s,\mathbf{w}))\frac{\partial (V^{\pi}(s)-\hat{V}(s,\mathbf{w}))}{\partial \mathbf{w}} \\
                             & = 2(V^{\pi}(s)-\hat{V}(s,\mathbf{w}))\cdot x(s).
    \end{aligned}
}
In different policy evaluation settings the term $V^{\pi}(s)-\hat{V}(s,\mathbf{w})$ could be replaced by different styles:
\tab{
\item Monte-Carlo: $\sum_{j=t}^{H_k}r_{k,j}-\hat{V}(s,\mathbf{w})$;
\item Temporal-difference: $r+\gamma\hat{V}^{\pi}(s^{\prime},\mathbf{w})-\hat{V}^{\pi}(s,\mathbf{w})$;
\item Q-learning (maximum TD(0)): $r+\gamma\operatorname*{max}_{a^{\prime}}\hat{Q}(s^{\prime},a^{\prime},\mathbf{w})-\hat{Q}(s,a,\mathbf{w})$.
}
One thing about VFA is that these algorithms may not converge or converge to a suboptimal solution.

\subsubsection{Deep Q-Learing}
This part would introduce Deep Q-learning (\cite{mnih2015human}), Double DQN (\cite{van2016deep}) and Dueling DQN (\cite{wang2016dueling}).\\
\sep{DQN}
The DQN is designed to estimate the state-action function $\hat{Q}(s,a,\mathbf{w})$.
\fig{DQN architecture}{DQN Architecture}{0.45}
The DQN architecture takes inputs consisting of an $84\times 84\times 4$ image, which denotes the state. But the original size of the
Atari game is $210\times160\times3$, so it needs some pre-processing:
\tab{
    \item \tit{Single frame encoding}:
    \item \tit{Dimensionality reduction}:
}
The loss function is given by:
\meq{
J(\mathbf{w})=\mathbb{E}_{(s_t,a_t,r_t,s_{t+1})}[(y_t^{DQN}-\hat{Q}(s_t,a_t,\mathbf{w}))^2],\\
y_t^{DQN}=r_t+\gamma\max_{a'}\hat{Q}(s_{t+1},a',\mathbf{w}^-).
}
Where $\mathbf{w}^-$ represents the parameters of the target network, and $\mathbf{w}$ is the online network after the update.\\
\tbf{Experience Replay}: The Q-network is updated by SGD with sampled gradients from minibatch data: $(s,a,r,s^{\prime})\sim\mathrm{Uniform}(D).$
$D$ is called the \tit{replay buffer}. This setting has some advantages and limitations:
\tab{
    \item \tit{Greater data efficiency}: Each step of experience can be potentially used for many updates, which improves data efficiency;
    \item \tit{Remove sample correlations}: Randomizing the transition experiences reduces the correlations between consecutive samples and therefore reduces the variance of updates
    and stabilizes the learning;
    \item \tit{Avoiding oscillations or divergence};
    \item \tit{Limitations}: Both the replay buffer and the uniform sampling don't t differentiate important transitions or informative transitions.
    A more sophisticated replay strategy is \tit{Prioritized Replay} by \cite{schaul2015prioritized}.
}
\tbf{Target Network} is used to improve the stability of learning and deal with the nonstationary learning targets.
For every $C=10000$ update steps the target network $\hat{Q}(s,a,\mathbf{w}^{-})$ is updated by copying the parameters' values $(\mathbf{w}^{-}=\mathbf{w})$
from the online network $\hat{Q}(s,a,\mathbf{w})$.
\fig{Deep Q-learning}{Deep Q-learning}{0.8}
\sep{DDQN}
Similar to the idea used in double Q-learning, DDQN uses two different networks to perform action selection and value function approximation.
However, DDQN doesn't maintain two independent networks but utilizes the target network by:
\eq{
y_t^{DDQN}=r_t+\gamma\hat{Q}(s_{t+1},\arg\max_{a'}\hat{Q}(s_{t+1},a',\mathbf{w}),\mathbf{w}^-).
}
\sep{Dueling DQN}
Recall the \tit{advantage function}:
\eq{
    A^\pi(s,a)=Q^\pi(s,a)-V^\pi(s).
}
We have $\mathbb{E}_{a\sim\pi(s)}[A^{\pi}(s,a)]=0.$ Like in DQN, the dueling network is also a neural network function approximator for
learning the Q-function. Differently, it approximates the Q function by decoupling the value function and the advantage function.
\fig{Dueling DQN v.s. DQN}{Dueling DQN v.s. DQN}{0.5}
Where the green layer conducts the following operations:
\meq{
    \hat{Q}(s,a,\mathbf{w},\mathbf{w_A},\mathbf{w_V})=\hat{V}(s,\mathbf{w},\mathbf{w_V})+\left(A(s,a,\mathbf{w},\mathbf{w_A})-\max_{a'\in A}A(s,a',\mathbf{w},\mathbf{w_A})\right);\\
    \hat{Q}(s,a,\mathbf{w},\mathbf{w_A},\mathbf{w_V})=\hat{V}(s,\mathbf{w},\mathbf{w_V})+\left(A(s,a,\mathbf{w},\mathbf{w_A})-\frac{1}{|\mathcal{A}|}\sum_{a'}A(s,a',\mathbf{w},\mathbf{w_A})\right).
}
in which \mathbf{w_V} is the parameter of the FC layer to predict the value functions, and \mathbf{w_A} is the parameter of the FC layer to predict the advantage functions.

\subsection{Policy-Based Algorithms}

In most RL algorithms, the policy y does not exist without the action value estimates $Q(s,a)$, these are called \tbf{value-based} algorithms.
These algorithms have several limitations:
\lis{
    \item Can only address discrete action spaces due to the $\arg\max_{a\in A}$ operation;
    \item Computing $Q(s,a)$ for all state-action pairs is costly;
    \item Can only improve the policy indirectly by improving the estimates of the value function.
}
\tbf{Policy-based} method directly parameterizes the policy function $\pi_\theta(s)$ without calculating the value functions.
A value function may still be used to learn the policy parameters, but is not required for action selection. To implement the policy-based methods,
an important step is to approximate policies with parametrization. In discrete case, we can use softmax and \gt{state-action preferences}
$h(s,a,\theta)$ to parameterize a policy:
\eq{
    \pi(a\mid s,\boldsymbol{\theta})=\frac{\exp(h(a,s,\boldsymbol{\theta}))}{\sum_{a'}\exp(h(a',s,\boldsymbol{\theta}))}.
}
In continuous cases, we can use the following functions to parameterize a policy:
\meq{
\pi(a\mid s,\boldsymbol{\theta_\mu},\boldsymbol{\theta_\sigma})=\frac{1}{\sigma(s,\theta_{\sigma})\sqrt{2\pi}}\exp(-\frac{(a-\mu(s,\theta_{\mu}))^2}{2\sigma(s,\theta_{\sigma})^2});\\
\mu(s,\boldsymbol{\theta})=\boldsymbol{\theta}_{\boldsymbol{\mu}}^T\boldsymbol{x}_{\boldsymbol{\mu}}(s),\quad\sigma(s,\boldsymbol{\theta})=\exp(\boldsymbol{\theta}_{\boldsymbol{\sigma}}^T\boldsymbol{x}_{\boldsymbol{\sigma}}(s)),
}

\subsubsection{Policy Gradient}
In the settings where the episode terminates at some terminal state set, we can define the objective function as:
\eq{
\boldsymbol{J}(\boldsymbol{\theta})=V^{\pi_\theta}(s_0)=\sum_{s\in\mathcal{S}}\rho^{\pi_\theta}(s\mid s_0)r(s),
}
where $r(s)=\mathbb{E}_{a\operatorname*{\sim}_{T}}[\mathcal{R}(s,a)]$ and $\rho^{\pi_\theta}(s\mid s_0)=\frac{1}{T}\sum_{t=0}^T\mathbb{P}(s_t=s\mid s_0,\pi_\theta)$.
In the continuous setting where the process continues infinitely, we can define:
\eq{
    \begin{aligned}
        \boldsymbol{J}(\boldsymbol{\theta}) & =\lim_{T\to\infty}\frac1T\sum_{t=1}^T\mathbb{E}[r_t\mid s_0,\pi_\theta] \\
                                            & =\lim_{t\to\infty}\mathbb{E}[r_t\mid s_0,\pi_\theta]                    \\
                                            & =\sum_s\rho^{\pi_\theta}(s\mid s_0)r(s)                                 \\
                                            & =V^{\pi\theta}\left(s_0\right).
    \end{aligned}
}
\thm{Policy Gradient Theorem}{
    \eq{
        \begin{gathered}
            \nabla_\theta J(\theta) \propto\sum_{s\in\mathcal{S}}\rho^{\pi_\theta}(s\mid s_0)\sum_{a\in\mathcal{A}}Q^{\pi_\theta}(s,a)\nabla_\theta\pi_\theta(a\mid s) \\
            =\mathbb{E}_\pi\left[Q^{\pi_\theta}(s,a)\nabla_\theta\log\pi_\theta(a\mid s)\right].
        \end{gathered}
    }
    \proo{}{
        Here we only prove the episodic case.
        \eq{
            \begin{aligned}
                \nabla_\theta V^\pi(s)
                 & =\nabla_\theta\Big(\sum_{a\in\mathcal{A}}\pi_\theta(a\mid s)Q^\pi(s,a)\Big)                                                                                                                                                                                    \\
                 & =\sum_{a\in\mathcal{A}}\left(\nabla_\theta\pi_\theta(a\mid s)Q^\pi(s,a)+\pi_\theta(a\mid s)\nabla_\theta Q^\pi(s,a)\right)                                                                                                                                     \\
                 & =\sum_{a\in\mathcal{A}}\left(\nabla_\theta\pi_\theta(a\mid s)Q^\pi(s,a)+\pi_\theta(a\mid s)\nabla_\theta\sum_{s^{\prime},r}\mathbb{P}(s^{\prime},r\mid s,a)(r+V^\pi(s^{\prime}))\right)                                                                        \\
                 & =\sum_{a\in\mathcal{A}}\left(\nabla_\theta\pi_\theta(a\mid s)Q^\pi(s,a)+\pi_\theta(a\mid s)\sum_{s^{\prime},r}\mathbb{P}(s^{\prime},r\mid s,a)\nabla_\theta V^\pi(s^{\prime})\right)                                                                           \\
                 & \begin{aligned}=\sum_{a\in\mathcal{A}}\left(\nabla_\theta\pi_\theta(a\mid s)Q^\pi(s,a)+\pi_\theta(a\mid s)\sum_{s'}\mathbb{P}(s'\mid s,a)\nabla_\theta V^\pi(s')\right)\end{aligned}
            \end{aligned}
        }
        The above equation can be written as:
        \eq{
            \color{red}{\nabla_\theta V^\pi(s)}=\sum_{\alpha\in\mathcal{A}}\left(\nabla_\theta\pi_\theta(a\mid s)Q^\pi(s,a)+\pi_\theta(a\mid s)\sum_{s^{\prime}}\mathbb{P}(s^{\prime}\mid s,a)\color{red}{\nabla_\theta V^\pi(s^{\prime})}\right).
        }
        This equation has a nice recursive form (see the red parts!). To unroll the recursion of $\nabla_\theta V^\pi(s^{\prime})$, denote
        \meq{
            \phi(s)=\sum_{a\in\mathcal{A}}\nabla_\theta\pi_\theta(a\mid s)Q^\pi(s,a)\\
            \eta(s){=}\sum_{k=0}^\infty\rho^\pi(s_0{\to}s,k)
        }
        for simplicity, and denote $\rho^\pi(s\to s^\prime,k)$ as the probability of transitioning from $s$ to $s^\prime$ with policy $\pi$ after $k$ steps. We have:
        \eq{
            \begin{aligned}
                \nabla_\theta J(\theta) =\nabla_\theta V^\pi(s_0) & = \phi(s_0)+\sum_a\pi_\theta(a\mid s)\sum_{s'}\mathbb{P}(s'\mid s_0,a)\nabla_\theta V^\pi(s')                                      \\
                                                                  & = \phi(s_0)+\sum\rho^{\pi}(s_0\rightarrow s^{\prime},1)\nabla_{\theta}V^{\pi}(s^{\prime})                                          \\
                                                                  & = \phi(s_0)+\sum_{s'}\rho^{\pi}(s_0\rightarrow s',1)\phi(s')+\sum_{c''}\rho^{\pi}(s_0\rightarrow s'',2)\nabla_{\theta}V^{\pi}(s'') \\
                                                                  & = \sum_s\sum_{k=0}^\infty\rho^\pi(s_0\to s,k)\phi(s) = \sum_s\eta(s)\phi(s)                                                        \\
                                                                  & = \propto\sum_s\frac{\eta(s)}{\sum_s\eta(s)}\phi(s)=\sum_s\rho^\pi(s\mid s_0)\sum_a\nabla_\theta\pi_\theta(a|s)Q^\pi(s,a).
            \end{aligned}
        }
        Further, the policy gradient can be written as:
        \eq{
            \begin{aligned}
                \nabla_\theta J(\theta) & \propto\sum_{s\in\mathcal{S}}\rho^\pi(s\mid s_0)\sum_{a\in\mathcal{A}}Q^\pi(s,a)\nabla_\theta\pi_\theta(a\mid s)                                          \\
                                        & =\sum_{s\in\mathcal{S}}\rho^\pi(s\mid s_0)\sum_{a\in\mathcal{A}}Q^\pi(s,a)\pi_\theta(a\mid s)\frac{\nabla_\theta\pi_\theta(a\mid s)}{\pi_\theta(a\mid s)} \\
                                        & =\sum_{s\in\mathcal{S}}\rho^\pi(s\mid s_0)\sum_{a\in\mathcal{A}}\pi_\theta(a\mid s)Q^\pi(s,a)\frac{\nabla_\theta\pi_\theta(a\mid s)}{\pi_\theta(a\mid s)} \\
                                        & =\mathbb{E}_\pi[Q^\pi(s,a)\nabla_\theta\log\pi_\theta(a\mid s)],
            \end{aligned}
        }
    }
}
\fig{REINFORCE}{REINFORCE}{0.7}
For $Q^\pi(s,a)$, we can use $G_t=\sum\gamma^tr_t$ to estimate. For $\nabla_\theta\log\pi_\theta(a\mid s)$, it depends on its form.
Policy gradient suffers from \rt{high variance}, one solution is by subtracting a baseline $b(s)$ independent of $\theta$:
\eq{
\nabla_\theta J(\theta)\propto\sum_{s\in\mathcal{S}}\rho^\pi(s\mid s_0)\sum_{a\in\mathcal{A}}(Q^\pi(s,a)-b(s))\nabla\pi_\theta(a\mid s).
}
We can see that:
\eq{
    \sum_ab(s)\nabla\pi(a\mid s,\theta)=b(s)\nabla\sum_a\pi(a\mid s,\theta)=b(s)\nabla1=0,
}
which means the expectation value does not change. A natural choice for the baseline is an estimate of the state value $\hat{V}(s,\boldsymbol{w})$.
\fig{REINFORCE with baseline}{REINFORCE with baseline}{0.7}
\re{
    Why the introduction of baseline reduces the variance:\\
    Assume $b(s)$ is an unbiased estimator of $V^\pi(s)$, then the term $R_t(\tau)-b(s_t)$ has a mean $0$. Thus,
    \eq{
        \begin{aligned}
            \mathrm{Var}\left(\sum_{t=0}^{T-1}\nabla_\theta\log\pi_\theta(a_t|s_t)(R_t(\tau)-b(s_t))\right) & \approx\sum_{t=0}^{T-1}\mathbb{E}\tau\left[\left(\nabla_\theta\log\pi_\theta(a_t|s_t)(R_t(\tau)-b(s_t))\right)^2\right]                                 \\
                                                                                                            & \approx \sum_{t=0}^{T-1}\mathbb{E}_\tau\left[\left(\nabla_\theta\log\pi_\theta(a_t|s_t)\right)^2\right]\mathbb{E}_\tau\left[(R_t(\tau)-b(s_t))^2\right]
        \end{aligned}
    }
}

\subsubsection{Actor-Critic}

\clearpage
\chapter{Operations Management}

\section{Empirical Operations Management}

\cite{roth2007applications} describes the evolution of empirical OM from 1980 to 2007, the author selects 12 profounding papers
in this domain. \cite{brusco2017cluster} reviewed the clustering methods applied in 6 OM journals.\\
\cite{choi2016multi}, multi-methodological OM is advocated, which includes the empirical methodology.
\defi{Multi-methodological OM}{an approach for OM research in which at least two distinct OM research
    methods are employed nontrivially to meet the research goals.}
\cite{roth2022pioneering} classified 75 papers as empirical among the top 200 cited papers in \textit{POM}, these papers are mainly from 3 topical areas:
\lis{
    \item Responsibility Operations: covers environmental management, sustainability, and humanitarian efforts;
    \item Supply Chain Management: bullwhip effect, risk management, supply chain finance;
    \item Manufacturing Strategy and Quality Management.
}
Primary data (surveys, experiments, interviews) are used in these studies, followed by secondary data (public database, firm's data).
\no \hl{Roth's suggestions}:
\lis{
    \item For some topic which is very intuitive and not surprising, focus on the \textbf{size} of effect rather than sign;
    \item Avoid the confirmation bias and focus on consistency;
    \item Focus on endogeneity and causality, using common sense simultaneously.
}
\cite{kumar2022expanding} reviewed different domains of OM publications on \textit{POM}, within which a section about empirical OM is covered.\\
\cite{mithas2022causality} reviewed 411 empirical papers form 2016-2021 on \textit{POM}, with a
causal inference and counterfactual perspective.
\fig{identification strategies}{identification strategies from 2016 to 2021}{0.95}
\re{Two challenges in assessing causality:\\
    \eq{\begin{aligned}T & =ATE+\left[E(Y(0)|Z=1)-E(Y(0)|Z=0)\right] \\
                 & +(1-\pi)*\left[(ATT-ATU)\right].\end{aligned}}
    \tab{
        \item $\left[E(Y(0)|Z=1)-E(Y(0)|Z=0)\right]$ is the \textbf{baseline bias}, coming from \textit{OVB} or
        \textit{simultaneity};
        \item $\left[(ATT-ATU)\right]$ is the \textbf{differential treatment bias}.
    }
}
\fig{identification techniques}{How identification techniques works}{}
\cite{fisher2022innovations} especially investigate the empirical research in retail operations from
traditional ones like forecasting and inventory planning, to new technologies, like radio frequency
identification (RFID) and e-commerce.

\clearpage
\section{Revenue Management}

Revenue management is a data-driven system to price perishable assets tactically
at the micro-market level to maximize expected revenue or profit. Some critical reviews
before 2009 can be found in the book \cite{Gallego2019}.
There are some extra summary papers like \cite{Strauss2018},
\cite{Klein2020}.

\subsection{Traditional RM}


\key{\tab{
        \item Protection Level, Booking Limit, Littlewood's Rule
    }}

\ass{What does \textbf{"traditional"} means in RM?}{
    \lis{
        \item The traditional RM system doesn't consider the choice model,
        in particular, it assumes the demands are independent random variables;
        \item Further assumption: consumer will leave without purchasing if preferred fare
        class is unavailable (holds when gaps in fares are large enough);
        \item The capacity is fixed, the capacity's marginal profit is zero(can be relaxed);
        \item All booked consumers would arrive (another circumstance see \ref{subsec:overbooking}).}}
\ass{Single Resource RM}{
    \lis{
        \item The units of capacity is $c$, pricing at multiple different level $p_n<\cdots<p_1$;
        \item Low-before-high fare class arrival order: $D_2$ before $D_1$ for example
        (this is the worst case for revenue);
        \item \textbf{Protection level} for customer $j$: leave $y \in \{0,1,\cdots,c\}$ for $D_{j-1},\codts,D_1$; $c-y$ is
        the \textbf{booking limit} which serves $D_j$;
    }
}
\emocool So the problem is to solve the optimal protection level given the current consumer level $j$.

Let $V_j(x)$ be the optimal revenue given $D_j$ coming in, $x$ units remained. $V_0(x)=0$ by design.
Let $y$ be the protection level for $D_{j-1},\cdots,D_1$: sales at $p_j$ = $\min\{x-y,D_j\}  $.
The remaining capacity for $D_{j-1},\cdots,D_1$ is $x-\min\{x-y,D_j\}=max\{y,x-D_j\}$.
Now let $W_j(y,x)$ be the optimal solution. We have:

\eq{W_j(y,x)=p_j\mathbb{E}\{\min\{x-y,D_j\}\}+\mathbb{E}\{V_{j-1}(\max\{y,x-D_j\})\}}
\eq{V_{j}(x)=\max_{y\in\{0,\ldots,x\}}W_{j}(y,x) =
\max_{y\in\{0,...,x\}}\left\{p_{j}\mathbb{E}\{\min\{x-y,D_{j}\}\}+
\mathbb{E}\{V_{j-1}(\max\{y,x-D_{j}\})\}\right\}}

\pro{Structure of the Optimal Policy}{
    \begin{equation}
        y_{j-1}^*=\max\{y\in\mathbb{N}_+:\Delta V_{j-1}(y)>p_j\}.
    \end{equation}
    The maximizer of $W_j(y,x)$ is given by $y_j^*,\codts, y_1^*$
}

\re{The optimal solution for $y_j $is independent of the distribution of $D_j$;}

\co{When $j=2$:}{
    \thm{Littlewood's rule}{
        \eq{y_1^*=\max\{y\in\mathbb{N}_+:\mathbb{P}\{D_1\geq y\}>r\}};

    }
}

\re{The Littlewoood's Rule:\lis{
\item The solution depends on the \textbf{fare ratio}: $r:=p_2/p_1$;
\item When the distribution of $D_2$ is continuous: $F_{1}(y)=\mathbb{P}\{D_{1}\leq y\}$.
The optimal protection level is $y_1^*=F_1^{-1}(1-r)=\mu_{1}+\sigma_{1}\notin^{-1}(1-r)$:\lis{
    \item if $r>\frac{1}{2}$, $y_1^*<\mu_1$ and $y_1^*$ decreases with $\sigma_1$;
    \item if $r<\frac{1}{2}$, $y_1^*<\mu_1$ and $y_1^*$ increases with $\sigma_1$;
    \item if $r=\frac{1}{2}$, $y_1^*=\mu_1$;}
\item Using Littlewood's rule would result in some $D_1$ served by competitors (high spill rates).
Solution: add penalty to save more seats for the high fare consumers:
\eq{ y_1^*=\max\left\{y\in\mathbb{N}_+:\mathbb{R}\{D_1\geq y\}>\frac{p_2}{p_1+\rho}\right\}}
}}

\subsection{Overbooking} \label{subsec:overbooking}

\subsection{Traditional Consumer Choice Model}

\subsection{Current Consumer Choice Model}

\clearpage
\section{Dynamic Pricing}

\subsection{Basic Pricing Theory}

This subsection summarizes the basic pricing theory for \tbf{multi-product momnpoly} firms.\\
\sep{Perspective from the Firm}
The firm's profit function is given by:
\eq{
    R(p,z):=(p-z)^{\prime}d(p)=\sum_{i=1}^n(p_i-z_i)d_i(p_1,\ldots,p_n),
}
Where $z=(z_1,\cdots,z_n)$ is the variable cost vector, $p=(p_1,\cdots,p_n)$ is the price vector, $d(p)$ is the demands.
Currently it's popular to set $p_i \in [0,\infty]$, where by setting $p_i=\infty$ is equivalent to not offering product $i$.\\
Consider the revenue as a function only of $z$, given by:
\eq{\mathcal{R}(z):=\max_{p\in X}R(p,z),}
Where $X$ is the set of allowable prices.
\thm{The Decreasing Covex Property}{
    $\cR (z)$ is \tbf{decreasing convex} in $z$.
    \proo{}{
        The decreasing property is obvious since $R(p,z)$ is decreasing in $z$, here is the proof for convexity.\\
        For any $z,\tilde{z}$:
        \eq{
            \begin{aligned}
                \begin{aligned}\mathcal{R}(\alpha z+(1-\alpha)\tilde{z})\end{aligned} & =\max_{p\in X}R(p,\alpha z+(1-\alpha)\tilde{z})                                      \\
                                                                                      & =\max_{p\in X}R(\alpha p+(1-\alpha)p,\alpha z+(1-\alpha)\tilde{z}))                  \\
                                                                                      & =\max_{p\in X}\left[\alpha(p-z)^{\prime}+(1-\alpha)(p-\tilde{z})^{\prime}\right]d(p) \\
                                                                                      & =\max_{p\in X}\left[\alpha R(p,z)+(1-\alpha)R(p,\tilde{z})\right]                    \\
                                                                                      & \leq\alpha\max_{p\in X}R(p,z)+(1-\alpha)\max_{p\in X}R(p,\tilde{z})                  \\
                                                                                      & =\alpha\mathcal{R}(z)+(1-\alpha)\mathcal{R}(\tilde{z}).
            \end{aligned}
        }
        \emogood \rt{This proof is very similar to proving the convexity of the hyperplane}.
    }
}
Transforming the revenue function as a function only to $z$ is useful. Because by \tbf{Jensen's Inequality}, $\mathbb{E}[\mathcal{R}(\mathbb{Z})]\geq\mathcal{R}(\mathbb{E}[\mathbb{Z}])$.
Their differences can be interpreted as the difference between a dynamic pricing policy $p(Z)$ that responds to changes in $Z$ and a static policy $\mathcal{R}(\mathbb{E}[\mathbb{Z}])$,
the larger the variance of $Z$, the larger the gap between them.
\sep{Perspective from the Consumers}
To answer whether consumers are better off with pricing policy $p(Z)$ or $p(\bE(Z))$,
we can frame this using the \tbf{utility theory} (\cite{chen2019welfare}).
Assume that consumers purchase a non-negative vector $q=(q_1,q_2,\cdots,q_n)$ of products, their \tit{net utility} is given by:
\eq{
S(q,p):=U(q)-q^{\prime}p,
}
where $U(q)$ us an increasing concave function. Then the \tit{optimal surplus} can be conputed by:
\eq{
    \mathcal{S}(p):=\max_{q\geq0}S(q,p)
}
where the solution $q^{\star}=d(p)=\nabla^{-1}U(p)$. under the first-order condition. Similar to the analysis from the firm's
perspective, we can get $\mathbb{E}[\mathcal{S}(P)]\geq\mathcal{S}(\mathbb{E}[P])$.
And at last, we can achieve:
\eq{
\mathbb{E}[\mathcal{S}(p(\mathbb{Z}))]\geq\mathcal{S}(\mathbb{E}[p(\mathbb{Z})])\geq\mathcal{S}(p(\mathbb{E}[Z])).
}

\subsection{Dynamic Pricing and Reinforcement Learning}

\cite{misra2019dynamic} is probably the first paper that incorporates reinforcement learning (MAB) algorithm
into dynamic pricing.
\ass{\cite{misra2019dynamic}}{
For consumers:
\lis{
    \item She has stable preferences $v_i$ doesn't change over time;
    \item Her choice satisfies weak axiom of revealed preference: if $v_i>p$, then she would buy it;
    \item Heterogeneity among consumers can be separated as observable and unobservable heterogeneity: $v_i=f(\mathbb{Z}_i)+\nu_i$;
    \item There are bound for $\nu_i: \nu_{i}\in[-\delta,\delta]$；
}
Model Setting:
\lis{
\item There are $K$ prices that a firm can choose: $p\in\{p_1,\ldots,p_K\}$;
\item sample mean $\bar{\pi_{kt}}=1/n_{kt}\sum_{\tau=1}^{n_{kt}}\pi_{k\tau}$, a policy is defined as $p_t=\Psi(\{p_\tau,\pi_\tau|\tau=1,\ldots,t-1\})$;
\item The regret is given by: $\mathrm{Regret}(\Psi,\{\pi(\mathfrak{p_k})\},t)=\pi^*t-\sum_{k=1}^{K}\pi(p_k)\mathbb{E}[n_{kt}]$
}
There are $K$ segments of consumers, each segment is denoted by $\theta_k$ and $s$, and their behaviour follows:
\lis{
    \item $D(p_k)_{s,t}=0$ is consistent with consumers being of type $\{\theta_1,\cdots,\theta_{k-1}\}$;
    \item $D(p_k)_{s,t}=1$ is consistent with consumers being of type $\{\theta_k,\cdots,\theta_{K}\}$;
}
}
In this paper a novel algorithm (UCB-PI) is proposed and verified that can outperform other algorithms.\\
\cite{calvano2020artificial}

\clearpage
\section{Platform Operations Management}

\cite{rietveld2021platform} reviews literature in platform competitions.

\cite{wang2023} uses game theory framework to analyze the cross-licensing policy initiated by Qualcomm, which provides some insights
for the up-stream manufacturing company:
\lis{
    \item The supplier shouldn't adopt the cross-licensing policy if the inferior manufacturer's cost of innovation is high;
    \item Cross-licensing may achieve a higher level of total innovation if the superior manufacturer's cost of innovation is low;
    \item The superior manufacturer can benefit from cross-licensing, if innovation is costly but the manufacturers' costs of
    innovation is similar.
}

\subsection{Hotel Platform}
\sep{An overview of Airbnb}
\cite{Dolnicar2021} provides a comprehensive illustration of all aspects of Airbnb, including the business model, competitive landscape, and the
regulations of Airbnb. \\
\cite{guttentag2019progress} reviewed some tier c papers about the progress on Airbnb, their main focus is on the loyalty and motivation of
guests and hosts, Airbnb's regulation and culture, as well as Airbnb's impact on the tourism sector.\\
Airbnb makes money by renting out property that it doesn't own,
the hosts can be an individual or a company (But Airbnb doesn't own property, it's just an intermediary). (\cite{Folger2023})
In 2017, Airbnb invested in Niido, a hotel-like apartment program managed by Airbnb. In 2020, Airbnb ended its
partnership with Niido apartments. During the whole process, only 2 apartments were in service.\\
\cite{zhang2022makes} studies how Airbnb property demand changed after the acquisition of \tit{verified} images. Variables description:
\tab{
    \item Treatment variable: 212 properties had verified photos by the end of April 2017, the remaining 7,211 did not;
    \item Property demand: purchased date, the number of days in a month in which the property was open, blocked, and booked. $\frac{booked}{open}\times 100$
    \item Property Price: the price is endogenous because of the random demand shocks, characteristics of competing properties
    were used as IVs (The logic is that the characteristics of competing products are unlikely to be correlated with unobserved
    shocks in the demand for the focal property. However, the proximity of the characteristics of a property and its competitors influences
    the competition and as a result, the property markup and price). Cost-related variables are collected including residential utility fees.
    \item Property Photos: CNN architecture was used to predict the quality of a photo (dummy variable).
}
\tbf{Other Identification Techniques}:
\tab{
\item DiD model: $DEMAND_{itcym}=INTERCEPT+\alpha TREATIND_{it}+\lambda CONTROLS_{it}+PROPERTY_i+SEASONALITY_{cym}+\varepsilon_{it}$;
\item PSW method: calculate the prosensity score $\widehat{ps_i(X_i)}$ and use IPTW ($\omega_i(T,X_i)=\frac{T}{p\widehat{s_i(X_i)}}+\frac{1-T}{1-p\widehat{s_i(X_i)}}$) to weight the sample.
\item Relative time model was used to test the common trends assumption, Rosenbaum bounds test was used to test the validation of PSW methods.
}
Besides the work above, the authors investigate what makes a picture good. They listed 3 components (composition, color, figure-ground relationship) including 12 attributes and used the
following regression to give some human-interpretable suggestions:
\eq{
    \begin{aligned}
        DEMAND_{itcym}= & INTERCEPT+\alpha TREATIND_{it}                          \\
                        & +\mu IMAGE_{-}COUNT_{it}                                \\
                        & +\rho_1\text{ВАТНRООМ}_-\text{РНОТО}_-\text{КАТI}O_{it} \\
                        & +\rho_2BEDROOM_-PHOTO_-RATIO_{it}                       \\
                        & +\rho_3\text{КIТСНЕ}N_-\text{РНОТО}_-\text{КАТI}O_{it}  \\
                        & +\rho_4LIVING_-PHOTO_-RATIO_{it}                        \\
                        & +\eta IMAGE\_ATTRIBUTES_{it}                            \\
                        & +\lambda CONTROLS_{it}+SEASONALITY_{cym}
                        & + PROPERTY_i + \epsilon_{it}
    \end{aligned}
}
\cite{chen2023regulating} investigates the professional players' effects on the non-professional host. They define host who has more than
one properties on the platform simultaneously as professional players, and use a quasi-experiment (OHOH policy) and DiD model to analysis
whether competition effects or differentiation effects is dominant.\\
They predict 2 propositions:
\lis{
    \item If differentiation effects dominate, then OHOH would not affect the supply and price of non-professional hosts;
    \item If competition effects dominate, then OHOH would increase the supply and price of non-professional hosts.
}
\eq{
    Y_{it}=\mu_i+\nu_t+\beta{\cdot}1(\text{Policy})_{it}+\gamma^{\prime}\textbf{X}_{it}+\varepsilon_{it}
}
Their results show that the competition effects dominate the role of professional hosts.
\cite{farronato2022welfare} investigate the peer's entry on consumer's welfare in the accommodation industry using a structural model.\\
\tbf{Intuition}: Peer hosts are responsive to market conditions, expand supply as hotels fill up, and keep hotel prices down as a result.

\subsection{Platform Owner's Entry}

\key{Complementory Markets, Spillover Effects}

\cite{chen2023price} is the first paper considering the asymmetric information between the platform owner and the third-party sellers, and the effects of the information disadvantage
on consumer's welfare. They assemble data from Amazon: 122000 products, each with two sellers offering the same product. \tit{The information asymmetry}: when Amazon’s competitors make sales,
Amazon adjusts its prices accordingly; conversely, third-party sellers do not react to their competitors’ sales.\\
After observing the empirical evidence of the information asymmetry, they design a theoretical model and itentify the parameters. Using the structural regression, they find that by giving
information to third-party sellers. Both the Amazon, third-party sellers and the consumers' welfare would be increased.

\cite{zhu2018competing} surveys empirical studies that examine the direct entry of platform owners
into complementary product spaces.

\cite{zhu2018competing} studies the entry of Amazon platform. Logit regression is adopted to verify the following \tbf{Hypothesis}:
\tab{
    \item Platform owners are more likely to compete with a complementor when its products are successful;
    \item Platform owners are less likely to compete with a complementor when its products require significant platform-specific investments to grow.
}
\tbf{Identification Techniques}:
\tab{
    \item The sales ranking is used as proxy variable for the sales of the products;
    \item To overcome the impact of the referral rates by category-level fixed effects;
    \item To measure the seller's platform-specific investment, they calculate the seller's average answers;
}

\cite{he2020impact} investigates the effects and the mechanisms of platform owner's entry on third-party's online and offline demands using a B2B shopping platform's data.
They use DiD to identify that the platform owner's entry does harm the demands (more in online channels than offline channels).\\
What's more, they propose three mechanisms to explain the effects:
\lis{
    \item Competition Effects: The owner can appropriate value from the third-party sellers (only significant for online channels);
    \item Spillover Effects: increasing the exposure or awareness of the products (not the same as the mobile app market);
    \item Disintermediation effect: sellers would use defensive strategy to transact outside the platform (mostly in offline channel) see
    \cite{gu2021trust} and \cite{ha2022channel}.
}
Finally, DDD and PSM were adopted to identify the heterogeneity of the effects between large and small third-party stores.\\
\cite{deng2023can} use data from JD.com and provide an unexpected result, thet
\cite{wen2019threat} and \cite{foerderer2018does} focus on complementors' reactions, especially on innovation strategy for the platform owner entry.\\
\cite{shi2023comparing} investigates the timing of the platform owner's entry on the value creation.
\defi{Timing of Owner's Entry}{
    \tab{
        \item \tbf{Platform Complementors}: actors that offer an application that brings additional value to platform users when used in combination with the platform;
        \item \tbf{Early-entry}: the entry occurs when the ratio between the current and the eventual complementary market's size is low;
        \item \tbf{Late-entry}: entry to a relatively mature complementary market;
        \item \tbf{Value creation}: the activities geared toward increasing the perceived attractiveness of the platform ecosystem among customers and
        measure it as changes to complement popularity among customers. (proxy variable)
    }
}
\tbf{Identification Techniques}:
\tab{
    \item Use the \tit{the number of reviews} as the proxy for the popularity of complements (dependent variables);
    \item \tit{functional specificity} measures the heterogeneity of a complement based on the complexity of services offered by the compliment;
    \item Follows \cite{zhu2018competing} to account for platform-specific investments by \tit{interfacce coupling}: whether the complement connects with the platform core;
    \item To verify the exogeneity of Amazon's entry decision, a logit regression is conducted to test the number of reviews (popularity)
    does not influence Amazon's decision;
    \item PSM method is adopted.
}
\eq{
    Reviews_{it}=\alpha+\beta Treated_i \times After_t + \delta Controls_{it} + C_i + T_i + \epsilon_{it}
}
Many papers analyze the platform owner's entry from a game-theory perspective.\\
\cite{hagiu2020creating} investigate the owner's entry decision in the complementary markets.
\ass{Creating platforms by hosting rivals}{
    \tab{
        \item There are two companies $M$ and $S$: $M$ sells product $A$ and $B_M$, $S$ can only sell $B_S$;
        \item The costs to produce are all set to zero;
        \item The number of customers is normalized to one, $\lambda_A$ customers are only interested in $A$ ($A$-type), $1-\lambda_A$ are interested in both ($B$-type);
        \item $u_A>0$ for both type of customers, $u_B>0$ and $u_S=u_B+\Delta$ for $B$-type;
        \item There are costs for consumers to go to each store with a cost $\sigma$, and $0<\sigma<\min\{u_A,u_B\}, \Delta<\sigma$.
    }
}
The authors analyze two conditions: without hosting and hosting. By solving the equilibrium given two conditions, they find the following conclusion:
\lis{
    \item In the without hosting condition, $M$ would dominant the whole market, the profit for $S$ is zero;
    \item Even without transfer for $S$ to sell products in the platform $M$, $M$ can still benefit if $\lambda_A\leq\sigma/u_A$ and $\Delta>\lambda_A(u_A-\sigma)^+F/(1-\lambda_A)$.
}
\cite{cheng2022impact} analyze the owner's entry ob the incumbent sellers in the E-commerce platform.
\ass{
    Sell-on Contract
}{
    Without the entry, the demands for the two incumbent sellers are:
    \eq{
        \begin{aligned}D_1^s&=\frac12[1-p_1^s+\theta(p_2^s-p_1^s)]\\D_2^s&=\frac12[1-p_2^s+\theta(p_1^s-p_2^s)]\end{aligned}
    }
    After the entry, it is modified to:
    \eq{
        \begin{gathered}
            D_{r}^{o}= \frac{1}{2+a}\left[a-p_{r}^{o}+\frac{1}{2}[\delta(p_{1}^{o}-p_{r}^{o})+\delta(p_{2}^{o}-p_{r}^{o})]\right] \\
            D_{1}^{o} =\frac{1}{2+a}\left[1-p_{1}^{o}+\frac{1}{2}[\theta(p_{2}^{o}-p_{1}^{o})+\delta(p_{r}^{o}-p_{1}^{o})]\right] \\
            D_{2}^{o}= =\frac{1}{2+a}\left[1-p_{2}^{o}+\frac{1}{2}[\theta(p_{1}^{o}-p_{2}^{o})+\delta(p_{r}^{o}-p_{2}^{o})]\right]
        \end{gathered}
    }
    Where $a$ captures the strength of pltform's own brand. $\theta,\delta$ are the cross sensitivity.
}
They find that in both the sell-to and sell-on conditions, the entry of the platform owner may not harm the incumbent sellers.
What's more, comparing to the entry of a new third-party sellers, the incumbent sellers profit more when facing the entry of the owner.
\cite{lai2022fulfilled} investigates the introduction of Amazon's fullfillment program (FBA) on the third-party sellers in the E-commerce platform.
\ass{FBA on the third-party sellers}{
    \tab{
        \item Both Amazon and the third-party sell substitute products, at price of $p_A$ and $p_S$;
        \item Amazon procure the products from OEM supplier at price $w$ with a cost $c_0$, the third-party obtain its products with a cost $g$;
        \item The third party pays $r$ share of its revenue to Amazon as commission;
        \item The delivery cost for different means are all $c$, the Amazon provides a better service $S_A>S_s$, the third party can pay $T>c$ to use FBA;
    }
    Without FBA, the demands are given by:
    \eq{
        \begin{aligned}q_A&=Q_A-p_A+\beta p_S+\alpha s_A-\eta s_S,\\q_S&=Q_S-p_S+\beta p_A+\alpha s_S-\eta s_A,\end{aligned}
    }
    With FBA, the demands are given by:
    \eq{
        \begin{aligned}q_A&=Q_A-p_A+\beta p_S+\alpha s_A-\eta s_A,\\q_S&=Q_S-p_S+\beta p_A+\alpha s_A-\eta s_A.\end{aligned}
    }
    They assume are informations are common knowledge.
}
There are three decision makers: Amazon: $p_A, T$, Third-party: $p_S, FBA$, OEM: $w$:
\eq{
    \begin{array}{l}\Pi_{A,N}=(p_A-w-c)q_A(p_A,p_S)+rp_Sq_S(p_A,p_S)\\\Pi_{S,N}=[(1-r)p_S-g-c]q_S(p_A,p_S)\\\Pi_{M,N}=(w-c_0)q_A(p_A,p_S)\end{array}
}
To given insights which policy would benefit different parties, they took the following steps:
\lis{
\item In the without-FBA condition, given $w$, solve the equilibrium for $p_A^{\star}(w),p_S^{\star}(w)$;
\item Substituting them into the decision model of OEM, solve the optimal $w^{\star}$ in the SOSC condition.
\item In the FBA condition, repete above steps, solve the equilibrium for $p_A^{\star}(w),p_S^{\star}(w),w^{\star}$;
\item Substituting them into the decision model of OEM, solve the optimal $T^{\star}$.
}
They find some interesting insights:
\lis{
    \item The third party benifits from FBA when $T<\bar{T}$;
    \item Amazon can benefit from FBA if $\eta$ is either small or large;
    \item The FBA program can achieve a \tit{win-win-win} outcome for he third-party seller, Amazon and its OEM supplier.
}

\subsection{Consumer Polarization}

Consumer polarization is a topic in consumer research. \textbf{Group-Polarization Hypothesis} suggests that
group discussion generally produces attitudes that are more extreme in the direction of the average of
prediscussion attitudes in a variety of situations. Works like \cite{rao1991polarization} provides a mathematical
presentation for this phenomenon (in the domain of preference):
\eq{U_{\mathrm{s}}=\sum_{i=1}^{m}\lambda_{i}u_{i}+\phi(\bar{u}-K)\\
0\le \lambda_i \le 1, \sum_{i=1}^{m}\lambda_i = 1, \phi \ge 0}
In this model, the $\bar{u}$ is the algebraic mean of all consumers' utility, and $K$ is the \textbf{Pivot Point}.
Rewrite this formula:
\eq{\begin{align}
        U_{\mathrm{g}} & = \sum_{i=1}^{m} \left( \lambda_{i} + \frac{\phi}{m} \right) u_{i} - \phi K \\
                       & = w_{0} + w_{1} u_{1} + w_{2} u_{2} + \cdots + w_{m} u_{m}
    \end{align}\\
    \begin{array}{c}w_{0}\leq0; \\\sum\limits_{i=1}^{m}w_{i}\geq1;
        \\\\0\leq\frac{w_{0}}{1-\sum w_{i}}\leq1.\end{array}}

\re{\tab{
        \item \cite{Zhao2023} use experiment results to suggest that eWOM (electronic word of mouth) polarization (the
        degree of eWOM to which positive and negative sentiments are simultaneously strong) would decrease the consumers' intention
        to purchase, mediating by the enhancement of attitude ambivalence.
        (Ambivalence is a psychological state where a person endorses both positive and negative attitudinal positions)
        \item \cite{Iyer2021} use game theory framework, to get the conclusion that sequential decision-making could
        reduce the polarization.
    }}

\subsection{Network Effect}

\textbf{Network Effect} and \textbf{Network Externality}:

\cite{Narayan2011} verifies that peer influence affects attribute preferences via a Bayesian updating mechanism. In
their model, the utility is given as follows:
\eq{U_{ijp}^R=X_{jp}\beta_i^R+\lambda_i\varepsilon_{ijp}^R}
Where $U_{ijp}$ is the utility of consumer $i$ for product $j$ given choice set $p$, $X_{jp}$ is the attribute of
product $j$ in the choice set $p$, $\beta_i^R$ is the customers' weights. The Bayesian updating process is given below:
\eq{\begin{aligned}\beta_{ik}^R=\rho_{ik}\beta_{ik}^l+(1-\rho_{ik})\frac{\sum_{i=1,i\neq i}^Nw_{ii}\beta_{ik}^l}
        {\max\left[\left(\sum_{i=1,i\neq i}^Nw_{ii}\right),1\right]}, \\\mathrm{where~}0\leq\rho_{ik}\leq1.\end{aligned}}

Other research on peer influence:
\re{\item The consumers' interaction and social connections have a proposition proposed in \cite{Zhang2017} for
    their goal attainment and spending: a positive linear term plus a negative squared term;}

\subsection{Online Gaming}

Many industrial news about online gaming can be found in \cite{Chen2017}.
In \cite{lei2022revenue}, the dissertation fully discussed loot box pricing, matchmaking, and
price discrimination with fairness constraints.

\subsubsection{Play-Duration and Spending}

\cite{Zhang2017}'s work shows a nonlinear effect of social connections and interactions
on consumers’ goal attainment and spending: A positive linear terms and a negative squared term. Mechanism:
functional in providing useful information or tips that can facilitate goal attainment,
but would raise information overload problems.\\

Player engagement can be embodied by many specific metrics, such as time or money
spent in the game, the number of matches played within a time window, or churn risk.
\cite{Chen2017} define churn risk as the proportion of total players stopping playing the game over a period of time.

\subsubsection{Matchmaking}

Matchmaking connects multiple players to participate in online PvP games. (PvP(Player-versus-Player) games,
which cover many popular genres, such as multiplayer online battle arena (MOBA), first-person shooting (FPS),
and e-sports, have increased worldwide popularity in recent years.)\\

The past matchmaking strategy matches similar skilled players in the same round (SBMM), the current MM system focuses on improving
the players' engagement and decreasing the churn rate. For example, in \cite{Chen2017} EOMM (Engagement Optimization MatchMaking) is
proposed to minimize the churn rate.\\

\cite{chen2021matchmaking} propose an algorithm to maximize the cumulative active players.
\ass{Chen 2021 MatchMaking}{\lis{
\item players can have heterogeneous skill levels: level $1$ to level $K$;
\item the outcome of each match is a Bernoulli random variable: $p_{kj}=1-p_{jk},p_{kk}=0.5$,
$p_{kj}>0.5$ if $k>j$;
\item player's skill level is fixed: \textit{relative} level;
\item and their state depends on the win-loss outcomes of the past $m$ matches: $g\in\mathcal{G}$ ($2^m+1$
possible cardinality);
\item A geometric losing churn model: players churn with a fixed probability, starting
from the second loss in a row;
\item  $P_{win}^{k},P_{lose}^{k}\in[0,1]^{|\mathcal{G}|\times|\mathcal{G}|}$ is the transition matrix
of level $k$ player's engagement state;
\item $M_{kj}=p_{kj}P_{win}^{k}+(1-p_{kj})P_{lose}^{k}$ is the aggregate transition matrix.
($\bar{\matgcal{G}}$ is the reduced aggregate transition matrix);
\item using the fluid matching model and assume players are infinitely divisible;
}}
The \textbf{Dynamic Programming} formulation: $f_{kg,jg^{\prime}}$ is the amount of $kg$ players
matched with $jg^{\prime}$ players, $s^t_{kg}$ is the number of $kg$ players at time $t$.\\
\textbf{FB} \textit{flow balance} constraints:
\eq{\begin{aligned}
                                                                         & \sum_{j=1}^K\sum_{g^{\prime}\in\bar{\mathcal{G}}}f_{kg,jg^{\prime}}^t=s_{kg}^t,k=1,\ldots,K,\forall g\in\bar{\mathcal{G}}, \\
        \sum_{j=1}^K\sum_{g^{\prime}\in\bar{\cal G}}f_{jg^{\prime},kg}^t & =s_{kg}^t,k=1,\ldots,K,\forall g\in\bar{\mathcal{G}},                                                                      \\
        f_{kg,jg^{\prime}}^{t}                                           & =f_{jg^{\prime},kg}^t,j=1,\ldots,K,k=1,\ldots,K,\forall g\in\bar{\mathcal{G}},g^{\prime}\in\bar{\mathcal{G}}               \\
        f_{kg,jg^{\prime}}^{t}                                           & \geq0,j=1,\ldots,K,k=1,\ldots,K,\forall g\in\bar{\mathcal{G}},g^{\prime}\in\bar{\mathcal{G}}
    \end{aligned}}
\textbf{ED} \textit{evolution of demographics}:
\eq{\mathbf{s}_k^{t+1}=\sum_{j=1,\ldots,K}
\left(\mathbf{f}_{kj}^t\mathbf{1}\right)^\top\left(\bar{M}_{kj}+N_k\right)k=1,\ldots,K}
The value-to-go function is:
\eq{\begin{aligned}V^{\pi}(\mathbf{s}^{t})=&\sum_{k=1}^{K}\sum_{g\in\bar{\mathcal{G}}}s_{kg}^{t+1}+\gamma V^{\pi}(\mathbf{s}^{t+1})\\&\text{subject to (FB), (ED).}\end{aligned}}
The above model can be formulated in a linear programming style:
\thm{Chen 2021 MM LP Formulation}{
    \eq{\begin{aligned}
            V^{*}(\mathbf{s}^{0}) & =\max\sum_{t=1}^\infty\gamma^{t-1}\sum_k\sum_{g\in\bar{\mathcal{G}}}s_{kg}^t                                                                           \\
                                  & \mathrm{s.t.}\sum_{j=1}^{K}\sum_{g^{\prime}\in\bar{\mathcal{G}}}f_{kg,jg^{\prime}}^{t}=s_{kg}^{t},\forall k,\forall g\in\bar{\mathcal{G}},t=0,1,\ldots \\
                                  & \sum_{j=1}^K\sum_{g^{\prime}\in\bar{\mathcal{G}}}f_{jg^{\prime},kg}^t=s_{kg}^t,\forall k,\forall g\in\bar{\mathcal{G}},t=0,1,\ldots                    \\
                                  & f_{kg,jg^{\prime}}^t=f_{jg^{\prime},kg}^t,j=1,\ldots,K,k=1,\ldots,K,\forall g\in\bar{\mathcal{G}},g^{\prime}\in\bar{\mathcal{G}},t=0,1,\ldots          \\
                                  & f_{kg,jg^{\prime}}^t\geq0,j=1,\ldots,K,k=1,\ldots,K,\forall g\in\bar{\mathcal{G}},g^{\prime}\in\bar{\mathcal{G}},t=0,1,\ldots                          \\
                                  & \mathbf{s}_{k}^{t+1}=\sum_{j=1,\ldots,K}\left(\mathbf{f}_{kj}^{t}\mathbf{1}\right)^{\top}\left(\bar{M}_{kj}+N_{k}\right),\forall k,t=0,1,\ldots
        \end{aligned}}
}
\re{
    \tab{
        \item Using an optimal matchmaking policy instead of SBMM may reduce the required
        bot ratio significantly while maintaining the same level of engagement.
    }
}

\clearpage
\section{Behavioral Operations Management}

\clearpage
\section{Data-Driven Operations Management}

\clearpage
\chapter{Miscellaneous} \label{chap:miscellaneous}

\section{Notes on Tools} \label{sec:notestools}

\subsection{LaTeX Shortcuts}

There are 6x6 colors in the preset preamble:\\
\begin{center}
    \begin{tabular}{|*{6}{>{\centering\arraybackslash}m{2cm}|}}
        \hline
        \cellcolor{aa}\textcolor{black}{aa} &
        \cellcolor{ab}\textcolor{black}{ab} &
        \cellcolor{ac}\textcolor{black}{ac} &
        \cellcolor{ad}\textcolor{black}{ad} &
        \cellcolor{ae}\textcolor{black}{ae} &
        \cellcolor{af}\textcolor{black}{af}
        \\
        \hline
        \cellcolor{ba}\textcolor{black}{ba} &
        \cellcolor{bb}\textcolor{black}{bb} &
        \cellcolor{bc}\textcolor{black}{bc} &
        \cellcolor{bd}\textcolor{black}{bd} &
        \cellcolor{be}\textcolor{black}{be} &
        \cellcolor{bf}\textcolor{black}{bf}
        \\
        \hline
        \cellcolor{ca}\textcolor{black}{ca} &
        \cellcolor{cb}\textcolor{black}{cb} &
        \cellcolor{cc}\textcolor{black}{cc} &
        \cellcolor{cd}\textcolor{black}{cd} &
        \cellcolor{ce}\textcolor{black}{ce} &
        \cellcolor{cf}\textcolor{black}{cf}
        \\
        \hline
        \cellcolor{da}\textcolor{black}{da} &
        \cellcolor{db}\textcolor{black}{db} &
        \cellcolor{dc}\textcolor{black}{dc} &
        \cellcolor{dd}\textcolor{black}{dd} &
        \cellcolor{de}\textcolor{black}{de} &
        \cellcolor{df}\textcolor{black}{df}
        \\
        \hline
        \cellcolor{ea}\textcolor{black}{ea} &
        \cellcolor{eb}\textcolor{black}{eb} &
        \cellcolor{ec}\textcolor{black}{ec} &
        \cellcolor{ed}\textcolor{black}{ed} &
        \cellcolor{ee}\textcolor{black}{ee} &
        \cellcolor{ef}\textcolor{black}{ef}
        \\
        \hline
        \cellcolor{fa}\textcolor{black}{fa} &
        \cellcolor{fb}\textcolor{black}{fb} &
        \cellcolor{fc}\textcolor{black}{fc} &
        \cellcolor{fd}\textcolor{black}{fd} &
        \cellcolor{fe}\textcolor{black}{fe} &
        \cellcolor{ff}\textcolor{black}{ff}
    \end{tabular}
\end{center}

\no Using \verb=\href{URL}{text}= to refer a \href{EurekaTangChen.github.io}{website}.\\
\no Using \verb=\eq= to write equation, \verb=\tab= to get an unordered list,
\verb=\lis= to get an ordered list.\\
\eq{E=mc^2} \label{eq:Example}
\tab{
    \item item 1;
    \item item 2;
    \item item 3.
}
\lis{
    \item item 1;
    \item item 2;
    \item item 3.
}


\begin{tcblisting}{colback=red!5!white,colframe=red!75!black,listing side text,
        title=Format of Words,fonttitle=\bfseries}
    \hl{highlighted}, \ul{underlined}, \st{strikethrough}\\
    \rt{red}, \yt{yellow}, \bt{blue}, \gt{green}
\end{tcblisting}
\begin{tcblisting}{colback=red!5!white,colframe=red!75!black,listing side text,
        title=Shortcuts,fonttitle=\bfseries}
    \RR, \NN, \ZZ, \QQ\\
    \bA, \bB, \bC, \bD
\end{tcblisting}
\begin{tcblisting}{colback=red!5!white,colframe=red!75!black,listing side text,
        title=Shortcuts,fonttitle=\bfseries}
    \tbf{Text}, \tit{Text}\\
    \cA, \cB, \cC, \cD
\end{tcblisting}
\begin{tcblisting}{colback=red!5!white,colframe=red!75!black,listing side text,
        title=Emoji,fonttitle=\bfseries}
    \emogood, \emobad, \emocool, \emoheart, \emotree
\end{tcblisting}

\no Use \verb=\ass, \ax, \thm, \co, \pro, \defi, \re, \key, \ex, \proo= to
\no use preset tcolorboxes template.
\ass{Example}{\lis{
        \item item 1;
        \item item 2;
        \item item 3.
    }}
\ax{Exmaple}{Test} \label{ax:Example}
\thm{Exmaple}{Test}
\co{Exmaple}{Test}
\pro{Exmaple}{Test} \label{pro:Example}
\defi{Exmaple}{Test}
\re{Expamle}
\key{Example}
\ex{Problem example}{}
\proo{proposition \ref{pro:Example}}{The Formal Proof}

\no Using \verb=\label, \ref= to refer to chapters \ref{chap:miscellaneous},
sections \ref{sec:notestools}, equations \ref{eq:Example}, and boxes \ref{ax:Example}.\\
\no Using \verb=\cite= to cite the literature in apa style. For example:
\cite{Klein2020}\\
\no Using \verb=\sep= to insert a horizontal line with words in the middle:\\
\sep{Compilation}
Cleaning all the auxiliary files: LaTeXmk $\rightarrow$ BibTeX $\rightarrow$ LaTeXmk $\rightarrow$ LaTeXmk.
Or, zip the main files and upload them to Overleaf.\\
Put photos in the \textit{pic} file and use \verb|\fig| to show it.
\fig{temp}{Example.png}{0.6}
Using \verb=\alg= to write the pseudo code:
\alg{Example Code}{
    \Require $n \geq 0$
    \Ensure $y = x^n$
    \State $y \gets 1$
    \State $X \gets x$
    \State $N \gets n$
    \While{$N \neq 0$}
    \If{$N$ is even}
    \State $X \gets X \times X$
    \State $N \gets \frac{N}{2}$  \Comment{This is a comment}
    \ElsIf{$N$ is odd}
    \State $y \gets y \times X$
    \State $N \gets N - 1$
    \EndIf
    \EndWhile
}

\newpage
\listoffigures

\newpage
\listofalgorithms

\bibliographystyle{apalike}
\bibliography{reference}
\end{document}